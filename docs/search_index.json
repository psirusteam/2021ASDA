[["index.html", "Análisis de encuestas con R Prefacio", " Análisis de encuestas con R Andrés Gutiérrez1, Cristian Téllez2, Stalyn Guerrero3 2023-03-06 Prefacio La versión online de este libro está licenciada bajo una Licencia Internacional de Creative Commons para compartir con atribución no comercial 4.0. Este libro es el resultado de un compendio de las experiencias internacionales prácticas adquiridas por el autor como Experto Regional en Estadísticas Sociales de la CEPAL. Experto Regional en Estadísticas Sociales - Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ Profesor - Universidad Santo Tomás - cristiantellez@usta.edu.co↩︎ Consultor - Comisión Económica para América Latina y el Caribe (CEPAL), guerrerostalyn@gmail.com↩︎ "],["introducción.html", "Capítulo 1 Introducción", " Capítulo 1 Introducción FALTA ESTO: BADEHOG en la Cepal Las encuestas de hogares son uno de los instrumentos más importantes para hacer seguimiento a los indicadores de los Objetivos de Desarrollo Sostenible (ODS, por sus siglas) en el marco de la agenda 2030. Dada la importancia que tiene estas encuestas en la política pública de cada país, es necesario que los resultados que se obtengan de ellas sean lo más precisos y confiables posibles. En este sentido, las herramientas estadísticas utilizadas para obtener dichos resultados deben ser lo más robustas posibles. Particularmente, el diseño de muestreo utilizando, sin lugar a dudas, es un diseño de muestreo complejo. Entiéndase esto como como aquello diseños de muestreo en los cuales las unidades experimentales no pueden ser seleccionadas directamente del marco. Es decir, aquellos diseños que contienen más de una etapa, estratificación, conglomerados, etc. El objetivo principal de este libro es presentar los conceptos necesarios para hacer un análisis de encuestas complejas enfocadas en las dinámicas de los hogares. Particularmente, se presenta una guía práctica para analizar encuestas complejas usando R. Es por esto que, la dinámica que se trabaja en este texto es guiar al lector a cómo realizar un análisis completo de una encuesta compleja usando el software estadístico R con el paquete survey. En ese sentido, todos los ejemplos, tablas y gráficos que se presentan en este libro se producen con R, y los códigos computacionales para reproducir estarán disponibles para replicarlos. Se decide utilizar el software estadístico R para hacer los análisis puesto que, es un software de código abierto, lo que permite que cualquier investigador o instituto estadístico tenga acceso a él y es muy conocido y utilizado por el gremio estadístico, lo que lo hace conveniente para la enseñanza. El lector encontrará en este texto la siguiente estructura. En el capítulo 2 se describen los conceptos básicos de una encuesta compleja fundamentales para la correcta definición del diseño muestral en el entorno de las encuestas de hogares. En el capítulo 3 y 4 se definen los conceptos de variables aleatoria continua y discretas respectivamente en el contexto del muestreo probabilístico y, en el capítulo 5 se muestra como ajustar modelos de regresión lineal utilizando variables discretas y continuas empleando las herramientas del muestreo probabilístico. En el capítulo 6 se presentan las herramientas para ajustar modelos de regresión logística los cuales son fundamentales en el análisis de encuestas de hogares. Ahora bien, en los análisis estadísticos no solo son requeridos los modelos de regresión lineales, también, por la misma naturaleza de las variables capturadas en una encuesta de hogares, es necesario el ajuste de modelos lineales generalizados y multiniveles, estos conceptos son trabajados en el capítulo 7 y 8 respectivamente. Ahora bien, dada la pandemia la no respuesta en encuestas de hogares a aumentado de manera importante en los últimos años por lo que, es necesario recurrir a técnicas de imputación para la información no capturada en el trabajo de campo. Esta temática es trabajada en el capítulo 9. Por último, la presentación gráfica de los resultados en una encuesta de hogares será abordada en el capítulo 10. "],["conceptos-básicos-en-encuestas-de-hogares.html", "Capítulo 2 Conceptos básicos en encuestas de hogares", " Capítulo 2 Conceptos básicos en encuestas de hogares En este capítulo se presentan los conceptos básicos necesarios para la definición y análisis de una encuesta de hogares y son tomadas de Sarndal, Swensson &amp; Wretman (1992) &amp; Gutiérrez (2016). Alguno de los conceptos que se encontrarán están relacionados con la población objetivo, universo de estudio, marco muestral, etc. "],["universo-de-estudio-y-población-objetivo.html", "2.1 Universo de estudio y población objetivo", " 2.1 Universo de estudio y población objetivo El término encuesta se encuentra directamente relacionado con una población finita compuesta de individuos a los cuales es necesario entrevistar. El universo de estudio lo constituye el total de individuos o elementos que poseen dichas características a ser estudiadas. Ahora bien, conjunto de unidades de interés sobre los cuales se tendrán resultados recibe el nombre de población objetivo. Por ejemplo, la Encuesta Nacional de Empleo y Desempleo de Ecuador define su población objetivo como todas las personas mayores de 10 años residentes en viviendas particulares en Ecuador. "],["unidades-de-análisis.html", "2.2 Unidades de análisis", " 2.2 Unidades de análisis Corresponden a los diferentes niveles de desagregación establecidos para consolidar el diseño probabilístico y sobre los que se presentan los resultados de interés. En México, la Encuesta Nacional de Ingresos y Gastos de los Hogares define como unidades de análisis el ámbito al que pertenece la vivienda, urbano alto, complemento urbano y rural. La Gran Encuesta Integrada de Hogres de Colombia tiene cobertura nacional y sus unidades de análisis están definidas por 13 grandes ciudades junto con sus áreas metropolitanas. "],["unidades-de-muestreo.html", "2.3 Unidades de muestreo", " 2.3 Unidades de muestreo El diseño de una encuesta de hogares en América Latina plantea la necesidad de seleccionar en varias etapas ciertas unidades de muestreo que sirven como medio para seleccionar finalmente a los hogares que participarán de la muestra. La Pesquisa Nacional por Amostra de Domicilios en Brasil se realiza por medio de una muestra de viviendas en tres etapas, cada etapa se define como una unidad de muestreo. Por ejemplo, las unidades de muestreo en PNAD son: Las unidades primarias de muestreo (UPM) son los municipios, Las unidades secundarias de muestreo (USM) son los sectores censales, que conforman una malla territorial conformada en el último Censo Demográfico. Las últimas unidades en ser seleccionadas son las viviendas. "],["marcos-de-muestreo.html", "2.4 Marcos de muestreo", " 2.4 Marcos de muestreo Para realizar el proceso de selección sistemática de los hogares es necesario contar con un marco de muestreo que sirva de link entre los hogares y las unidades de muestreo y que permita tener acceso a la población de interés. En este sentido, el marco muestral es el conjunto en el cual se identifican a todos los elementos que componen la población objeto de estudio, de la cual se selecciona la muestra. Los marcos de muestreo más utilizados en encuestas complejas son de áreas geográficas que vinculan directamente a los hogares o personas. A modo de ejemplo, la Encuesta Nacional de Hogares de Costa Rica utiliza un marco muestral construido a partir de los censos nacionales de población y vivienda de 2011. Dicho marco corresponde a uno de áreas en donde sus unidades son superficies geográficas asociadas con las viviendas. Este marco permite la definición de UPM con 150 viviendas en las zonas urbanas y 100 viviendas en las zonas rurales. Este marco está conformado por 10461 UPM (64.5% urbanas y 35.5% rurales). "],["selección-de-una-muestra.html", "2.5 Selección de una muestra", " 2.5 Selección de una muestra "],["motivación.html", "2.6 Motivación", " 2.6 Motivación Desde que se popularizaron las encuestas de hogares en 1940, se ha hecho evidente algunas tendencias que están ligadas a los avances tecnológicos en las agencias estadísticas y en la sociedad y se han acelerado con la introducción del computador. Gambino &amp; Silva (2009) El muestreo es un procedimiento que responde a la necesidad de información estadística precisa sobre una población objetivo de estudio; Como lo menciona Gutiérrez (2016) el muestreo trata con investigaciones parciales sobre la población que apuntan a inferir a la población completa. Es así como en las últimas décadas ha tenido bastante desarrollo en diferentes campos principalmente en el sector gubernamental con la publicación de las estadísticas oficiales que permiten realizar un seguimiento a las metas del gobierno, en el sector académico, en el sector privado y de comunicaciones. Como se ha venido mencionando anteriormente, este libro está enfocado en el análisis de las encuestas de hogares. En ese sentido y para que el lector tenga una gama más amplia de ejemplos, en este capítulo se utilizará, para los ejemplos computacionales, la base de datos BigCity. Esta base es un conjunto de datos que contiene algunas variables socioeconómicas de \\(150266\\) personas de una ciudad en un año en particular. Alguna de las variables de esta base de datos son: HHID: Corresponde al identificador del hogar. PersonID: Corresponde al identificador de la persona dentro del hogar. Stratum: Corresponde al estrato geográfico del hogar. Son 119 estratos. PSU: Corresponde a las unidades primarias de muestreo. La base de datos cuenta con \\(1664\\) PSU. Zone: Corresponde a las áreas urbanas o rurales a lo largo de la ciudad. Sex: Corresponde al sexo del entrevistado. Income: Corresponde a los ingresos mensual per cápita. Expenditure: Corresponde a los gastos mensual per cápita. Employment: Situación laboral de la persona entrevistada. Poverty: Esta variable indica si la persona es pobre o no. Depende de los ingresos. "],["muestreo-aleatorio-simple-en-dos-etapas-estratificado.html", "2.7 Muestreo aleatorio simple en dos etapas estratificado", " 2.7 Muestreo aleatorio simple en dos etapas estratificado Con la finalidad de mantener un equilibrio entre los costos económicos y las propiedades estadísticas de la estrategia de muestreo se puede aprovechar la homogeneidad dentro de los conglomerados y, así, no tener que realizar censos dentro de cada Unidad Primaria de Muestreo (UPM) sino, proceder a seleccionar una sub-muestra dentro del conglomerado seleccionado. Los diseños de muestreo en las encuestas de hogares se caracterizan por ser diseños complejos los cuales involucran, entre otras, más de una etapa en la selección de las unidades de observación, estratos y estimadores complejos. En su mayoría, las unidades primarias de muestreo son seleccionadas dentro de los estrato. Ahora bien, según la teoría de muestreo (Cochran, W. G., 1977) se asume que el muestreo en cada estrato respeta el principio de la independencia. Esto es, las estimaciones del total, así como el cálculo y estimación de la varianza son el resultado de añadir o sumar para cada estrato la respectiva cantidad. Dentro de cada estrato \\(U_h\\) con \\(h=1,\\ldots, H\\) existen \\(N_{Ih}\\) unidades primarias de muestreo, de las cuales se selecciona una muestra \\(s_{Ih}\\) de tamaño \\(n_{Ih}\\) mediante un diseño de muestreo aleatorio simple. Suponga, además que el sub-muestreo dentro de cada unidad primaria seleccionada es también aleatorio simple. En este sentido, para cada unidad primaria de muestreo seleccionada \\(i\\in s_{Ih}\\) de tamaño \\(N_i\\) se selecciona una muestra \\(s_i\\) de elementos de tamaño \\(n_i\\). Como es ampliamente conocido, el proceso de estimación de un parámetro particular, por ejemplo, la media de los ingresos consiste en multiplicar la observación obtenida en la muestra por su respectivo factor de expansión y dividirlo sobre la suma de los factores de expansión de acuerdo con el nivel de desagregación que se quiera estimar. Sin embargo, cuando el diseño es complejo como es el caso de las encuestas de hogares, la estimación de la varianza se torna un poco difícil de realizar utilizando ecuaciones cerradas. Para estos casos y como lo recomienda la literatura especializada (Hansen, M. H., &amp; Steinberg, J., 1956)), se procede a utilizar la técnica del último conglomerado. Esta técnica consiste en aproximar la varianza sólo teniendo en cuenta la varianza de los estimadores en la primera etapa. Para esto se debe suponer que el diseño de muestreo fue realizado con reemplazo. Para poder utilizar los principios de estimación del último conglomerado en las encuestas de hogares se definen las siguientes cantidades: \\(d_{I_i} = \\dfrac{N_{Ih}}{n_{Ih}}\\), que es el factor de expansión de la \\(i\\)-ésima UPM en el estrato \\(h\\). \\(d_{k|i} = \\dfrac{N_{i}}{n_{i}}\\), que es el factor de expansión del \\(k\\)-ésimo hogar para la \\(i\\)-ésima UPM. \\(d_k = d_{I_i} \\times d_{k|i} = \\dfrac{N_{Ih}}{n_{Ih}} \\times \\dfrac{N_{i}}{n_{i}}\\), que es el factor de expansión final del \\(k\\)-ésimo elemento para toda la población \\(U\\). "],["práctica-en-r.html", "2.8 Práctica en R", " 2.8 Práctica en R En esta sección se utilizarán las funciones estudiadas en el capítulo anterior para la manipulación de la base de datos de ejemplo. Inicialmente, se cargarán las librerías ggplot2 que permitirá generar gráficos de alta calidad en R, TeachingSampling que permite tomar muestras probabilísticas utilizando los diseños de muestreo usuales, survey y srvyr que permitirán definir los diseños muestrales y por último dplyr que permite la manipulación de las bases de datos. library(ggplot2) library(TeachingSampling) library(dplyr) library(survey) library(srvyr) Una vez cargada las librerías, se procede a calcular la cantidad de personas en la base de datos, el total de ingresos y total de gastos para cada UPM dentro de cada estrato: data(&#39;BigCity&#39;) FrameI &lt;- BigCity %&gt;% group_by(PSU) %&gt;% summarise(Stratum = unique(Stratum), Persons = n(), Income = sum(Income), Expenditure = sum(Expenditure)) attach(FrameI) head(FrameI, 10) PSU Stratum Persons Income Expenditure PSU0001 idStrt001 118 70911.72 44231.78 PSU0002 idStrt001 136 68886.60 38381.90 PSU0003 idStrt001 96 37213.10 19494.78 PSU0004 idStrt001 88 36926.46 24030.74 PSU0005 idStrt001 110 57493.88 31142.36 PSU0006 idStrt001 116 75272.06 43473.28 PSU0007 idStrt001 68 33027.84 21832.66 PSU0008 idStrt001 136 64293.02 47660.02 PSU0009 idStrt001 122 33156.14 23292.16 PSU0010 idStrt002 70 65253.78 37114.76 Ahora bien, para calcular los tamaños poblacionales de los estratos (NIh) y los tamaños de muestra dentro de cada estrato (nIh), se realiza de la siguiente manera: sizes = FrameI %&gt;% group_by(Stratum) %&gt;% summarise(NIh = n(), nIh = 2, dI = NIh/nIh) NIh &lt;- sizes$NIh nIh &lt;- sizes$nIh head(sizes, 10) Stratum NIh nIh dI idStrt001 9 2 4.5 idStrt002 11 2 5.5 idStrt003 7 2 3.5 idStrt004 13 2 6.5 idStrt005 11 2 5.5 idStrt006 5 2 2.5 idStrt007 14 2 7.0 idStrt008 7 2 3.5 idStrt009 8 2 4.0 idStrt010 8 2 4.0 Si se desea extraer una muestra probabilística bajo un diseño aleatorio simple estratificado, se procede a utilizar la función S.STSI de la librería TeachingSampling como se muestra a continuación: samI &lt;- S.STSI(Stratum, NIh, nIh) UI &lt;- levels(as.factor(FrameI$PSU)) sampleI &lt;- UI[samI] Ahora bien, con la función left_join se procede a pegar los tamaños muestrales a aquellas UPM’s que fueron seleccionadas en la muestra: FrameII &lt;- left_join(sizes, BigCity[which(BigCity$PSU %in% sampleI), ]) attach(FrameII) Una vez se tiene la base de datos con la muestra de UMP’s. se selecciona aquellas variables que son de inetrés para el estudio como sigue a continuación: head(FrameII, 10) %&gt;% select(Stratum:Zone) Stratum NIh nIh dI HHID PersonID PSU Zone idStrt001 9 2 4.5 idHH00001 idPer01 PSU0001 Rural idStrt001 9 2 4.5 idHH00001 idPer02 PSU0001 Rural idStrt001 9 2 4.5 idHH00001 idPer03 PSU0001 Rural idStrt001 9 2 4.5 idHH00001 idPer04 PSU0001 Rural idStrt001 9 2 4.5 idHH00001 idPer05 PSU0001 Rural idStrt001 9 2 4.5 idHH00002 idPer01 PSU0001 Rural idStrt001 9 2 4.5 idHH00002 idPer02 PSU0001 Rural idStrt001 9 2 4.5 idHH00002 idPer03 PSU0001 Rural idStrt001 9 2 4.5 idHH00002 idPer04 PSU0001 Rural idStrt001 9 2 4.5 idHH00002 idPer05 PSU0001 Rural Luego de tener la información muestral de la primera etapa en la base FrameII se procede a calcular los tamaños de muestra dentro de cada UPM’s. En este caso, a modo de ejemplo, se tomará el 10% del tamaño de la UPM y se utilizará la función ceiling la cual aproxima al siguiente entero. HHdb &lt;- FrameII %&gt;% group_by(PSU) %&gt;% summarise(Ni = length(unique(HHID))) Ni &lt;- as.numeric(HHdb$Ni) ni &lt;- ceiling(Ni * 0.1) sum(ni) ## [1] 704 Teniendo el vector de tamaños de muestra para cada UMP, se procede a realizar la selección mediante un muestreo aleatorio simple con la función S.SI de la librería TeachingSampling. A modo ilustrativo, la selección en la segunda etapa del diseño se realizará, inicialmente para la primera UPM. Posterior a eso, se realizará un ciclo “for” para hacerlo con las demás UPM’s. Para la primera UPM se realiza de la siguiente manera: sam = S.SI(Ni[1], ni[1]) clusterII = FrameII[which(FrameII$PSU == sampleI[1]),] sam.HH &lt;- data.frame(HHID = unique(clusterII$HHID)[sam]) clusterHH &lt;- left_join(sam.HH, clusterII, by = &quot;HHID&quot;) clusterHH$dki &lt;- Ni[1] / ni[1] clusterHH$dk &lt;- clusterHH$dI * clusterHH$dki sam_data = clusterHH head(sam_data, 10) %&gt;% select(Stratum:Zone) Stratum NIh nIh dI PersonID PSU Zone idStrt001 9 2 4.5 idPer01 PSU0001 Rural idStrt001 9 2 4.5 idPer02 PSU0001 Rural idStrt001 9 2 4.5 idPer03 PSU0001 Rural idStrt001 9 2 4.5 idPer04 PSU0001 Rural idStrt001 9 2 4.5 idPer05 PSU0001 Rural idStrt001 9 2 4.5 idPer01 PSU0001 Rural idStrt001 9 2 4.5 idPer02 PSU0001 Rural idStrt001 9 2 4.5 idPer03 PSU0001 Rural idStrt001 9 2 4.5 idPer04 PSU0001 Rural idStrt001 9 2 4.5 idPer05 PSU0001 Rural Para las demás UPM’s seleccionadas en la etapa 1, for (i in 2:length(Ni)) { sam = S.SI(Ni[i], ni[i]) clusterII = FrameII[which(FrameII$PSU == sampleI[i]),] sam.HH &lt;- data.frame(HHID = unique(clusterII$HHID)[sam]) clusterHH &lt;- left_join(sam.HH, clusterII, by = &quot;HHID&quot;) clusterHH$dki &lt;- Ni[i] / ni[i] clusterHH$dk &lt;- clusterHH$dI * clusterHH$dki data1 = clusterHH sam_data = rbind(sam_data, data1) } encuesta &lt;- sam_data attach(encuesta) Una vez se obtiene la muestra (como se mostró anteriormente), el paso siguiente es definir el diseño utilizado y guardarlo como un objeto en R para posteriormente poderlo utilizar y realizar el proceso de estimación de parámetros y cálculo de indicadores. Para realizar esta tarea, se utilizará el paquete srvyr el cual ya fue definido en el capítulo anterior. Para este ejemplo, el diseño de muestreo utilizado fue un estratificado-multietápico en el cual, los estratos correspondieron a la variable Stratum, las UPM’s correspondieron a la variable PSU, los factores de expansión están en la variable dk y por último, se le indica a la función as_survey_design que las UPM’s están dentro de los estrato con el argumento nest = T. A continuación, se presenta el código computacional: diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = dk, nest = T ) Ya definido el diseño de muestreo como un objeto de R se puede empezar a extraer información del mismo. Por ejemplo, se pueden extraer los pesos de muestreo de dicho diseño con la función weights y luego sumarlos para revisar hasta cuánto me está expandiendo mi muestra. El código es el siguiente: sum(weights(diseno)) ## [1] 153533.5 Como se puede observar, el tamaño poblacional estimado utilizando el diseño propuesto es de \\(140579.2\\). Sin embargo, el tamaño poblacional de la base BigCity es de \\(150266\\). Es normal que esto suceda pero debe ser corregido puesto que la suma de los factores de expansión debe sumar el total de la población. La solución para esto es calibrar los pesos de muestreo que se abordará a continuación. "],["calibrando-con-r.html", "2.9 Calibrando con R", " 2.9 Calibrando con R La calibración es un ajuste que se realiza a los pesos de muestreo con el propósito de que las estimaciones de algunas variables de control reproduzcan de forma perfecta los totales poblacionales de estas variables (Sarndal, 2003). Esta propiedad de consistencia es deseable en un sistema de ponderadores. En este sentido, cuando los estudios por muestreo están afectados por la ausencia de respuesta, como en muchos casos pasa en las encuestas de hogares, es deseable tener las siguientes propiedades en la estructura inferencial que sustenta el muestreo: Sesgo pequeño o nulo. Errores estándares pequeños. Un sistema de ponderación que reproduzca la información auxiliar disponible. Un sistema de ponderación que sea eficiente al momento de estimar cualquier característica de interés en un estudio multipropósito. La calibración es usualmente el último paso en el ajuste de los ponderadores. Hace uso de información auxiliar que reduce la varianza y corrige los problemas de cobertura que no pudieron ser corregidos en los pasos previos. Puesto que el estimador de calibración depende exclusivamente de la información auxiliar disponible, esta información puede aparecer en diversas formas: Puede estar de forma explícita en el marco de unidades. \\(x_k \\ (\\forall \\ k \\in U)\\) Puede ser un agregado poblacional proveniente de un censo o de registros administrativos. \\(t_x = \\sum_U x_k\\) Puede ser una estimación poblacional \\(\\hat{t}_x = \\sum_s w_kx_k\\) muy confiable. Particularmente, en encuestas de hogares, existen conteos de personas disponibles a nivel de desagregaciones de interés. Por ejemplo, número de personas por edad, raza y género que se permite utilizar como información auxiliar para calibrar las estimaciones. La necesidad de calibrar en las encuestas de hogares es porque no todos los grupos de personas se cubren apropiadamente desde el diseño de muestreo. Además, las estimaciones del número de personas en estos subgrupos son menores a las proyecciones que se tienen desde los censos. Por último, al ajustar los pesos para que sumen exactamente la cifra de los conteos censales, se reduce el sesgo de subcobertura. Para ejemplificar el estimador de calibración en R usando la base de datos de ejemplo se utilizarán la función calibrate del paquete survey. En primer lugar, para poder calibrar se requiere construir la información poblacional a la cual se desea calibrar. En este ejemplo se calibrará a nivel de zona y sexo. Por tanto, los totales se obtienen como sigue: library(survey) totales &lt;- colSums( model.matrix(~ -1 + Zone:Sex, BigCity)) En la salida anterior se puede observar que, por ejemplo, en la zona rural hay 37238 mujeres mientras que en la urbana hay 41952. De igual manera se puede leer para el caso de los hombres. Una vez obtenido estos totales, se procede a utilizar la función calibrate para calibrar los pesos de muestreo como sigue: diseno_cal &lt;- calibrate( diseno, ~ -1 + Zone:Sex, totales, calfun = &quot;linear&quot;) Luego de que se hayan calibrado los pesos se puede observar que, al sumar los pesos calibrados estos reproducen el total poblacional de la base de ejemplo. sum(weights(diseno_cal)) ## [1] 150266 encuesta$wk &lt;- weights(diseno_cal) Dado que uno de los principios de los pesos calibrados es que dichos pesos no sean muy diferentes a los pesos originales que provienen del diseño de muestreo, se puede observar a continuación, la distribución de los pesos, sin calibrar y calibrados respectivamente. par(mfrow = c(1,2)) hist(encuesta$dk) hist(encuesta$wk) plot(encuesta$dk,encuesta$wk) Region &lt;- as.numeric( gsub(pattern = &quot;\\\\D&quot;, replacement = &quot;&quot;, x = encuesta$Stratum)) encuesta$Region &lt;- cut(Region, breaks = 5, labels = c(&quot;Norte&quot;,&quot;Sur&quot;,&quot;Centro&quot;,&quot;Occidente&quot;,&quot;Oriente&quot;)) encuesta %&lt;&gt;% mutate( CatAge = case_when( Age &lt;= 5 ~ &quot;0-5&quot;, Age &lt;= 15 ~ &quot;6-15&quot;, Age &lt;= 30 ~ &quot;16-30&quot;, Age &lt;= 45 ~ &quot;31-45&quot;, Age &lt;= 60 ~ &quot;46-60&quot;, TRUE ~ &quot;Más de 60&quot; ), CatAge = factor( CatAge, levels = c(&quot;0-5&quot;, &quot;6-15&quot;, &quot;16-30&quot;, &quot;31-45&quot;, &quot;46-60&quot;, &quot;Más de 60&quot;), ordered = TRUE ) ) saveRDS(object = encuesta, file = &quot;../Curso Tellez/Data/encuesta.rds&quot;) "],["manejando-una-base-de-encuestas-de-hogares-con-r.html", "Capítulo 3 Manejando una base de encuestas de hogares con R ", " Capítulo 3 Manejando una base de encuestas de hogares con R "],["fundamentos-básicos-de-r-y-rstudio.html", "3.1 Fundamentos básicos de R y Rstudio", " 3.1 Fundamentos básicos de R y Rstudio R fue creado en 1992 en Nueva Zelanda por Ross Ihaka y Robert Gentleman. A manera introductoria, R es un software diseñado para realizar análisis estadístico tanto sencillos como complejos. Este software a ganado popularidad en el gremio estadístico y no estadístico puesto que su manejo es sencillo y además, es de libre uso (Puede descargarse en https://www.r-project.org). Es decir, no requiere de ninguna licencia para su utilización. Como lo menciona Santana Sepúlveda, S., &amp; Mateos Farfán, E. (2014) R es un lenguaje de programación de libre distribución, bajo Licencia GNU, y se mantiene en un ambiente para el cómputo estadístico y gráfico. Este software está diseñado para utilizarse en distintos ambientes como, Windows, MacOS o Linux. El concepto de ambiente está enfocado en caracterizarlo como un sistema totalmente planificado y coherente, en lugar de una acumulación gradual de herramientas muy específicas y poco flexibles, como suele ser con otro software de análisis de datos. Ahora bien, como se mencionó anteriormente, R es un lenguaje de programación por ende, su interfase es poco amigable para los que inician en este lenguaje. Por esto, se creó RStudio el cual es un Entorno de Desarrollo Integrado (IDE, por sus siglas en inglés), lo que significa que RStudio es un programa que permite manejar R y utilizarlo de manera más cómoda y agradable. "],["algunas-librerías-de-interés.html", "3.2 Algunas librerías de interés", " 3.2 Algunas librerías de interés Puesto que R es un lenguaje colaborativo el cual permite que la comunidad vaya haciendo aportes al desarrollo de funciones dentro de paquetes o librerías. Alguna de las librerías más usadas para el análisis de bases de datos son las siguientes: dplyr, dplyr es la evolución del paquete plyr, enfocada en herramientas para trabajar con marcos de datos (de ahí la d en el nombre). Según Hadley Wickham, las siguientes son las tres propiedades principales de la librería: Identificar las herramientas de manipulación de datos más importantes necesarias para el análisis de datos y hacerlas fáciles de usar desde R. Proporcionar un rendimiento ultrarrápido para los datos en memoria escribiendo piezas clave en C++. Utilizar la misma interfaz para trabajar con datos sin importar dónde estén almacenados, ya sea en un marco de datos, una tabla de datos o una base de datos.Esta librería permite manejar eficientemente las bases de datos. tidyverse, es una colección de paquetes disponibles en R y orientados a la manipulación, importación, exploración y visualización de datos y que se utiliza exhaustivamente en ciencia de datos. El uso de tidyverse permite facilitar el trabajo estadístico y la generación de trabajos reproducibles. Está compuesto de los siguientes paquetes: readr, dplyr, ggplot2, tibble, tidyr, purr, stringr, forcats readstata13, este paquete permite leer y escribir todos los formatos de archivo de Stata (versión 17 y anteriores) en un marco de datos R. Se admiten las versiones de formato de archivo de datos 102 a 119. para leer las bases de datos de STATA. Además, el paquete admite muchas características del formato Stata dta, como conjuntos de etiquetas en diferentes idiomas o calendarios comerciales. survey, este paquete ha sido elaborado por el Profesor Thomas Lumley (Lumley, T. 2011) y nos proporciona funciones en R útiles para analizar datos provenientes de encuestas complejas.Alguno de los parámetros que se pueden estimar usando este paquete son medias, totales, razones, cuantiles, tablas de contingencias, modelos de regresión, modelos loglineales, entre otros. srvyr, este paquete permite utilizar el operador pipe operators en las consultas que se realizan con el paquete survey. ggplot2, es un paquete de visualización de datos para el lenguaje R que implementa lo que se conoce como la Gramática de los Gráficos, que no es más que una representación esquemática y en capas de lo que se dibuja en dichos gráficos, como lo pueden ser los marcos y los ejes, el texto de los mismos, los títulos, así como, por supuesto, los datos o la información que se grafica, el tipo de gráfico que se utiliza, los colores, los símbolos y tamaños, entre otros. TeachingSampling, este paquete permite al usuario extraer muestras probabilísticas y hacer inferencias a partir de una población finita basada en varios diseños de muestreo. Entre los diseño empleados en esta librería están: Muestreo Aleatorio Simple (MAS), Muestreo Bernoullí, Muestreo Sistemático, PiPT, PPT, estre otros. samplesize4surveys, este paquete permite calcular el tamaño de muestra requerido para la estimación de totales, medias y proporciones bajo diseños de muestreo complejos. Antes de poder utilizar las diferentes funciones que cada librería tiene, es necesario descargarlas de antemano de la web. El comando install.packages permite realizar esta tarea. Note que algunas librerías pueden depender de otras, así que para poder utilizarlas es necesario instalar también las dependencias. install.packages(&quot;dplyr&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;readstata13&quot;) install.packages(&quot;survey&quot;) install.packages(&quot;srvyr&quot;) install.packages(&quot;ggplot2&quot;) install.packages(&quot;TeachingSampling&quot;) install.packages(&quot;samplesize4surveys&quot;) Una vez instaladas las librerías hay que informarle al software que vamos a utilizarlas con el comando library. Recuerde que es necesario haber instalado las librerías para poder utilizarlas. rm(list = ls()) library(&quot;dplyr&quot;) library(&quot;tidyverse&quot;) library(&quot;readstata13&quot;) library(&quot;survey&quot;) library(&quot;srvyr&quot;) library(&quot;ggplot2&quot;) library(&quot;TeachingSampling&quot;) library(&quot;samplesize4surveys&quot;) "],["cración-de-proyectos-en-r.html", "3.3 Cración de proyectos en R", " 3.3 Cración de proyectos en R Una vez se descargan e instalan las librerías o paquetes en R el paso siguientes es crear proyectos. Un proyecto de R se define como un archivo que contiene los archivos de origen y contenido asociados con el trabajo que se está realizando. Adicionalmente, contiene información que permite la compilación de cada archivo de R a utilizar, mantiene la información para integrarse con sistemas de control de código fuente y ayuda a organizar la aplicación en componentes lógicos. Ahora bien, por una cultura de buenas practicas de programación, se recomienda crear un proyecto en el cual se tenga disponible toda la información a trabajar. A continuación, se muestran los pasos para crear un proyecto dentro de RStrudio. Paso 1: Abrir RStudio. Paso 2: ir a file -&gt; New Project Paso 3: Tipos de proyecto. Para este ejemplo se tomará New Directory Tipos de proyectos Algo a tener en cuenta en este paso es que en New Directory RStudio brinda una variedad de opciones dependiendo las características del procesamiento que desea realizar. Ahora bien, si se cuenta con algunos código previamente desarrollados y se desea continuar con ese proyecto, se debe tomar la opción Existing Directory . Por último, Si se cuenta con cuenta en Git y se desea tener una copia de seguridad, se debe emplear la opción Version Control. Paso 4: Seleccionar el tipo de proyecto. Seleccionar el tipo de proyecto Paso 5: Diligenciar el nombre del proyecto y la carpeta de destino. Nombre de proyecto Al realizar esto pasos permite que todas rutinas creadas dentro del proyecto estén ancladas a la carpeta del proyecto. "],["lectura-de-las-bases-de-datos-y-manipulación.html", "3.4 Lectura de las bases de datos y manipulación", " 3.4 Lectura de las bases de datos y manipulación Es muy usual que al trabajar proyectos en R sea necesario importar bases de datos con información relevante para un estudio en particular. En Colombia, por ejemplo, en la Encuesta de Calidad de Vida (ECV, por sus siglas) es necesario, una vez se realiza el trabajo de campo, importar la información recolectada para poder ajustar los factores de expansión y posteriormente estimar los parámetros. Los formatos de bases de datos que R permite importar son diversos, entre ellos se tienen xlsx, csv, txt, STATA, etc. Particularmente, para la lectura de bases de datos provenientes de STATA 13 se realiza con la función read.dta13. Una vez leída la base de datos en el formato mencionado anteriormente se procede a transformar en el formato .RDS el cual es un formato más eficiente y propio de R. Para ejemplificar los procedimientos en R se utilizará la base de datos de Pesquisa Nacional por Amostra de Domicílios 2015 de Brasil la cual está en formato .dta el cual se lee en R con la función read.dta13. Posteriormente se transformará al formato .rds con la función saveRDS el cual es un formato propio de R y por último se cargar esta base. Lo pasos anteriores se realiza como sigue: Primero se carga la base en formato dta con la librería read.dta13 y se guarda en formato rds con la función saveRDS ` data1 &lt;- read.dta13(&quot;Z:/BC/BRA_2015N.dta&quot;) saveRDS(data1, &quot;../data/BRA_2015N.rds&quot;) Una vez guardada la base en nuestros archivos de trabajo, se procede a cargar la base a R con la función readRDS para poder utilizar toda la información que en ella se contiene. data2 &lt;- readRDS(&quot;Data/BRA_2015N.rds&quot;) Una vez cargada la base de datos en R ésta se puede empezar a manipular según las necesidades de cada investigador. En este sentido, una de las primeras revisiones que se realizan al cargar las bases de datos es revisar su dimensión, es decir, chequear la cantidad de filas y columnas que tenga la base. Lo anterior se puede hacer con la función nrow. Dicha función identifica el número de registros (unidades efectivamente medidas) en la base de datos y la función ncol muestra el número de variables en la base de datos. Los códigos computacionales son los siguientes: nrow(data2) ## [1] 356904 ncol(data2) ## [1] 109 Una forma resumida de revisar la cantidad de filas y columnas que tiene la base de datos es usar la función dim. Esta función nos devuelve un vector indicado en su primera componente la cantidad de fila y en su segundo la cantidad de columnas como se muestra a continuación: dim(data2) ## [1] 356904 109 Es usual que en las encuestas de hogares las bases de datos sean muy extensas, es decir, contengan una cantidad importante de variables medidas (filas) y por lo general, el tamaño de la muestra de estos estudios con grandes. Es por lo anterior que, para poder visualizar dichas bases una vez cargadas en R, es necesario hacerlo de manera externa. Esto es, abrir una pestaña diferente en R y hacer la navegación de la base como un texto plano. Lo anterior se realiza con la función View como se muestra a continuación: View(data2) Visor de bases de datos de RStudio Otro chequeo importante que se debe realizar al momento de cargar una base de datos en R es el reconocimiento de las variables que incluye. Esto se puede hacer utilizando la función names la cual identifica las variables de la base de datos. names(data2) La función names solo devuelve un vector un vector con los nombres de las variables que contiene la base. Sin embargo, si se quiere profundizar en qué información contiene cada variable, La función str muestra de manera compacta la estructura de un objeto y sus componentes. Para nuestra base se utilizaría de la siguiente manera: str(data2) Como se puede observar en la salida anterior, por ejemplo, la variable id_hogar es de tipo Entero al igual que id_pers mientras que cotiza_ee es un factor con 2 niveles. Como se observa, esta función es muy útil al momento de querer tener un panorama amplio del contenido y clase de cada variable en una base de datos, particularmente, en una encuesta de hogares en donde se tiene, por la misma estructura del estudio, muchas clases o tipos de variables medidas. "],["el-operador-pipe.html", "3.5 El operador pipe", " 3.5 El operador pipe El software estadístico R es un lenguaje de programación creado por estadísticos para estadísticos. Una de las contribuciones recientes es el desarrollo de los pipelines que permiten de una forma intuitiva generar consultas y objetos desde una base de datos. El operador pipe, %&gt;%, viene del paquete magrittr (Bache, S. et al., 2022) y está cargado automáticamente en los paquetes del Tidyverse. El objetivo del operador pipe es ayudar a escribir código de una manera que sea más fácil de leer y entender. En este sentido, el operador %&gt;% permite “encadenar” operaciones en el sentido que el resultado de una operación anterior se convierta en el input de la siguiente operación. A continuación, ejemplificaremos el uso del %&gt;% en la base de datos de Brasil haciendo un conteo del total de elementos que contiene la base de datos utilizando la función count. data2 %&gt;% count() ## n ## 1 356904 Otra operación que se puede realizar en R es re-codificar los niveles de los factores que en muchas ocasiones son necesarios en las encuestas de hogares. El siguiente código permite generar los nombres de los estados en Brasil. data2$estados &lt;- factor(data2$uf, levels = c(11:17, 21:29, 31:33, 35, 41:43, 50:53), labels = c(&quot;Rondonia&quot;, &quot;Acre&quot;, &quot;Amazonas&quot;, &quot;Roraima&quot;, &quot;Para&quot;, &quot;Amapa&quot;, &quot;Tocantins&quot;, &quot;Maranhao&quot;, &quot;Piaui&quot;, &quot;Ceara&quot;, &quot;RioGrandeNorte&quot;, &quot;Paraiba&quot;, &quot;Pernambuco&quot;, &quot;Alagoas&quot;, &quot;Sergipe&quot;, &quot;Bahia&quot;, &quot;MinasGerais&quot;, &quot;EspirituSanto&quot;, &quot;RioJaneiro&quot;, &quot;SaoPaulo&quot;, &quot;Parana&quot;, &quot;SantaCatarina&quot;, &quot;RioGrandeSur&quot;, &quot;MatoGrossoSur&quot;, &quot;MatoGrosso&quot;, &quot;Goias&quot;, &quot;DistritoFederal&quot;)) Adicionalmente, para efectos de visualización en tablas y gráficos es conviene codificar los nombres de las variables. Para este ejemplo, se codificarán de la siguiente manera: data2$deptos &lt;- factor(data2$uf, levels = c(11:17, 21:29, 31:33, 35, 41:43, 50:53), labels = c(&quot;RO&quot;, &quot;AC&quot;, &quot;AM&quot;, &quot;RR&quot;, &quot;PA&quot;, &quot;AP&quot;, &quot;TO&quot;, &quot;MA&quot;, &quot;PI&quot;, &quot;CE&quot;, &quot;RN&quot;, &quot;PB&quot;, &quot;PE&quot;, &quot;AL&quot;, &quot;SE&quot;, &quot;BA&quot;, &quot;MG&quot;, &quot;ES&quot;, &quot;RJ&quot;, &quot;SP&quot;, &quot;PR&quot;, &quot;SC&quot;, &quot;RS&quot;, &quot;MS&quot;, &quot;MT&quot;, &quot;GO&quot;, &quot;DF&quot;)) Por otro lado, existe una gama amplia de funciones que se pueden utilizar con el operador %&gt;%, A continuación, se enlistan una serie de funciones muy útiles al momento de hacer análisis con bases de datos provenientes de encuestas de hogares: filter: mantiene un criterio de filtro sobre alguna variable o mezcla de variables. select: selecciona columnas por nombres. arrange: ordena las filas de la base de datos. mutate: añade nuevas variables a la base de datos. summarise: reduce variables a valores y los presenta en una tabla. group_by: ejecuta funciones y agrupa el resultado por las variables de interés. Ejemplificando alguna de las funciones mostradas anteriormente, una de las primeras consultas que se realizan en las encuestas de hogares es saber el número de encuestas (personas) realizadas y que están contenida en la base de datos. Usando %&gt;% se realiza de la siguiente manera: data2 %&gt;% count() ## n ## 1 356904 Otro de los ejercicios que se hacen usualmente con las encuestas de hogares está relacionado con saber la cantidad de hogares que hay en el país de estudio. Una de las formas más sencillas de hacer esta revisión es usar la función filter. Las encuestas de hogares muchas veces recopilan información a nivel de viviendas, hogares y personas. Particularmente, las bases de datos que están disponibles en BADEHOG están a nivel de persona. Ahora bien, para saber la cantidad de hogares que se encuestaron basta con filtrar por hogar porque sólo hay un jefe de hogar por hogar, como se muestra a continuación: datahogar1 &lt;- data2 %&gt;% filter(parentco == 1) datahogar2 &lt;- data2 %&gt;% filter(paren_ee == &quot;Jefe&quot;) Por otro lado, si el interés ahora es filtrar la base de datos por la ubicación de la persona en el área rural y urbana se realiza de la siguiente manera: dataurbano &lt;- data2 %&gt;% filter(area_ee == &quot;Area urbana&quot;) datarural &lt;- data2 %&gt;% filter(area_ee == &quot;Area rural&quot;) En este mismo sentido, si el objetivo ahora es filtrar la base de datos por algunos ingresos particulares mensuales por personas, por ejemplo, altos o bajos, se realiza de la siguiente manera: dataingreso1 &lt;- data2 %&gt;% filter(ingcorte %in% c(50, 100)) dataingreso2 &lt;- data2 %&gt;% filter(ingcorte %in% c(1000, 2000)) Otra función muy útil en el análisis en encuestas de hogares es la función select la cual, como se mencionó anteriormente permite seleccionar un grupo de variables de interés a analizar. Si por ejemplo, se desea seleccionar de la base de ejemplo solo las variables identificación del hogar (id_hogar), unidades primarias de muestreo (_upm), factores de expansión (_feh) y estratos muestrales ( _estrato) se realiza de la siguiente manera: datared &lt;- data2 %&gt;% select(`id_hogar`, `_upm`, `_feh`, `_estrato`) datablue &lt;- data2 %&gt;% select(id_pers, edad, sexo, ingcorte) La función select no solo sirve para seleccionar variables de una base de datos, también se puede utilizar para eliminar algunas variables de la base de datos que ya no son de interés para el análisis o que simplemente se generaron en la manipulación de la base de datos como variables puentes para realizar algunos cálculos de interés. Por ejemplo, si se desea eliminar de la base de datos de ejemplo las variables identificación del hogar (id_hogar) e identificación de las personas (id_pers) se realiza introduciendo un signo “menos” (-) delante del nombre de la variable como sigue: datagrey &lt;- data2 %&gt;% select(-id_hogar, -id_pers) Por otro lado, si el objetivo ahora en análisis de las encuestas de hogares es ordenar las filas de la base por alguna variable en particular, se utiliza en R la función arrange para realizar esta operación. A continuación, se ejemplifica con la base de datos de ejemplo, cómo se ordena la base de acuerdo con la variable ingcorte: datadog &lt;- datablue %&gt;% arrange(ingcorte) datadog %&gt;% head() ## id_pers edad sexo ingcorte ## 1 1 38 Mujer 0 ## 2 2 12 Mujer 0 ## 3 1 26 Hombre 0 ## 4 2 29 Mujer 0 ## 5 1 50 Hombre 0 ## 6 1 53 Mujer 0 Es posible utilizar la función arrange para hacer ordenamientos más complicados. Por ejemplo, ordenar por más de una variable. A modo de ejemplo, ordenemos la base de datos datablue de acuerdo con las variables sexo y edad datablue %&gt;% arrange(sexo, edad) %&gt;% head() ## id_pers edad sexo ingcorte ## 1 6 0 Hombre 660.4400 ## 2 6 0 Hombre 162.5000 ## 3 3 0 Hombre 381.6667 ## 4 5 0 Hombre 320.0000 ## 5 6 0 Hombre 375.0000 ## 6 4 0 Hombre 1425.0000 También es posible utilizar la función arrange junto con la opción desc() para que el ordenamiento sea descendente. datablue %&gt;% arrange(desc(edad)) %&gt;% head() ## id_pers edad sexo ingcorte ## 1 2 115 Mujer 103.0000 ## 2 4 110 Mujer 1156.5300 ## 3 2 107 Hombre 415.5904 ## 4 1 107 Mujer 1754.4600 ## 5 3 105 Mujer 380.7904 ## 6 2 105 Mujer 898.3200 "],["funciones-mutate-summarise-y-group_by-en-encuestas-de-hogares.html", "3.6 Funciones mutate, summarise y group_by en encuestas de hogares", " 3.6 Funciones mutate, summarise y group_by en encuestas de hogares Las funciones mutate, summarise y group_by están cargadas en el paquete tidyverse y son muy importantes al momento de realizar análisis en encuestas de hogares. En primer lugar, la función mutate permite computar transformaciones de variables en una base de datos. Usualmente, en las encuestas de hogares es necesario crear nuevas variables, por ejemplo, si el hogar está en estado de pobreza extrema o no la cual se calcula a partir de los ingresos del hogar, la función mutate proporciona una interface clara para realizar este tipo de operaciones. A modo de ejemplo, utilizaremos la base de ejemplo para crear una nueva variable llamada ingreso2 la cual es el doble de los ingresos por persona dentro de un hogar. Los códigos computacionales se muestran a continuación: datablue2 &lt;- datablue %&gt;% mutate(ingreso2 = 2 * ingcorte) datablue2 %&gt;% head() ## id_pers edad sexo ingcorte ingreso2 ## 1 1 23 Hombre 800.0 1600.0 ## 2 1 23 Mujer 1150.0 2300.0 ## 3 1 35 Mujer 904.4 1808.8 ## 4 2 34 Hombre 904.4 1808.8 ## 5 3 11 Mujer 904.4 1808.8 ## 6 4 7 Mujer 904.4 1808.8 No solo se puede crear una nueva variable, si es necesario, se pueden crear más de una variable en la base de datos. Cabe recalcar que la función mutate reconoce sistemáticamente las variables que van siendo creadas de manera ordenada. A continuación, se presenta cómo crear más de una nueva variable en la base de datos: datacat &lt;- datablue %&gt;% mutate(ingreso2 = 2 * ingcorte, ingreso4 = 2 * ingreso2) datacat %&gt;% head() ## id_pers edad sexo ingcorte ingreso2 ingreso4 ## 1 1 23 Hombre 800.0 1600.0 3200.0 ## 2 1 23 Mujer 1150.0 2300.0 4600.0 ## 3 1 35 Mujer 904.4 1808.8 3617.6 ## 4 2 34 Hombre 904.4 1808.8 3617.6 ## 5 3 11 Mujer 904.4 1808.8 3617.6 ## 6 4 7 Mujer 904.4 1808.8 3617.6 Ahora bien, la función summarise funciona de forma similar a la función mutate, excepto que en lugar de añadir nuevas columnas crea un nuevo data frame. Como se mencionó anteriormente esta función sirve para resumir o “colapsar filas”. Toma un grupo de valores como input y devuelve un solo valor; por ejemplo, hallar la media de los ingresos, percentiles o medidas de dispersión. Por otro lado, la función group_by permite agrupar información de acuerdo con una(s) variable(s) de interés. El siguiente código permite generar el número de encuestas efectivas en cada uno de los estados de Brasil. El comando group_by agrupa los datos por estados, el comando summarise hace los cálculos requeridos y el comando arrange ordena los resultados data2 %&gt;% group_by(estados) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) %&gt;% head() ## # A tibble: 6 × 2 ## estados n ## &lt;fct&gt; &lt;int&gt; ## 1 SaoPaulo 40008 ## 2 MinasGerais 32933 ## 3 RioGrandeSur 26259 ## 4 Bahia 26155 ## 5 RioJaneiro 25858 ## 6 Para 22489 Hay otro tipos de análisis que se quieren realizar en encuestas de hogares, por ejemplo, generar el número de encuestas efectivas discriminado por el sexo del respondiente. A continuación, se presenta el código computacional: data2 %&gt;% group_by(sexo) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## # A tibble: 2 × 2 ## sexo n ## &lt;fct&gt; &lt;int&gt; ## 1 Mujer 183681 ## 2 Hombre 173223 Si ahora se desea realizar la consulta del número de encuestas efectivas por área geográfica, se realiza de la siguiente manera: data2 %&gt;% group_by(area_ee) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## # A tibble: 2 × 2 ## area_ee n ## &lt;fct&gt; &lt;int&gt; ## 1 Area urbana 304564 ## 2 Area rural 52340 Otras consultas que se realizan de manera frecuente en encuestas de hogares es reporta el número efectivo de encuestas clasificado por parentezco (jefe de hogar, hijos, conyugues, etc) data2 %&gt;% group_by(paren_ee) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## # A tibble: 6 × 2 ## paren_ee n ## &lt;fct&gt; &lt;int&gt; ## 1 Hijos 126206 ## 2 Jefe 117939 ## 3 Cónyuge 73725 ## 4 Otros parientes 36508 ## 5 Otros no parientes 2342 ## 6 Servicio doméstico 184 "],["medidas-descriptivos-y-reflexiones.html", "3.7 Medidas descriptivos y reflexiones", " 3.7 Medidas descriptivos y reflexiones En estadística, según Tellez Piñerez, C. F., &amp; Lemus Polanía, D. F. (2015) las medidas descriptivas permiten la presentación y caracterización de un conjunto de datos con el fin de poder describir apropiadamente las diversas características presentes en la información de la muestra. Involucra cualquier labor o actividad para resumir y describir los datos univariados o multivariados sin tratar de hacer inferencia más allá de los mismos. Este tipo de análisis son primordiales en cualquier encuesta de hogares dado que, permiten tener una idea inicial del comportamiento de la población en ciertas variables de estudio. A continuación, se presentan las funciones básicas en R para realizar análisis descriptivo. Media: mean() Mediana: median() Varianza: var() Desviación estándar: sd() Percentiles: quantile() Algunas medidas descriptivas: summary() Covarianza: cov( , ) Correlación: cor( , ) Ahora bien, para continuar con lo análisis de las encuestas de hogares es necesario que el lector tenga claro algunos conceptos básicos en el muestreo probabilístico. A continuación, se dan unas definiciones básicas: ¿Qué es una encuesta? Según Groves, R. M., et al (2011) una encuesta es un método sistemático para recopilar información de una muestra de elementos con el propósito de construir descriptores cuantitativos de los parámetros de la población. ¿Qué es una muestra? La definición más básica de una muestra es un subconjunto de la población. Esta definición es muy general dado que, no es específico de si la muestra es representativa de una población o no. ¿Qué es una muestra representativa? Según Gutiérrez (2016) una muestra representativa es un modelo reducido de la población y de aquí se desprende un argumento de validez sobre la muestra. En pocas palabras, se desea que la muestra representativa tenga la cantidad de información suficiente para poder hacer una inferencia adecuada a la población. ¿Está bien sacar conclusiones sobre una muestra? Si la muestra es representativa, las conclusiones que se obtienen de la población utilizando las técnicas de muestreo adecuadas, son correctas. Sin embargo, si se toma una muestra no representativa, no es correcto realizar inferencias dado que estas no representan la realidad de la población. "],["algunas-reflexiones-generales.html", "3.8 Algunas reflexiones generales", " 3.8 Algunas reflexiones generales Como se mencionó anteriormente, antes de realizar los análisis en las encuestas de hogares es necesario hacernos algunas preguntas que nos permiten dar claridad de los análisis que se desean hacer. A continuación, se presentan las preguntas: Si calculamos el promedio de los ingresos en una encuesta, ¿qué significa esa cifra? Esta cifra representa los ingresos medios que reportaron las personas entrevistadas en el estudio. En ningún momento se puede hablar de que este valor representa a la población a la cual queremos hacer inferencia. Para poder realizar las conclusiones a nivel poblacional se deben utilizar los factores de expansión que se obtuvieron empleando el diseño muestral. Si calculamos el total de los ingresos en una encuesta, ¿qué significa esa cifra? Similar a lo anterior, significa los ingresos totales que reportaron los entrevistados en el estudio. Se recalca que, bajo ninguna circunstancia se puede inferir que este valor muestral representa a la población de estudio. ¿Qué necesitamos para que la inferencia sea precisa y exacta? Se requiere de un buen diseño muestral, que la muestra que se recolecte sea representativa de la población en estudio y que el tamaño de muestra sea suficiente para poder inferir en todas las desagregaciones, tanto geográficas como temáticas que se plantearon en el diseño muestral. ¿Qué es el principio de representatividad? La representatividad es la característica más importante de una muestra probabilística, y se define como la capacidad que tiene una muestra de poder representar a la población a la cual se desea hacer inferencia. En este sentido, el muestreo adquiere todo su sentido en cuanto se garantice que las características que se quieren medir en la población quedan reflejadas adecuadamente en la muestra. Cabe resaltar que, una muestra representativa no es aquella que se parece a la población, de tal forma que las categorías aparecen con las mismas proporciones que en la población dado que, en algunas ocasiones es fundamental sobre-representar algunas categorías o incluso seleccionar unidades con probabilidades desiguales para poderlas medir con precisión (Tillé, 2006) ¿Qué es el factor de expansión? Según Guitiérrez (2016) el factor de expansión es el número de elementos menos uno de la población (no incluidos en la muestra) representados por el elemento incluido. También se conoce como el inverso de la probabilidad de inclusión. Dadas las definiciones hechas anteriormente, una encuesta de hogares requiere el análisis de todas las variables que dispuestas en la encuesta. Este proceso debe ser llevado a cabo por separado para asegurar la calidad y consistencia de los datos recolectados. Sin embargo, no vamos a adentrarnos en el análisis de las variables en la muestra, porque los datos muestrales no son de interés para el investigador. El interés se centra en lo que suceda a nivel poblacional y este análisis se debe abordar desde la teoría del muestreo. "],["observación-importante.html", "3.9 ¡Observación importante!", " 3.9 ¡Observación importante! Los siguientes resultados no tienen interpretación poblacional y se realizan con el único propósito de ilustrar el manejo de las bases de datos de las encuestas. "],["medias-y-totales.html", "3.10 Medias y totales", " 3.10 Medias y totales La función summarise permite conocer el total de los ingresos en la base de datos y la media de los ingresos sobre los respondientes. data2 %&gt;% summarise(total.ing = sum(ingcorte), media.ing = mean(ingcorte)) ## total.ing media.ing ## 1 422286293 1183.193 También se puede calcular medias de manera agrupada. Particularmente, si se desea calcular la media de los ingresos por área se hace de la siguiente manera: data2 %&gt;% group_by(area_ee) %&gt;% summarise(n = n(), media = mean(ingcorte)) ## # A tibble: 2 × 3 ## area_ee n media ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Area urbana 304564 1278. ## 2 Area rural 52340 634. Si ahora el análisis de los ingresos se desea hacer por sexo se realiza de la siguiente manera: data2 %&gt;% group_by(sexo) %&gt;% summarise(n = n(), media = mean(ingcorte)) ## # A tibble: 2 × 3 ## sexo n media ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Hombre 173223 1192. ## 2 Mujer 183681 1174. "],["medianas-y-percentiles.html", "3.11 Medianas y percentiles", " 3.11 Medianas y percentiles La función summarise también permite conocer algunas medidas de localización de los ingresos en la base de datos. data2 %&gt;% summarise(mediana = median(ingcorte), decil1 = quantile(ingcorte, 0.1), decil9 = quantile(ingcorte, 0.9), rangodecil = decil9 - decil1) ## mediana decil1 decil9 rangodecil ## 1 732.8571 244.8872 2308.5 2063.613 "],["varianza-desviación-estándar-y-rangos.html", "3.12 Varianza, desviación estándar y rangos", " 3.12 Varianza, desviación estándar y rangos Utilizando la función summarise podemos conocer también el comportamiento variacional de los ingresos sobre los respondientes. data2 %&gt;% summarise(varianza = var(ingcorte), desv = sd(ingcorte)) ## varianza desv ## 1 3407496 1845.94 data2 %&gt;% summarise(mini = min(ingcorte), maxi = max(ingcorte), rango = maxi - mini, rangoiq = IQR(ingcorte)) ## mini maxi rango rangoiq ## 1 0 171000 171000 869.8312 Ahora bien, si se desea realizar el cálculo de la media, la desviación estándar y el rango de los ingresos por hogares, se realiza de la siguiente manera: data2 %&gt;% filter(paren_ee == &quot;Jefe&quot;) %&gt;% group_by(sexoj) %&gt;% summarise(n = n(), media = mean(ingcorte), desv = sd(ingcorte), rangoiq = IQR(ingcorte)) ## # A tibble: 2 × 5 ## sexoj n media desv rangoiq ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jefe hombre 70154 1456. 2325. 1026. ## 2 Jefa mujer 47785 1334. 2076. 943. y por condicción de ocupación se realizaría: data2 %&gt;% group_by(condact) %&gt;% summarise(n = n(), media = mean(ingcorte), desv = sd(ingcorte), rangoiq = IQR(ingcorte)) ## # A tibble: 4 × 5 ## condact n media desv rangoiq ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1 22937 764. 1136. 524. ## 2 1 165325 1458. 2191. 1028. ## 3 2 17896 695. 949. 497. ## 4 3 150746 1003. 1527. 706. a nivel de hogar: data2 %&gt;% filter(paren_ee == &quot;Jefe&quot;) %&gt;% group_by(condact) %&gt;% summarise(n = n(), media = mean(ingcorte), desv = sd(ingcorte), rangoiq = IQR(ingcorte)) ## # A tibble: 3 × 5 ## condact n media desv rangoiq ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 77852 1526. 2459. 1096. ## 2 2 4469 535. 778. 441. ## 3 3 35618 1256. 1730. 880. Si se desea hacer un descriptivo a nivel de hogar para el ingreso se realizaría de la siguiente manera: data2 %&gt;% filter(paren_ee == &quot;Jefe&quot;) %&gt;% group_by(pobreza) %&gt;% summarise(n = n(), media = mean(ingcorte), desv = sd(ingcorte), rangoiq = IQR(ingcorte)) ## # A tibble: 3 × 5 ## pobreza n media desv rangoiq ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Pobreza extrema 3918 79.9 52.7 88.9 ## 2 Pobreza no extrema 13688 269. 62.5 107. ## 3 Fuera de la pobreza 100333 1614. 2355. 1055. "],["análisis-de-las-variables-continuas-en-encuestas-de-hogares.html", "Capítulo 4 Análisis de las variables continuas en encuestas de hogares", " Capítulo 4 Análisis de las variables continuas en encuestas de hogares Los desarrollos estadísticos están en permanente evolución, surgiendo nuevas metodologías y desarrollando nuevos enfoques en el análisis de encuestas. Estos desarrollos parten de la academia, luego son adoptados por las empresas (privadas o estatales) y entidades estatales, las cuales crean la necesidad que estos desarrollos sean incluidos en software estadísticos licenciados, proceso que puede llevar mucho tiempo. Algunos investigadores para acortar los tiempos y poner al servicio de la comunidad sus descubrimientos y desarrollos, hacen la implementación de sus metodologías en paquetes estadísticos de código abierto como R o Python. Teniendo R un mayor número de desarrollos en el procesamiento de las encuestas. Como se ha venido mencionando anteriormente, dentro del software R se disponen de múltiples librerías para el procesamiento de encuestas, estas varían dependiendo del enfoque de programación desarrollado por el autor o la necesidad que se busque suplir. Como es el objetivo de este libro y como se ha venido trabajando en los capítulos anteriores nos centraremos en las libreria survey y srvyr. Se incluiran más librerías de acuerdo a las necesidades que se presenten. "],["lectura-de-bases-de-datos-y-definición-del-diseño-muestral.html", "4.1 Lectura de bases de datos y definición del diseño muestral", " 4.1 Lectura de bases de datos y definición del diseño muestral Las bases de datos (tablas de datos) pueden estar disponibles en una variedad de formatos (.xlsx, .dat, .cvs, .sav, .txt, etc.), sin embargo, por experiencia es recomendable realizar la lectura de cualesquiera de estos formatos y proceder inmediatamente a guardarlo en un archivo de extensión .rds, la cual es nativa de R. las extensiones rds permiten almacenar cualquier objeto o información en R como pueden ser marco de datos, vectores, matrices, lista, entre otros. Los archivos .rds se carcaterizan por su flexibilidad a la hora de almacenarlos, sin limitarse a su base de datos, y por su perfecta compatibilidad con R. Por otro lado, existe otro tipos de archivos propios de R como lo es .Rdata. Sin embargo existen diferencia entre ellos. Por ejemplo, mientras que los archivos .rds pueden contener cualquier número de objetos, los .Rdata se limitan a un solo objeto. Es por lo anterior que, se recomeinda trabajar con archivos .rds. Para ejemplifcar las sintaxis que se utilizarán en R, se tomará la misma base del capítulo anterior la cual contiene una muestra de 2427 registro y proviene de un muestreo complejo. A continuación, se muestra la sintaxis en R de cómo cargar un archivo con extensión .rsd library(tidyverse) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) head(encuesta) ## HHID Stratum NIh nIh dI PersonID PSU Zone Sex Age MaritalST ## 1 idHH00031 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Male 68 Married ## 2 idHH00031 idStrt001 9 2 4.5 idPer02 PSU0003 Rural Female 56 Married ## 3 idHH00031 idStrt001 9 2 4.5 idPer03 PSU0003 Rural Female 24 Married ## 4 idHH00031 idStrt001 9 2 4.5 idPer04 PSU0003 Rural Male 26 Married ## 5 idHH00031 idStrt001 9 2 4.5 idPer05 PSU0003 Rural Female 3 &lt;NA&gt; ## 6 idHH00041 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Female 61 Widowed ## Income Expenditure Employment Poverty dki dk wk Region CatAge ## 1 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte Más de 60 ## 2 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 46-60 ## 3 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 16-30 ## 4 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte 16-30 ## 5 409.87 346.34 &lt;NA&gt; NotPoor 8 36 33.63761 Norte 0-5 ## 6 823.75 392.24 Employed NotPoor 8 36 33.63761 Norte Más de 60 Una vez caraga la muestra de hogares en R, el siguiente paso es definir el diseño muestral del cual proviene dicha muestra. Para esto se utilizará el paquete srvyr el cual, como se definió anteriormente, surge como un complemento para survey. Estas librerías permiten definir objetos tipo survey.design a los que se aplican las funciones de estimación y análisis de encuestas cargadas en el paquete srvyr complementados con la programación de tubería ( %&gt;% ) del paquete tidyverse. A manera de ejemplificar los conceptos mencionados anteriormente, se definirá en R el diseño de muestreo del cual proviene la muestra contenida en el objeto encuesta: options(survey.lonely.psu = &quot;adjust&quot;) library(srvyr) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T) En el código anterior se puede observar que, en primera instancia se debe definir la base de datos en la cual se encuentra la muestra seleccionada. Seguido de eso, se debe definir el tipo de objeto en R con el cual se trabajará, para nuestro caso, será un objeto survey_design el cual se define usando la función as_survey_design. ahora bien, una vez definido el tipo de objeto se procede a definir los parámetros del diseño definido. Para este caso fue un diseño de muestreo estratificado y en varias etapas. Estos argumentos se definen dentro de la función as_survey_design como sigue. Para definir los estratos de utiliza el argumento strata y se define en qué columna están los estratos en mi base de datos. Ahora bien, para definir las UPM´s, en el argumento ids se definen la columna donde se encuntran los conglomerados seleccionados en la primera etapa. También, se definen los pesos de muestreo en el argumento weights y, por último, con el argumento nest=T se define que las UPM´s están dentro de los estratos. "],["análisis-gráfico-histogramas-y-boxplot.html", "4.2 Análisis gráfico: Histogramas y Boxplot", " 4.2 Análisis gráfico: Histogramas y Boxplot Una vez cargada la muestra a R y definido el diseño muestral del cual proviene se pueden hacer los primeros análisis. Como es natural, se inician con análisis gráficos. A continuación, se muestran los códigos computacionales con los cuales se hacen histogramas en R para la variable ingresos teniendo en cuenta el diseño muestral y los factores de expansión haciendo uso la función svyhist de la librería survey. library(survey) library(srvyr) svyhist( ~ Income , diseno, main = &quot;Ingresos por hogar&quot;, col = &quot;grey80&quot;, xlab = &quot;Ingreso&quot;, probability = FALSE ) Como se pudo observar en el código anterior, para generar un histograma teniendo en cuenta el diseño muestral se usó la función svyhist. En primer lugar, se definió la variable a graficar, que para nuestro caso fue Income. Seguido, se define el diseño muestral utilizado en la encuesta. Luego, se definen los argumentos relacionados con la estética del gráfico como lo son: el título principal (main), el color (col) y el título horizontal (xlab). Finalmente, se define si el histograma es de frecuencias o probabilidades con el argumento probability. Para este ejemplo, se tomó la opción probability = False indicando que es un histograma de frecuencias. Una pregunta que surge de manera natural es ¿cuál es la diferencia entre los histogramas sin usar los factores de expansión y utilizándolo? A continuación, se generan 3 histogramas, en el primero se grafica la variable ingreso utilizando los factores de expansión, en el segundo se grafica la misma variable sin usar los factores de expansión y en el tercero, se hace el gráfico poblacional. library(survey) data(&quot;BigCity&quot;, package = &quot;TeachingSampling&quot;) par(mfrow = c(1,3)) svyhist(~ Income, diseno, main = &quot;Ponderado&quot;, col = &quot;green&quot;, breaks = 50) hist( encuesta$Income, main = &quot;Sin ponderar&quot;, col = &quot;red&quot;, prob = TRUE, breaks = 50) hist(BigCity$Income, main = &quot;Poblacional&quot;, col = &quot;purple&quot;, prob = TRUE, xlim = c(0, 2500), breaks = 500) Uno de los análisis gráficos más comunes que se realizan ene encuestas de hogares están relacionados con subgrupos geográficos como lo son las zonas (urbano - rural) o también realizar desagregaciones temáticas como lo son por sexo (hombre mujer). A continuación, se muestra la sintaxis en R como se realizan histogramas para hombres y mujeres mayores de 18 años: sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) par(mfrow = c(1,2)) svyhist( ~ Income , design = subset(sub_Mujer, Age &gt;= 18), main = &quot;Mujer&quot;, breaks = 30, col = &quot;grey80&quot;, xlab = &quot;Ingreso&quot;) svyhist( ~ Income , design = subset(sub_Hombre, Age &gt;= 18), main = &quot;Hombre&quot;, breaks = 30, col = &quot;grey80&quot;, xlab = &quot;Ingreso&quot;) Como se puede observar, los argumentos utilizando para realizar los gráficos son los mismo que se utilizaron y ejemplificaron anteriormente. Cabe notar que la función subset permite hacer un subconjunto de la población, que para nuetro caso son aquellos hombres y mujeres mayores o iguales a 18 años. Si el objetivo ahora es realizar análisis de localización y variablidad, por ejemplo, graficar Bloxplot teniendo en cuenta los factores de expansión, a continuación, se muestran las sintaxis de como realizarlo en R. sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) par(mfrow = c(1,2)) svyboxplot( Income~1 , sub_Urbano, col = &quot;grey80&quot;, ylab = &quot;Ingreso&quot;, xlab = &quot;Urbano&quot;) svyboxplot( Income ~ 1 , sub_Rural, col = &quot;grey80&quot;, ylab = &quot;Ingreso&quot;, xlab = &quot;Rural&quot;) Los argumentos usados en la función svyboxplot para generar el gráfico son muy similares a los usados en la función svyhist. Algo a recalcar el los argumentos de esta función es que el símbolo “Income ~ 1” hace referencia a que todas las personas pertenecen a un solo grupo que puede ser urbano o rural dependiendo del caso y por eso se requiere indicarle a R esa restricción, lo cual se hace con el símbolo “~1”. "],["estimación-puntual.html", "4.3 Estimación puntual", " 4.3 Estimación puntual Después de realizar el análisis gráfico de las tendencias de las variables continuas de la encuesta, es necesario obtener las estimaciones puntuales de los parámetros que se midieron. Dichas estimaciones se obtienen de forma general o desagregado por niveles de acuerdo con las necesidades de la investigación. Entiéndase como estimaciones puntuales en el contexto de las encuestas de hogares a la estimación de totales, pormedios, razones, etc. Como lo menciona Heeringa, et al (2017) la estimación del total o promedio de una población y su varianza muestral ha jugado un papel muy importante en el desarrollo de la teoría del muestreo probabilístico, particularmente en las encuestas de hogares dado que, permiten dar un valor muy acertado de lo que puede estar pasando en los hogares estudiados y con ello tomar decisiones de política publica de manera informada. 4.3.1 Estimación de totales e intervalos de confianza Una vez definido el diseño muestral, lo cual se hizo en la sección anterior), se procede a realizar los procesos de estimación de los parámetros de interés. Para efectos de este texto se iniciará con la estimación del total de los ingresos de los hogares. En su mayoría, los paquetes estadístico actuales no utilizan técnicas avanzadas para estimar los totales de la población, por ejemplo, usar estimadores generales de regresión (GREG) o métodos de calibración. Sin embargo, Valiente et al. (2000) realizó una librería implementada en S-plus que premite realizar estos procedimientos de estimación, que se pueden escribir de manera sencilla en códigos en R (Valliant et al., 2013). Para la estimación de totales con diseños muestrales complejos que incluya estratificación \\(\\left(h=1,2,...,H\\right)\\)y muestreo por conglomerados (cuyos conglomerados están dentro del estrato \\(h\\)) indexados por \\(\\alpha=1,2,...,a_{h}\\), el estimador para el total se puede escribir como: \\[\\begin{eqnarray*} \\hat{Y}_{\\omega} &amp; = &amp; \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}y_{h\\alpha i}. \\end{eqnarray*}\\] El estimador insesgado de la varianza para este estimador es: \\[\\begin{eqnarray*} var\\left(\\hat{Y}_{\\omega}\\right) &amp; = &amp; \\sum_{h=1}^{H}\\frac{a_{h}}{\\left(a_{h}-1\\right)}\\left[\\sum_{\\alpha=1}^{a_{h}}\\left(\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}y_{h\\alpha i}\\right)^{2}-\\frac{\\left({ \\sum_{\\alpha=1}^{a_{h}}}\\omega_{h\\alpha i}y_{h\\alpha i}\\right)^{2}}{a_{h}}\\right] \\end{eqnarray*}\\] Como se puede observar, calcular la estimación del total y su varianza estimada es complejo. Sin embargo, dichos cálculos se pueden hacer en R mediante la función svytotal y el intervalos de confianza se calcula con la función confint, ambos usando la librería survey. A continuación, se muestran los códigos: total_Ingresos&lt;- svytotal(~Income, diseno, deff=T, ) total_Ingresos ## total SE DEff ## Income 85793667 4778674 11 confint(total_Ingresos, level = 0.95) ## 2.5 % 97.5 % ## Income 76427637 95159697 Los argumentos que utiliza de la función svytotal con muy sencillos. Para el ejemplo, se le introduce primero la variable en la cual está la información que se desea estimar (Income). Posterior a esto, se introduce el diseño muestral del cual proviene la muestra y, por último, se indica si desea que se reporte el deff de la estimación o no. Por otro lado, para el cálculo del intervalo de confianza, lo único que requiere es indicarle a la función confint el estimador y la confianza requerida. Paras seguir ilustrando el uso de la función svytotal y de confint, estimemos el total de gastos de los hogares, pero ahora el intervalo de confianza se calculará al 90% de confianza. Los siguientes códigos realizan las estimaciones: total_gastos&lt;- svytotal (~Expenditure, diseno, deff=T) total_gastos ## total SE DEff ## Expenditure 55677504 2604138 10.222 confint(total_gastos, level = 0.9) ## 5 % 95 % ## Expenditure 51394077 59960931 Si el objetivo ahora es estimar el total de los ingreso de los hogares pero discriminado por sexo, se utilizará ahora la función cascadede la libraría srvyr, la cual permite agregar la suma de las categorías al final la tabla. También se utilizará la función group_by la cual permite obtener resultados agrupados por los niveles de interés. diseno %&gt;% group_by(Sex) %&gt;% cascade(Total = survey_total( Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;Total ingreso&quot;) ## # A tibble: 3 × 5 ## Sex Total Total_se Total_low Total_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 44153820. 2324452. 39551172. 48756467. ## 2 Male 41639847. 2870194. 35956576. 47323118. ## 3 Total ingreso 85793667. 4778674. 76331414. 95255920. Como se pudo observar en lo códigos anteriores, otra forma de obtener las estimaciones del total, su desviación estándar y el intervalo de confianza es usando el argumento vartype e indicándole las opciones “se”, “ci” respectivamente. 4.3.2 Estimación de la media e intervalo de confianza La estimación de la media poblacional es un parámetro muy importante en las encuestas de hogares, dado que, por ejemplo, uno de los indicadores trazadores en este tipo de encuestas son los ingresos medios por hogar. Además, este tipo de parámetros no permiten describir y analizar las tendencias centrales de estas variables en poblaciones de interés. Según Gutiérrez (2016) un estimador de la media poblacional se puede escribir como una razón no lineal de dos totales de población finitas estimados como sigue: \\[\\begin{eqnarray*} \\bar{Y}_{\\omega} &amp; = &amp; \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}y_{h\\alpha i}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}}\\\\ &amp; = &amp; \\frac{\\hat{Y}}{\\hat{N}}. \\end{eqnarray*}\\] Como una observación tenga en cuenta que, si \\(y\\) es una variable binaria, la media ponderada estima la proporción de la población. Por otro lado, como \\(\\bar{Y}_{\\omega}\\) no es una estadística lineal, no existe una fórmula cerrada para la varianza de este estimador. Es por lo anterior que, se deben recurrir a usar métodos de remuestreo o series de Taylor. Para este caso en particular, usando series de Taylor el estimador insesgado de la varianza para este estimador es: \\[\\begin{eqnarray*} var\\left(\\bar{Y}_{\\omega}\\right) &amp; \\dot{=} &amp; \\frac{var\\left(\\hat{Y}\\right)+\\bar{Y}_{\\omega}^{2}\\times var\\left(\\hat{N}\\right)-2\\times\\bar{Y}_{\\omega}\\times cov\\left(\\hat{Y},\\hat{N}\\right)}{\\hat{N}^{2}} \\end{eqnarray*}\\] Como se puede observar, el cálculo de la estimación de la varianza tiene componentes complejos de calcular de manera analítica, como la covarianza entre el total estimado y el tamaño poblacional estimado. Sin embargo, R tiene funciones que incorpora estos estimadores. A continuación, se presenta la sintaxis para hacer dichos cálculos. Media_ingresos&lt;- svymean(~Income, diseno, deff=T) Media_ingresos ## mean SE DEff ## Income 570.945 28.478 8.8211 confint(Media_ingresos, level = 0.95) ## 2.5 % 97.5 % ## Income 515.1299 626.7607 Como se puede observar, los argumentos que utiliza la función svymean para realizar la estimación de la media de los ingresos de los hogares y la desviación estándar estimada del estimador son similares a los utilizando con la función svytotal. Similarmente ocurre con el intervalo de confianza. Por otro lado, tal como se realizó con el total, a manera de ejemplo, se estima la media de los gastos en los hogares como sigue a continuación: Media_gastos&lt;- svymean (~Expenditure, diseno, deff=T) Media_gastos ## mean SE DEff ## Expenditure 370.526 13.294 6.0156 confint(Media_gastos) ## 2.5 % 97.5 % ## Expenditure 344.4697 396.5829 También se pueden realizar estimaciones de la media por subgrupos siguiendo el mismo esquema mostrado para la función svytotal. Particularmente, los gastos de los hogares discriminados por sexo es: diseno %&gt;% group_by(Sex) %&gt;% cascade( Media = survey_mean( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;El gasto medio&quot; ) %&gt;% arrange(desc(Sex)) ## # A tibble: 3 × 5 ## Sex Media Media_se Media_low Media_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Male 374. 16.1 343. 406. ## 2 Female 367. 12.3 343. 391. ## 3 El gasto medio 371. 13.3 344. 397. Por zona, diseno %&gt;% group_by(Zone) %&gt;% cascade( Media = survey_mean( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;El gasto medio&quot;)%&gt;% arrange(desc(Zone)) ## # A tibble: 3 × 5 ## Zone Media Media_se Media_low Media_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Urban 460. 22.2 416. 504. ## 2 Rural 274. 10.3 254. 294. ## 3 El gasto medio 371. 13.3 344. 397. Por sexo y zona, diseno %&gt;% group_by(Zone, Sex) %&gt;% cascade( Media = survey_mean( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;El gasto medio&quot;) %&gt;% arrange(desc(Zone), desc(Sex)) %&gt;% data.frame() ## Zone Sex Media Media_se Media_low Media_upp ## 1 Urban Male 469.8124 26.96068 416.4276 523.1973 ## 2 Urban Female 450.8151 20.11853 410.9784 490.6518 ## 3 Urban El gasto medio 459.6162 22.20655 415.6450 503.5874 ## 4 Rural Male 275.3018 10.24848 255.0088 295.5948 ## 5 Rural Female 272.6769 11.61470 249.6786 295.6751 ## 6 Rural El gasto medio 273.9461 10.26141 253.6275 294.2647 ## 7 El gasto medio El gasto medio 370.5263 13.29444 344.2020 396.8506 4.3.3 Estimación de medidas de dispersión y localización En las encuestas de hogares siempre es necesario estimar medidas de dispersión de las variables estudiadas. Esto con el fin de, por ejemplo, ver qué tan disímiles son los ingresos medios de los hogares en un país determinado y con esto poder tomar acciones de política pública. Por lo anterior, es importante estudiar este parámetro en este texto. A continuación, se presenta el estimador de la desviación estándar: \\[\\begin{eqnarray} s\\left(y\\right){}_{\\omega} &amp; = &amp; \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-\\bar{Y}_{\\omega}\\right)^{2}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}-1} \\end{eqnarray}\\] Para llevar a cabo la estimación en R de la desviación estándar en encuestas de hogares, se utilizan la función survey_var la cual se ejemplifica a continuación: (sd_Est &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise(Sd = sqrt( survey_var( Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), ) ))) ## # A tibble: 2 × 5 ## Zone Sd Sd_se Sd_low Sd_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 310. 117. 263. 352. ## 2 Urban 582. 285. 422. 707. Como se pudo ver en el ejemplo anterior, se estimó la desviación estándar de los ingresos por zona reportando el error estándar en la estimación y un intervalo de confianza al 95%. Los argumentos que utiliza la función survey_var son similares a los usados en las funciones anteriores para estimar medias y totales. Si el interés ahora se centra en estimar la desviación estándar clasificando por sexo y zona, los códigos computacionales son los siguientes: (sd_Est &lt;- diseno %&gt;% group_by(Zone, Sex) %&gt;% summarise(Sd = sqrt( survey_var( Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), ) ))) %&gt;% data.frame() ## Zone Sex Sd Sd_se Sd_low Sd_upp ## 1 Rural Female 294.8683 111.6203 249.5537 334.0921 ## 2 Rural Male 325.7584 124.9643 274.2209 370.1890 ## 3 Urban Female 568.3920 286.4585 400.7312 696.8166 ## 4 Urban Male 596.7756 288.9435 436.8362 722.1194 Las medidas de posición no central (Percentiles) se diseñaron con el fin de conocer otros puntos característicos de la distribución de los datos que no son los valores centrales. Entre las medidas de posición no central más importantes están la mediana, cuartiles y percentiles. En la mayoría de las encuestas de hogares no solo estiman totales, medias y proporciones. En algunos indicadores es necesario estimar otros parámetros, por ejemplo, medianas y percentiles. Como lo menciona Tellez et al (2015) la mediana una medida de tendencia central la cual, a diferencia del promedio, no es fácilmente influenciada por datos atípicos y, por esto, se conoce como una medida robusta. La mediana es el valor que divide la población en dos partes iguales. Lo que implica que, la mitad de las observaciones de la característica de interés está por encima de la media y la otra mitad está por debajo. Por otro lado, la estimación de percentiles de ingresos en un país determinado puede definir el inicio de una política pública. por ejemplo, poner a tributar aquellas personas naturales que son el 10% más alto de la distribución de los ingresos o por el contrario, generar subsidios de transporte a aquellas familias que están en el 15% inferior de la distribución de los ingresos. La estimación de cuantiles (Loomis et al., 2005) se basa en los resultados relacionados con el estimador ponderado para totales, empleando una estimación de la función de distribución (CDF, por sus siglas en inglés) acumulada de la población. Específicamente, la CDF para una variable y en una población finita dada de tamaño \\(N\\) se define de la siguiente manera: \\[\\begin{eqnarray*} F\\left(x\\right) &amp; = &amp; \\frac{{ \\sum_{i=1}^{N}}I\\left(y_{i}\\leq x\\right)}{N} \\end{eqnarray*}\\] Donde, \\(I\\left(y_{i}\\leq x\\right)\\) es una variable indicadora la cual es igual a 1 si \\(y_{i}\\) es menor o igual a un valor específico \\(x\\), 0 en otro caso. Un estimador de la CDF en un diseño complejo (encuesta de hogares) de tamaño \\(n\\) está dado por: \\[\\begin{eqnarray*} \\hat{F}\\left(x\\right) &amp; = &amp; \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}I\\left(y_{i}\\leq x\\right)}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}} \\end{eqnarray*}\\] Una vez estimada la CDF utilizando los pesos del diseño muestral, el cuantil q-ésimo de una variable \\(y\\) es el valor más pequeño de \\(y\\) tal que la CDF de la población es mayor o igual que \\(q\\). Como es bien sabido, la mediana es aquel valor donde la CDF es mayor o igual a 0.5 y, por tanto, la media estimada es aquel valor donde la estimación de CDF es mayor o igual a 0.5. Siguiendo las recomendaciones de Heeringa et al (2017) para estimar cuantiles, primero se considera las estadísticas de orden que se denotan como \\(y_{1},\\ldots,y_{n}\\), y encuentra el valor de \\(j\\) \\((j=1,\\ldots,n)\\) tal que: \\[\\begin{eqnarray*} &amp; \\hat{F}\\left(y_{j}\\right)\\leq q\\leq\\hat{F}\\left(y_{j+1}\\right) \\end{eqnarray*}\\] Ahora bien, la estimación del q-ésimo cuantil \\(Y_{q}\\) en un diseño de muestreo complejo está dado por: \\[\\begin{eqnarray*} \\hat{Y}_{q} &amp; = &amp; y_{j}+\\frac{q-\\hat{F}\\left(y_{j}\\right)}{\\hat{F}\\left(y_{j+1}\\right)-\\hat{F}\\left(y_{j}\\right)}\\left(y_{j+1}-y_{j}\\right) \\end{eqnarray*}\\] Para la estimación de la varianza e intervalos de confianza de cuantiles, Kovar et al. (1988) muestra los resultados de un estudio de simulación en donde recomienda el uso de Balanced Repeated Replication (BRR) para estimarla. Los estimadores y procedimientos antes mencionados para la estimación de percentiles y sus varianzas están implementados en R. Particularmente, la estimación de la mediana se realiza usando la función survey_median. A continuación, se muestra la sintaxis de cómo calcular la mediana de los gastos, la desviación estándar y el intervalo de confianza al 95% de los hogares en la base de datos de ejemplo. diseno %&gt;% summarise(Mediana = survey_median( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), )) ## # A tibble: 1 × 4 ## Mediana Mediana_se Mediana_low Mediana_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 298. 8.83 282. 317. Como se puede observar, los argumentos de la función survey_median son similares a los del total y la media. Ahora bien, al igual que con los demás parámetros, si el objetivo ahora es estimar la mediana de los gastos de los hogares, pero esta vez discriminada por zona y también por sexo, el código computacional sería el siguiente: diseno %&gt;% group_by(Zone) %&gt;% summarise(Mediana = survey_median( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), )) ## # A tibble: 2 × 5 ## Zone Mediana Mediana_se Mediana_low Mediana_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 241. 11.0 214. 258. ## 2 Urban 381. 19.8 337. 416. diseno %&gt;% group_by(Sex) %&gt;% summarise(Mediana = survey_median( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), )) ## # A tibble: 2 × 5 ## Sex Mediana Mediana_se Mediana_low Mediana_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 300. 10.5 282. 324. ## 2 Male 297. 9.29 277. 314. Si el objetivo ahora es estimar cuantiles, por ejemplo, el cuantil 0.25 de los gastos de los hogares, se realizaría usando la función survey_quantile como sigue: diseno %&gt;% summarise( Q = survey_quantile( Expenditure, quantiles = 0.5, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), interval_type = &quot;score&quot; )) ## # A tibble: 1 × 4 ## Q_q50 Q_q50_se Q_q50_low Q_q50_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 298. 12.0 265. 312. si ahora se desea estimar el cuantil 0.25 pero discriminando por sexo y por zona se realizaría como sigue: diseno %&gt;% group_by(Sex) %&gt;% summarise( Q = survey_quantile( Expenditure, quantiles = 0.25, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), interval_type = &quot;score&quot; )) ## # A tibble: 2 × 5 ## Sex Q_q25 Q_q25_se Q_q25_low Q_q25_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 210. 14.9 169. 228. ## 2 Male 193. 10.4 163. 205. diseno %&gt;% group_by(Zone) %&gt;% summarise( Q = survey_quantile( Expenditure, quantiles = 0.25, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), interval_type = &quot;score&quot; )) ## # A tibble: 2 × 5 ## Zone Q_q25 Q_q25_se Q_q25_low Q_q25_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 160. 4.64 145. 163. ## 2 Urban 258. 9.05 256. 292. "],["estimación-del-coeficiente-de-ginni-en-encuestas-de-hogares.html", "4.4 Estimación del coeficiente de Ginni en encuestas de hogares", " 4.4 Estimación del coeficiente de Ginni en encuestas de hogares Para iniciar esta sección tengamos en cuenta la siguiente reflexión: Definir lo justo siempre será difícil y es algo a lo que quizá sea poco realista aspirar a conseguir. Sin embargo si estamos un poco más conscientes de cómo la desigualdad afecta nuestra libertad y cómo se refleja en el bienestar y calidad de vida de las personas, podremos poner en contexto una discusión que tendremos cada vez más presente en el mundo y en el país. La desigualdad en todos los aspectos es un problema más comunes en todos los países del mundo. Particularmente, la desigualdad económica es un problema que atañe a muchas instituciones internacionales como, por ejemplo, Naciones Unidas quien tiene este problema detectado en los Objetivos de Desarrollo Sostenibles (ODS, por sus siglas). Dado lo anterior, es clave poder medir la desigualdad económica de los hogares en los países y para esto, el indicador más utilizado es el coeficiente de Gini (CG). El valor del índice de Gini se encuentra entre 0 y 1. Un valor del coeficiente de Gini de \\(G = 0\\) indica perfecta igualdad en la distribución de la riqueza, con valores más grandes significa una desigualdad cada vez mayor en la distribución de la riqueza. Siguiendo la ecuación de estimación de Binder y Kovacevic (1995), un estimador del coeficiente de Gini es: \\[\\begin{eqnarray*} \\hat{G}\\left(y\\right) &amp; = &amp; \\frac{2\\times\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}^{*}\\hat{F}_{h\\alpha i}y_{h\\alpha i}-1}{\\bar{y}_{\\omega}} \\end{eqnarray*}\\] Donde, \\(\\omega_{h\\alpha i}^{*}=\\frac{\\omega_{h\\alpha i}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}}\\). \\(\\hat{F}_{h\\alpha i}=\\) La estimación de la CDF en el conglomerado \\(\\alpha\\) en el estrato \\(h\\). \\(\\bar{y}_{\\omega}=\\) La estimación del promedio. Para calcular el índice de Gini y su varianza estimada en una encuesta de hogares, R tiene cargados los procedimientos en la librería convey. A continuación, se muestra la sintaxis de cómo se realiza la estimación del índice de Gini para los hogares en la base de ejemplo de este capítulo. library(convey) diseno_gini &lt;- convey_prep(diseno) svygini( ~Income, design = diseno_gini) %&gt;% data.frame() ## gini Income ## Income 0.4132757 0.0186633 En primer lugar, se carga el diseño de muestreo con la función convey_prep. Luego, se estima el índice Gini con la función svygini. En los argumentos de esta última función se introducen la variable ingresos y el diseño muestral complejo. Por otro lado, si el interés ahora es estimar la curva de Lorenz. La cual, según Kovacevic, M. S. et. al (1997) para una distribución dada de ingresos, traza el porcentaje acumulado de la población (desplegado desde el más pobre hasta el más rico) frente a su participación en el ingreso total. El área entre la curva de Lorenz y la línea de 45 grados se conoce como el área de Lorenz. El índice de Gini es igual al doble del área de Lorenz. Una población con la curva de Lorenz más cerca de la línea de 45 grados tiene una distribución de ingresos más equitativa. Si todos los ingresos son iguales, la curva de Lorenz degenera a la línea de 45 grados. Para realizar la curva de Lorenz en R se utiliza la función svylorenz. A continuación, se muestran los códigos computacionales para realizar la curva de Lorenz para los ingresos: library(convey) svylorenz(formula = ~Income, design = diseno_gini, quantiles = seq(0,1,.05), alpha = .01 ) ## $quantiles ## 0 0.05 0.1 0.15 0.2 0.25 0.3 ## Income 0 0.006819101 0.01759645 0.03165963 0.04922299 0.06943653 0.09258712 ## 0.35 0.4 0.45 0.5 0.55 0.6 0.65 ## Income 0.1181331 0.1469261 0.1791978 0.2158231 0.2565784 0.3027002 0.3537989 ## 0.7 0.75 0.8 0.85 0.9 0.95 1 ## Income 0.4096304 0.4706565 0.5398749 0.6174169 0.7042464 0.8151774 1 ## ## $CIs ## , , Income ## ## 0 0.05 0.1 0.15 0.2 0.25 0.3 ## (lower 0 0.006189268 0.01644571 0.02973329 0.04643347 0.06564963 0.08770299 ## upper) 0 0.007448933 0.01874718 0.03358597 0.05201251 0.07322343 0.09747124 ## 0.35 0.4 0.45 0.5 0.55 0.6 0.65 ## (lower 0.1120660 0.1396359 0.1705777 0.2056539 0.2447898 0.2898807 0.3397823 ## upper) 0.1242001 0.1542163 0.1878180 0.2259923 0.2683671 0.3155197 0.3678154 ## 0.7 0.75 0.8 0.85 0.9 0.95 1 ## (lower 0.3943025 0.4542246 0.5227820 0.5991455 0.6835287 0.7983336 1 ## upper) 0.4249582 0.4870884 0.5569677 0.6356883 0.7249642 0.8320213 1 Los argumentos que requiere la función son, inicialmente, los ingresos de los hogares y el diseño muestral complejo. Adicionalmente, se definen una secuencia de probabilidades que define la suma de los cuantiles a calcular (quantiles) y por último, un número que especifica el nivel de confianza para el gráfico (alpha). "],["análisis-de-la-relación-entre-dos-variable-continuas.html", "4.5 Análisis de la relación entre dos variable continuas", " 4.5 Análisis de la relación entre dos variable continuas En muchos análisis de variables relacionadas con encuestas de hogares no solo basta con analizar el comportamiento de variables de manera individual, por ejemplo, ingresos medios de hombres y mujeres en un país sino también, analizar la diferencia entre los ingresos de los hombres y las mujeres. Esto último con el fin de ir cerrando la brecha salarial que existe. En este capítulo se estudiará la prueba de hipótesis para diferencia de medias, se darán las herramientas computacionales para estimar razones y contrastes. "],["prueba-de-hipótesis-para-la-diferencia-de-medias-en-encuestas-de-hogares.html", "4.6 Prueba de hipótesis para la diferencia de medias en encuestas de hogares", " 4.6 Prueba de hipótesis para la diferencia de medias en encuestas de hogares Es llamado prueba de hipótesis a una técnica la cual consiste en hacer una afirmación acerca del valor que el parámetro de la población bajo estudio puede tomar. Esta afirmación puede estar basada en alguna creencia o experiencia pasada que será contrastada con la evidencia que se obtengan a través de la información contenida en la muestra. Como dicha afirmación puede ser o no cierta, dos hipótesis pueden ser planteadas (antagónicas) las cuales se conocen como \\(H_{0}:\\) Hipótesis nula y \\(H_{1}:\\) Hipótesis alterna. Si se sospecha que el parámetro \\(\\theta\\) es igual a cierto valor particular \\(\\theta_{0}\\), los posibles juegos de hipótesis a contrastar son: \\[ \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta\\neq\\theta_{0} \\end{cases}\\,\\,\\, \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta&gt;\\theta_{0} \\end{cases}\\,\\,\\, \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta&lt;\\theta_{0} \\end{cases} \\] Se dirá que una de las dos hipótesis es cierta solo si la evidencia estadística, la cual es obtenida de la muestra, la apoya. El proceso por medio del cual se escoge una de las dos hipótesis es llamado Prueba de Hipótesis. En términos generales, algunos parámetros importantes en la estadística descriptivas se pueden escribir como una combinación lineal de medidas de interés. Los casos más usuales son diferencias de medias, sumas ponderadas de medias utilizadas para construir índices económicos, etc. Considere una función que es una combinación lineal de \\(j\\) estadísticas descriptivas como se muestra a continuación: \\[\\begin{eqnarray*} f\\left(\\theta_{1},\\theta_{2},...,\\theta_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\theta_{j} \\end{eqnarray*}\\] Una estimación de esta función está dada por: \\[\\begin{eqnarray*} f\\left(\\hat{\\theta}_{1},\\hat{\\theta}_{2},...,\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j} \\end{eqnarray*}\\] cuya varianza del estimador se calcula como sigue: \\[\\begin{eqnarray*} var\\left(\\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}^{2}var\\left(\\hat{\\theta}_{j}\\right)+2\\times\\sum_{j=1}^{J-1}\\sum_{k&gt;j}^{J}a_{j}a_{k}\\,cov\\left(\\hat{\\theta}_{j},\\hat{\\theta}_{k}\\right) \\end{eqnarray*}\\] Como se pudo observar en la ecuación de la varianza del estimador, esta incorpora las varianzas de las estimaciones de los componentes individuales, así como las covarianzas de las estadísticas estimadas. En primer lugar, una combinación lineal de estadísticas descriptivas de interés en este capítulo es la diferencia de media cuyo parámetro es \\({\\bar{Y}_{1}-\\bar{Y}_{2}}\\), donde, \\(\\bar{Y}_{1}\\) es la media de la población 1, por ejemplo, ingresos medios en los hogares obtenido por los padres de familia y \\(\\bar{Y}_{2}\\) es la media de la población 2, que para seguir el ejemplo serían, los ingresos medios de las madres en un hogar. Considerando el parámetro de interés en esta sección, las hipótesis a estudiar serían las siguientes: \\[\\begin{eqnarray*} \\begin{cases} H_{0}:\\bar{Y}_{1}-\\bar{Y}_{2}=0\\\\ H_{1}:\\bar{Y}_{1}-\\bar{Y}_{2}\\neq0 \\end{cases} &amp; \\begin{cases} H_{0}:\\bar{Y}_{1}-\\bar{Y}_{2}=0\\\\ H_{1}:\\bar{Y}_{1}-\\bar{Y}_{2}&gt;0 \\end{cases} &amp; \\begin{cases} H_{0}:\\bar{Y}_{1}-\\bar{Y}_{2}=0\\\\ H_{1}:\\bar{Y}_{1}-\\bar{Y}_{2}&lt;0 \\end{cases} \\end{eqnarray*}\\] Para probar estas hipótesis se utiliza el siguiente estadístico de prueba que se distribuye t-student: \\[\\begin{eqnarray*} t &amp; = &amp; \\frac{\\bar{Y}_{1}-\\bar{Y}_{2}}{se\\left(\\bar{Y}_{1}-\\bar{Y}_{2}\\right)}, \\end{eqnarray*}\\] donde, \\[\\begin{eqnarray*} se\\left(\\bar{Y}_{1}-\\bar{Y}_{2}\\right) &amp; = &amp; \\sqrt{var\\left(\\bar{y}_{1}\\right)+var\\left(\\bar{y}_{2}\\right)-2cov\\left(\\bar{y}_{1},\\bar{y}_{2}\\right)} \\end{eqnarray*}\\] Si se desea construir un intervalo de confianza para la diferencia de media se realizaría de la siguiente manera: \\[\\begin{eqnarray*} &amp; \\left(\\bar{Y}_{1}-\\bar{Y}_{2}\\right)\\pm t_{gl,\\,\\alpha/2}\\,se\\left(\\bar{Y}_{1}-\\bar{Y}_{2}\\right) \\end{eqnarray*}\\] Para poder llevar a cabo la prueba de hipótesis para la diferencia de media de los ingresos en un hogar por sexo, tomemos la base de datos que tenemos como ejemplo. La función que se encarga de realizar la prueba es svyttest y solo requiere como argumentos la variable ingreso (o variable de interés), la variable sexo (variable discriminadora), el diseño muestral y el nivel de confianza. A continuación, se muestran los códigos computacionales que se requieren: svyttest(Income ~ Sex, design = diseno, level=0.95) ## ## Design-based t-test ## ## data: Income ~ Sex ## t = 1.3625, df = 118, p-value = 0.1756 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -12.82205 69.38503 ## sample estimates: ## difference in mean ## 28.28149 En esta salida podemos observar que el p-valor de la prueba es 0.14. Si tomamos una significancia del 5% para la prueba se puede concluir que, con una confianza del 95% y basados en la muestra, no existe suficiente evidencia estadística para decir que los ingresos medios en los hogares son diferentes por sexo. Por otro lado, el intervalo de confianza al 95% para la diferencia de medias entre los ingresos de hombres y mujeres es \\(\\left(-77.35,\\,11.41\\right)\\). Si ahora el objetivo es realizar la prueba de diferencia de medias para los ingresos entre hombres y mujeres pero solo en la zona urbana, los códigos computacionales son los siguientes: svyttest(Income ~ Sex, design = sub_Urbano, level = 0.95) ## ## Design-based t-test ## ## data: Income ~ Sex ## t = 1.5667, df = 63, p-value = 0.1222 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -12.31754 101.74023 ## sample estimates: ## difference in mean ## 44.71134 En donde, al igual que el anterior, no se rechaza la hipótesis nula con una confianza del 95%. Por otro lado, la función svyttest permite usar filtro. Si se requiere probar la hipótesis de diferencia de medias de ingresos por sexo pero solo en aquellas personas del hogar mayores a 18 años, se utilizará dentro de la función svyttest la función filter como se muestra a continuación: svyttest(Income ~ Sex, design = diseno %&gt;% filter(Age &gt; 18), level = 0.95 ) ## ## Design-based t-test ## ## data: Income ~ Sex ## t = 1.5263, df = 118, p-value = 0.1296 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -10.72746 82.85253 ## sample estimates: ## difference in mean ## 36.06253 y con una confianza del 95% y basado en la muestra tampoco se rechaza la hipótesis hula. Es decir, no existe evidencia estadística para concluir que los ingresos medios entre hombres y mujeres mayores de 18 años son diferentes. "],["estimando-razones-en-encuestas-de-hogares.html", "4.7 Estimando razones en encuestas de hogares", " 4.7 Estimando razones en encuestas de hogares Un caso particular de una función no lineal de totales es la razón poblacional. Esta se define como el cociente de dos totales poblacionales de características de interés. En las encuestas de hogares, en ocasiones se requiere estimar este parámetro, por ejemplo, cantidad de hombres por cada mujer o la cantidad de mascotas por cada hogar en un país determinado. Puesto que la razón es un cociente de totales, tanto en numerador como el denominador son cantidades desconocidas y por tanto requieren estimarse (Bautista, 1998). Por definición la razón poblacional se define de la siguiente manera: \\[\\begin{eqnarray*} R &amp; = &amp; \\frac{Y}{X} \\end{eqnarray*}\\] El estimador puntual de una razón en muestreos complejos no es más que estimar los totales por separados como se define a continuación: \\[\\begin{eqnarray*} \\hat{R} &amp; = &amp; \\frac{\\hat{Y}}{\\hat{X}}\\\\ &amp; = &amp; \\frac{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i=1}^{nh\\alpha}}\\omega_{h\\alpha i}y_{h\\alpha i}}{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i=1}^{nh\\alpha}}\\omega_{h\\alpha i}x_{h\\alpha i}} \\end{eqnarray*}\\] Sin embargo, dado que estimador de la razón es un cociente entre dos estimadores, es decir, dos variables aleatorias, el cálculo de la estimación de la varianza no es sencillo de obtener. Para ellos, se debe aplicar linealización de Taylor como lo muestra Gutiérrez (2016). De manera computacional, la función survey_ratio tiene implementado los procedimientos para estimar las razones y sus varianzas. Para un correcto cálculo de la estimación de la razón y su varianza estimada se le debe introducir a la función el numerados de la razón (numerator) y el denominador (denominator). Adicional a esto, se le debe indicar el nivel de confianza de los intervalos y qué estadística de resúmenes debe calcular (vartype). A continuación, se muestran los códigos computacionales para estimar la razón entre el gasto y el ingreso. diseno %&gt;% summarise( Razon = survey_ratio( numerator = Expenditure, denominator = Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.649 0.0232 0.603 0.695 Como se puede observar, la razón entre el gasto y el ingreso es, aproximando, 0.71. Lo que implica que por cada unidad 100 unidades monetarias que le ingrese al hogar, se gastan 71 unidades, consiguiendo un intervalo de confianza al 95% de 0.65 y 0.76. Si ahora el objetivo es estimar la razón entre mujeres y hombres en la base de ejemplo, se realiza de la siguiente manera: diseno %&gt;% summarise( Razon = survey_ratio( numerator = (Sex == &quot;Female&quot;), denominator = (Sex == &quot;Male&quot;), level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.11 0.0351 1.04 1.18 Como la variable sexo en la base de datos es una variable categórica, se tuvo la necesidad de generar las variables dummys para su cálculo realizando, Sex == “Female” para el caso de las mujeres y Sex == “Male” para el caso de los hombres. Los resultados del ejercicio anterior muestran que en la base de datos hay más mujeres que hombres, generando una razón de 1.13. Esto significa que, por cada 100 hombres hay aproximadamente 113 mujeres con un intervalo que varía entre 1.04 y 1.21. Si se desea hacer la razón de mujeres y hombres pero en la zona rural, se haría de la siguiente manera: sub_Rural %&gt;% summarise( Razon = survey_ratio( numerator = (Sex == &quot;Female&quot;), denominator = (Sex == &quot;Male&quot;), level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.07 0.0352 0.997 1.14 Obteniendo nuevamente que hay más mujeres que hombres. Ahora bien, otro análisis de interés es estimar la razón de gastos pero solo en la población femenina. A continuación, se presentan los códigos computacionales. sub_Mujer %&gt;% summarise( Razon = survey_ratio( numerator = Expenditure, denominator = Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.658 0.0199 0.619 0.698 Dando como resultado que por cada 100 unidades monetarias que le ingresan a las mujeres se gastan 70 con un intervalo de confianza entre 0.65 y 0.76. Por último, análogamente para los hombres, la razón de gastos resulta muy similar que para las mujeres. sub_Hombre %&gt;% summarise( Razon = survey_ratio( numerator = Expenditure, denominator = Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.639 0.0288 0.582 0.696 "],["estimando-contrastes-en-encuestas-de-hogares.html", "4.8 Estimando contrastes en encuestas de hogares", " 4.8 Estimando contrastes en encuestas de hogares En muchas ocasiones, en encuestas de hogares se requiere comparar más de dos poblaciones al mismo tiempo, por ejemplo, comparar los ingresos medios de los hogares en 3 regiones o municipalidades en la postpandemia con el fin de verificar y sectorizar aquellas municipalidades o regiones donde más impacto en el desempleo y falta de ingresos tuvo el Covid-19 en los hogares. En casos como estos la diferencia de media que estudiamos en capítulos anteriores se queda corta dado que permite solo comprar parejas de poblaciones y por ende que, hacer contraste resulta una muy buena alternativa para abordar este tipo de problemas. Recurriendo en las definiciones que se han trabajado en este capítulo, un contraste es una combinación lineal de parámetros de la forma: \\[\\begin{eqnarray*} f\\left(\\theta_{1},\\theta_{2},...,\\theta_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\theta_{j} \\end{eqnarray*}\\] Una estimación de esta función está dada por: \\[\\begin{eqnarray*} f\\left(\\hat{\\theta}_{1},\\hat{\\theta}_{2},...,\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j} \\end{eqnarray*}\\] cuya varianza del estimador se calcula como sigue: \\[\\begin{eqnarray*} var\\left(\\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}^{2}var\\left(\\hat{\\theta}_{j}\\right)+2\\times\\sum_{j=1}^{J-1}\\sum_{k&gt;j}^{J}a_{j}a_{k}\\,cov\\left(\\hat{\\theta}_{j},\\hat{\\theta}_{k}\\right) \\end{eqnarray*}\\] Los procedimientos metodológicos para implementar los contrastes en diseños de muestreo complejos están desarrolladas en la función svycontrast. A continuación, se muestra el uso de dicha función para el cálculo de contraste en la base de datos de ejemplo, comparando el promedio de ingresos por región. Como primer ejemplo, se realizará la comparación de dos poblaciones, las regiones Norte y Sur (\\(\\bar{Y}_{Norte} - \\bar{Y}_{Sur}\\)) y luego sí se compararán todas las regiones. Puesto que esto es un contraste en donde hay 5 regiones y solo se construirá el contraste para la región Norte y la Sur, el contraste queda definido de la siguiente manera: \\[\\begin{eqnarray*} &amp; 1\\times\\hat{\\bar{Y}}_{Norte}+\\left(-1\\right)\\times\\hat{\\bar{Y}}_{Sur}+0\\times\\hat{\\bar{Y}}_{Centro}+0\\times\\hat{\\bar{Y}}_{Occidente}+0\\times\\hat{\\bar{Y}}_{Oriente} \\end{eqnarray*}\\] que de forma matricial queda de la siguiente manera: \\[\\begin{eqnarray*} &amp; \\left[1,\\,-1,\\,0,\\,0,\\,0\\right]\\times\\left[\\begin{array}{c} \\hat{\\bar{Y}}_{Norte}\\\\ \\hat{\\bar{Y}}_{Sur}\\\\ \\hat{\\bar{Y}}_{Centro}\\\\ \\hat{\\bar{Y}}_{Occidente}\\\\ \\hat{\\bar{Y}}_{Oriente} \\end{array}\\right] \\end{eqnarray*}\\] Como se puede observar, en este caso el vector de contraste es \\(\\left[1,\\,-1,\\,0,\\,0,\\,0\\right]\\). Ahora bien, para realizar el procesos de la construcción del estimador del contraste y su varianza estimada paso a paso se inicia con calcular las medias estimadas por región con la función svyby como se muestra a continuación: prom_region &lt;- svyby(formula = ~Income, by = ~Region, design = diseno, FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) prom_region ## Region Income se ci_l ci_u ## Norte Norte 552.3637 55.35987 443.8603 660.8670 ## Sur Sur 625.7740 62.40574 503.4610 748.0870 ## Centro Centro 650.7820 61.46886 530.3053 771.2588 ## Occidente Occidente 517.0071 46.22077 426.4161 607.5982 ## Oriente Oriente 541.7543 71.66487 401.2938 682.2149 La función svyby permite aplicar una función, en este caso la media (svymean) por región (by) utilizando el diseño muestral empleado (design). Las demás componentes de la función ya se han utilizado previamente. Como resultado de aplicar esta función se obtienen las medias estimadas de los ingresos por región. Se tomarán solo los ingresos medios estimados de las regiones Norte y Sur y calcularemos su diferencia: # Paso 1: diferencia de estimaciones (Norte - Sur) 552.4 - 625.8 ## [1] -73.4 El paso siguiente es calcular la matriz de varianzas y covarianzas y de allí extraer las varianzas y covarianzas de las regiones Norte y Sur: # Paso 2: Matriz de varianzas y covarianzas vcov(prom_region) ## Norte Sur Centro Occidente Oriente ## Norte 3064.715 0.000 0.00 0.000 0.000 ## Sur 0.000 3894.476 0.00 0.000 0.000 ## Centro 0.000 0.000 3778.42 0.000 0.000 ## Occidente 0.000 0.000 0.00 2136.359 0.000 ## Oriente 0.000 0.000 0.00 0.000 5135.854 Para calcular el error estándar de la diferencia (contraste) se usará las propiedades de la varianza como es \\(se\\left(\\hat{\\bar{y}}_{Norte}-\\hat{\\bar{y}}_{Sur}\\right)=\\sqrt{var\\left(\\hat{\\bar{y}}_{Norte}\\right)+var\\left(\\hat{\\bar{y}}_{Sur}\\right)-2\\,cov\\left(\\hat{\\bar{y}}_{Norte},\\hat{\\bar{y}}_{Sur}\\right)}\\) tenemos: sqrt(3065 + 3894 - 2*0) ## [1] 83.42062 Finalmente, la función svycontrast nos devuelve el contraste estimado y su error estándar. Los argumentos de esta función son los promedios de los ingresos estimados (stat) y las constantes de contraste (contrasts). svycontrast(stat = prom_region, contrasts = list(diff_NS = c(1, -1, 0, 0, 0))) %&gt;% data.frame() ## contrast diff_NS ## diff_NS -73.41034 83.42176 Obteniendo como resultado que los ingresos medios estimados para la región Sur es 73.4 unidades monetarias mayor que los ingresos en la región Norte con un error estándar de 83.42 unidades. Ahora bien, si el objetivo es estimar los siguientes contrastes: \\(\\bar{Y}_{Norte} - \\bar{Y}_{Centro}\\), \\(\\bar{Y}_{Sur}-\\bar{Y}_{Centro}\\) \\(\\bar{Y}_{Occidente}-\\bar{Y}_{Oriente}\\) Que escritas de forma matricial se tiene: \\[ \\left[\\begin{array}{ccccc} 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right] \\] Ahora, aplicando la función svycontrast en R se obtiene: svycontrast(stat = prom_region, contrasts = list( Norte_sur = c(1, 0, -1, 0, 0), Sur_centro = c(0, 1, -1, 0, 0), Occidente_Oriente = c(0, 0, 0, 1, -1))) %&gt;% data.frame() ## contrast SE ## Norte_sur -98.41834 82.72324 ## Sur_centro -25.00800 87.59507 ## Occidente_Oriente -24.74720 85.27727 De lo cual se puede concluir que, las regiones con los ingresos medios de los hogares más similares son la región sur y la región centro. También es posible construir contraste en variables que estén correlacionadas. Por ejemplo, Ingreso y Sexo. Como se hizo en el ejemplo anterior, se inicia con el promedio estimado por sexo. prom_sexo &lt;- svyby(formula = ~Income, by = ~Sex, design = diseno, FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) prom_sexo ## Sex Income se ci_l ci_u ## Female Female 557.5681 25.82995 506.9423 608.1939 ## Male Male 585.8496 34.58759 518.0592 653.6400 El contraste a estimar es: \\[ \\bar{Y}_{F} - \\bar{Y}_{M}\\] Por tanto, usando la función svycontrast se obtiene el contraste estimado: svycontrast(stat = prom_sexo, contrasts = list(diff_Sexo = c(1, -1))) %&gt;% data.frame() ## contrast diff_Sexo ## diff_Sexo -28.28149 20.75651 Obteniendo como resultado que, en promedio, los hombres obtienen 28.3 unidades monetarias más que las mujeres con una desviación de 20.76. Otra posibilidad es poder obtener resultados agregados, por ejemplo: \\(\\hat{\\bar{y}}_{Norte}+\\hat{\\bar{y}}_{Sur} +\\hat{\\bar{y}}_{Centro}\\) sum_region &lt;- svyby( ~ Income, ~ Region, diseno, svytotal, na.rm = T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) sum_region ## Region Income se ci_l ci_u ## Norte Norte 14277323 1507575 11322530 17232115 ## Sur Sur 16068151 1877989 12387359 19748942 ## Centro Centro 16483319 2383556 11811634 21155003 ## Occidente Occidente 16853540 1823807 13278944 20428135 ## Oriente Oriente 22111335 2833460 16557856 27664814 La matriz de contraste queda como: \\[ \\left[\\begin{array}{cccccc} 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\end{array}\\right] \\] el procedimiento en R es: svycontrast(stat = sum_region, contrasts = list( Agregado_NCS = c(1, 1, 1, 0, 0))) %&gt;% data.frame() ## contrast Agregado_NCS ## Agregado_NCS 46828792 3388357 Por otro lado, si se desean obtener los promedios por categorías. Por ejemplo: \\[ \\hat{\\bar{y}}_{Edad} = \\frac{1}{k}\\sum_{k=1}^K\\hat{\\bar{y}}_{k} \\] donde \\(K\\) es el número de categorías de la variable. En R se hace de la siguiente manera: prom_edad &lt;- svyby(formula = ~Income, by = ~CatAge, design = diseno, FUN = svymean, na.rm=T, covmat = TRUE) prom_edad ## CatAge Income se ## 0-5 0-5 463.7715 28.86795 ## 6-15 6-15 511.6179 34.88031 ## 16-30 16-30 607.2917 37.41561 ## 31-45 31-45 573.4167 26.94744 ## 46-60 46-60 763.0610 58.97170 ## Más de 60 Más de 60 466.6133 31.20795 Cuya matriz de contraste estaría dada por: \\[ \\left[\\begin{array}{cccccc} \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} \\end{array}\\right] \\] El procedimiento en R es: svycontrast(stat = prom_edad, contrasts = list( agregado_edad = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))) %&gt;% data.frame() ## contrast agregado_edad ## agregado_edad 564.2954 25.40408 Puesto que los contrastes, como ya se mencionó, es una función lineal de parámetros, se puede también realizar contraste con parámetros tipo razón. Por ejemplo, la relación de gastos contra ingresos por sexo. A continuación, se muestran los códigos computacionales: razon_sexo &lt;- svyby( formula = ~Income, by = ~Sex, denominator = ~Expenditure, design = diseno, FUN = svyratio, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) razon_sexo ## Sex Income/Expenditure se.Income/Expenditure ci_l ci_u ## Female Female 1.519060 0.04582607 1.429243 1.608878 ## Male Male 1.564762 0.07044239 1.426698 1.702827 Cuya estimación de contraste sería: svycontrast(stat = razon_sexo, contrasts = list( diff_sexo = c(1, -1))) %&gt;% data.frame() ## contrast diff_sexo ## diff_sexo -0.04570214 0.04163431 de lo que se puede concluir que la diferencia de las proporciones es 0.045 en favor de los hombres. "],["análisis-de-variables-categóricas-en-encuestas-de-hogares.html", "Capítulo 5 Análisis de variables categóricas en encuestas de hogares", " Capítulo 5 Análisis de variables categóricas en encuestas de hogares En ocasiones, no es sencillo distinguir entre las variables denominada cualitativos y cuantitativos puesto que, algunas variables de tipo cuantitativo pueden llegar a considerarse como categóricas si se divide el rango de valores de la variable en intervalos o categorías. Un ejemplo de esto es la variable edad, que en una encuesta de hogares se pregunta como variable cuantitativa y esta se puede dividir, por ejemplo, en Colombia, en las siguientes categorías: Adolescencia (12 - 18 años), Juventud (14 - 26 años), Adultez (27- 59 años), Persona Mayor (60 años o más), envejecimiento y vejez. Por otro lado, una variable categórica también se puede convertir en una variable cuantitativa realizando, por ejemplo, un análisis de correspondencias. Esto ocurre en muchas situaciones cuando se requiere construir índices. Por ejemplo, índice de fuerza laboral. En el contexto de encuestas, las preguntas que contienen variables categóricas son uno de los tipos de preguntas más usuales. Estas preguntas suelen representarse en resultados de porcentajes. Por ejemplo, preguntas relacionadas con parentesco, sexo, si es jefe o jefa de hogar, si la vivienda contiene agua potable, etc. library(tidyverse) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) head(encuesta) ## HHID Stratum NIh nIh dI PersonID PSU Zone Sex Age MaritalST ## 1 idHH00031 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Male 68 Married ## 2 idHH00031 idStrt001 9 2 4.5 idPer02 PSU0003 Rural Female 56 Married ## 3 idHH00031 idStrt001 9 2 4.5 idPer03 PSU0003 Rural Female 24 Married ## 4 idHH00031 idStrt001 9 2 4.5 idPer04 PSU0003 Rural Male 26 Married ## 5 idHH00031 idStrt001 9 2 4.5 idPer05 PSU0003 Rural Female 3 &lt;NA&gt; ## 6 idHH00041 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Female 61 Widowed ## Income Expenditure Employment Poverty dki dk wk Region CatAge ## 1 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte Más de 60 ## 2 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 46-60 ## 3 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 16-30 ## 4 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte 16-30 ## 5 409.87 346.34 &lt;NA&gt; NotPoor 8 36 33.63761 Norte 0-5 ## 6 823.75 392.24 Employed NotPoor 8 36 33.63761 Norte Más de 60 Definición del diseño y creación de variables categóricas Se inicia este capítulo haciendo el ajuste del diseño muestral (como se mostró en capítulos anteriores) usando como ejemplo la misma base de datos del capítulo anterior. Luego, para efectos del ejemplo, se genera una variable categórica la cual indica si la persona encuestada está en estado de pobreza o no como sigue: library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = TRUE) A continuación, se define una variable categórica que nace de variables propias de la encuesta, diseno &lt;- diseno %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0), edad_18 = case_when(Age &lt; 18 ~ &quot;&lt; 18 anios&quot;, TRUE ~ &quot;&gt;= 18 anios&quot;) ) Como se pudo observar en el código anterior, se ha introducido la función case_when la cual es una extensión del a función ifelse que permite crear múltiples categorías a partir de una o varias condiciones. Como se ha mostrado anteriormente, en ocasiones se desea realizar estimaciones por sub-grupos de la población, en este caso se extraer 4 sub-grupos de la encuesta y se definen a continuación: sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) "],["estimaciones-de-totales.html", "5.1 Estimaciones de totales", " 5.1 Estimaciones de totales En esta sección se realizarán los procesos de estimación de variables categóricas. En primera instancia se presenta cómo se estima los tamaños de la población y subpoblaciones. tamano_zona &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise( n = unweighted(n()), Nd = survey_total(vartype = c(&quot;se&quot;,&quot;ci&quot;))) tamano_zona ## # A tibble: 2 × 6 ## Zone n Nd Nd_se Nd_low Nd_upp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 1297 72102 3062. 66039. 78165. ## 2 Urban 1308 78164 2847. 72526. 83802. En la tabla anterior, n denota el número de observaciones en la muestra por Zona y Nd denota la estimación del total de observaciones en la población. Adicionalmente, en el código anterior se introdujo la función unweighted la cual, calcula resúmenes no ponderados a partir de un conjunto de datos de encuestas. Para el ejemplo, el tamaño de muestra en la zona rural fue de 1297 personas y para la urbana fue de 1308. Con esta información se logró estimar una población de 72102 con una desviación estándar de 3062.204 en la zona rural y una población de 78164 con desviación estándar de 2847.221 en la zona urbana. Así mismo, con una confianza del 95% se construyeron unos intervalos de confianza para el tamaño poblacional en la zona rural de (66038.5, 78165.4) y para la urbana de (72526.2, 83801.7). Ahora bien, empleando una sintaxis similar a la anterior es posible estimar el número de personas en condición de pobreza extrema, pobreza y no pobres como sigue: tamano_pobreza &lt;- diseno %&gt;% group_by(Poverty) %&gt;% summarise( Nd = survey_total(vartype = c(&quot;se&quot;,&quot;ci&quot;)) ) tamano_pobreza ## # A tibble: 3 × 5 ## Poverty Nd Nd_se Nd_low Nd_upp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NotPoor 91398. 4395. 82696. 100101. ## 2 Extreme 21519. 4949. 11719. 31319. ## 3 Relative 37349. 3695. 30032. 44666. De la tabla anterior podemos concluir que, la cantidad estimada de personas en estado de no pobreza son 91398.3, en pobreza 37348.9 y pobreza extrema de 21518.7. os demás parámetros estimados se interpretan de la misma manera que para la estimación desagregada por zona. En forma similar es posible estimar el número de personas debajo de la línea de pobreza. tamano_pobreza &lt;- diseno %&gt;% group_by(pobreza) %&gt;% summarise( Nd = survey_total(vartype = c(&quot;se&quot;,&quot;ci&quot;))) tamano_pobreza ## # A tibble: 2 × 5 ## pobreza Nd Nd_se Nd_low Nd_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 91398. 4395. 82696. 100101. ## 2 1 58868. 5731. 47519. 70216. Concluyendo para este ejemplo que, 58867.6 personas están por debajo de la línea de pobreza con una desviación estándar de 5731.3 y un intervalo de confianza (47518.9 70216.3). Otra variable de interés en encuestas de hogares es conocer el estado de ocupación de las personas. A continuación, se muestra el código computacional: tamano_ocupacion &lt;- diseno %&gt;% group_by(Employment) %&gt;% summarise( Nd = survey_total(vartype = c(&quot;se&quot;,&quot;ci&quot;))) tamano_ocupacion ## # A tibble: 4 × 5 ## Employment Nd Nd_se Nd_low Nd_upp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Unemployed 4635. 761. 3129. 6141. ## 2 Inactive 41465. 2163. 37183. 45748. ## 3 Employed 61877. 2540. 56847. 66907. ## 4 &lt;NA&gt; 42289. 2780. 36784. 47794. De los resultados de la estimación se puede concluir que, 4634.8 personas están desempleadas con un intervalo de confianza de (3128.6, 6140.9). 41465.2 personas están inactivas con un intervalo de confianza de (37182.6, 45747.8) y por último, 61877.0 personas empleadas con intervalos de confianza (36784.2, 47793.5). Utilizando la función group_by es posible obtener resultados por más de un nivel de agregación. A continuación, se muestra la estimación ocupación desagregada por niveles de pobreza: tamano_ocupacion_pobreza &lt;- diseno %&gt;% group_by(Employment, Poverty) %&gt;% cascade( Nd = survey_total(vartype = c(&quot;se&quot;,&quot;ci&quot;)), .fill = &quot;Total&quot;) %&gt;% data.frame() tamano_ocupacion_pobreza ## Employment Poverty Nd Nd_se Nd_low Nd_upp ## 1 Unemployed NotPoor 1768.375 405.3765 965.6891 2571.061 ## 2 Unemployed Extreme 1169.201 348.1340 479.8603 1858.541 ## 3 Unemployed Relative 1697.231 457.8077 790.7262 2603.736 ## 4 Unemployed Total 4634.807 760.6242 3128.6948 6140.919 ## 5 Inactive NotPoor 24346.008 1736.2770 20908.0064 27784.010 ## 6 Inactive Extreme 6421.825 1320.7349 3806.6383 9037.012 ## 7 Inactive Relative 10697.414 1460.2792 7805.9155 13588.913 ## 8 Inactive Total 41465.248 2162.8040 37182.6798 45747.816 ## 9 Employed NotPoor 44600.347 2596.1915 39459.6282 49741.065 ## 10 Employed Extreme 5127.531 1121.6461 2906.5601 7348.503 ## 11 Employed Relative 12149.142 1346.6159 9482.7078 14815.576 ## 12 Employed Total 61877.020 2540.0762 56847.4153 66906.624 ## 13 Total Total 150266.000 4181.3587 141986.4921 158545.508 ## 14 &lt;NA&gt; NotPoor 20683.603 1256.6158 18195.3777 23171.827 ## 15 &lt;NA&gt; Extreme 8800.209 2979.9150 2899.6792 14700.738 ## 16 &lt;NA&gt; Relative 12805.115 1551.0291 9733.9220 15876.307 ## 17 &lt;NA&gt; Total 42288.926 2779.9913 36784.2652 47793.586 De lo cual se puede concluir, entre otros que, 44600.3 personas que trabajan no son pobres con un intervalo de confianza (39459.6, 49741.0) y 6421.8 inactivas están en pobreza extrema con un intervalo de confianza de (3806.6, 9037.0). "],["estimación-de-proporciones.html", "5.2 Estimación de proporciones", " 5.2 Estimación de proporciones Otro parámetro de interés en las encuestas de hogares, particularmente con variables categóricas es la estimación de las proporciones poblacionales. En esta sección se estudiará la estimación de proporciones y sus errores estándares. En términos de notación se define la estimación de proporciones de población como \\(p\\) y proporciones de población como \\(\\pi\\). Es normal observar que en muchos paquetes estadísticos optan por generar estimaciones de proporciones y errores estándar en la escala de porcentaje. R Genera las estimaciones de proporciones en escala [0,1]. A continuación, se presenta el código computacional para estimar la proporción de personas por zona: prop_zona &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise( prop = survey_mean(vartype = c(&quot;se&quot;,&quot;ci&quot;), proportion = TRUE )) prop_zona ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.480 0.0140 0.452 0.508 ## 2 Urban 0.520 0.0140 0.492 0.548 Como se pudo observar, se usó la función survey_mean para la estimación. Sin embargo, con el parámetro “proportion = TRUE”, se le indica a R que lo que se desea estimar es una proporción. Para este ejemplo se puede observar que, el 47.9% de las personas viven en zona rural obteniendo un intervalo de confianza comprendido entre (45.2%, 50.7%) y el 52% de las personas viven en la zona urbana con un intervalo de confianza de (49.2%, 54.7%). La librería survey tiene implementado una función específica para estimar proporciones la cual es survey_prop que genera los mismos resultados mostrados anteriormente. Le queda al lector la decisión de usar la función con la que más cómodo se sienta. A continuación, se muestra un ejemplo del uso de la función survey_prop: prop_zona2 &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise( prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;) )) prop_zona2 ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.480 0.0140 0.452 0.508 ## 2 Urban 0.520 0.0140 0.492 0.548 Si el interés ahora se centra en estimar subpoblaciones por ejemplo, proporción de hombres y mujeres que viven en la zona urbana, el código computacional es: prop_sexoU &lt;- sub_Urbano %&gt;% group_by(Sex) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) prop_sexoU ## # A tibble: 2 × 5 ## Sex prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 0.537 0.0130 0.511 0.563 ## 2 Male 0.463 0.0130 0.437 0.489 Arrojando como resultado que, el 53.6% de las mujeres y 46.4% de los hombres viven en la zona urbana y con intervalos de confianza (51%, 56.2%) y (43.7%, 48.9%) respectivamente. Los intervalos anteriores nos reflejan que, con una confianza del 95% la cantidad estimada de mujeres que viven en la zona urbana es de56% y de hombres es de 48%. Realizando el mismo ejercicio anterior, pero ahora en la zona rural se tiene: prop_sexoR &lt;- sub_Rural %&gt;% group_by(Sex) %&gt;% summarise( n = unweighted(n()), prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) prop_sexoR ## # A tibble: 2 × 6 ## Sex n prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 679 0.516 0.00824 0.500 0.533 ## 2 Male 618 0.484 0.00824 0.467 0.500 el 51.6% de las mujeres y el 48.4% de los hombres viven en la zona rural con intervalos de confianza de (49.9%, 53.2%) y (46,7%, 50%) respectivamente. Los intervalos de confianza anteriores nos reflejan que, inclusive, con una confianza del 95%, la cantidad estimada de mujeres en la zona rural es de 53% y de hombres es de 50%. Ahora bien, si nos centramos solo en la población de hombres en la base de datos y se desea estimar la proporción de hombres por zona, el código computacional es el siguiente: prop_ZonaH &lt;- sub_Hombre %&gt;% group_by(Zone) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) prop_ZonaH ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.491 0.0178 0.455 0.526 ## 2 Urban 0.509 0.0178 0.474 0.545 En la anterior tabla se puede observar que el 49% de los hombres están en la zona rural y el 51% en la zona urbana. Si se observa el intervalo de confianza se puede concluir que, con una confianza del 95%, la población estimada de hombres que viven en la zona rural puede llegar a ser el 52% y en urbana un 54%. Si se realiza ahora el mismo ejercicio para la mujeres el código computacional es: prop_ZonaM &lt;- sub_Mujer %&gt;% group_by(Zone) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) prop_ZonaM ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.470 0.0140 0.443 0.498 ## 2 Urban 0.530 0.0140 0.502 0.557 De la tabla anterior se puede inferir que, el 47% de las mujeres están en la zona rural y el 52% en la zona urbana. Observando también intervalos de confianza al 95% de (44%, 49%) y (50%, 55%) para las zonas rural y urbana respectivamente. Si se desea estimar por varios niveles de desagregación, con el uso de la función group_by es posible estimar un mayor número de niveles de agregación al combinar dos o más variables. Por ejemplo, si se desea estimar la proporción de hombres por zona y en estado de pobreza, se realiza de la siguiente manera: prop_ZonaH_Pobreza &lt;- sub_Hombre %&gt;% group_by(Zone, Poverty) %&gt;% summarise( prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;)))%&gt;% data.frame() prop_ZonaH_Pobreza ## Zone Poverty prop prop_se prop_low prop_upp ## 1 Rural NotPoor 0.5488453 0.06264753 0.42434340 0.6675180 ## 2 Rural Extreme 0.1975254 0.06745258 0.09582905 0.3637294 ## 3 Rural Relative 0.2536293 0.03724070 0.18711180 0.3340755 ## 4 Urban NotPoor 0.6599255 0.03662268 0.58415144 0.7283141 ## 5 Urban Extreme 0.1128564 0.02451869 0.07264146 0.1712240 ## 6 Urban Relative 0.2272181 0.02604053 0.17979436 0.2828371 De la salida anterior se puede observar que, en la ruralidad, el 19% de los hombres están en pobreza extrema mientras que en la zona urbana el 11% también están en pobreza extrema. Por otro lado, el 54% de los hombres que viven en la zona rural no están en pobreza mientras que, en la zona urbana el 65% no está en esta condición. El mismo ejercicio anterior para la población de mujeres sería: prop_ZonaM_Pobreza &lt;- sub_Mujer %&gt;% group_by(Zone, Poverty) %&gt;% summarise( prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) %&gt;% data.frame() prop_ZonaM_Pobreza ## Zone Poverty prop prop_se prop_low prop_upp ## 1 Rural NotPoor 0.5539176 0.05568825 0.44281376 0.6598834 ## 2 Rural Extreme 0.1599702 0.05574533 0.07728197 0.3021593 ## 3 Rural Relative 0.2861122 0.04357612 0.20803909 0.3794466 ## 4 Urban NotPoor 0.6612172 0.03224726 0.59475977 0.7218725 ## 5 Urban Extreme 0.1093753 0.02209821 0.07267359 0.1613865 ## 6 Urban Relative 0.2294075 0.02655874 0.18106582 0.2861459 De la salida anterior se puede observar que, en la ruralidad, el 16% de las mujeres están en pobreza extrema mientras que en la zona urbana el 10% también están en pobreza extrema. Por otro lado, el 55% de las mujeres que viven en la zona rural no están en pobreza mientras que, en la zona urbana el 66% no está en esta condición. Si lo que se desea ahora es estimar la proporción de hombres empleados o no por zona, se realiza de la siguiente manera: prop_ZonaH_Ocupacion &lt;- sub_Hombre %&gt;% group_by(Zone, Employment) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;)))%&gt;% data.frame() prop_ZonaH_Ocupacion ## Zone Employment prop prop_se prop_low prop_upp ## 1 Rural Unemployed 0.05125186 0.015733138 0.02767737 0.09298588 ## 2 Rural Inactive 0.10351629 0.020267044 0.06970747 0.15106011 ## 3 Rural Employed 0.52251375 0.026522751 0.46994089 0.57459249 ## 4 Rural &lt;NA&gt; 0.32271810 0.034987840 0.25763953 0.39547790 ## 5 Urban Unemployed 0.04374724 0.008492664 0.02969659 0.06400729 ## 6 Urban Inactive 0.16331307 0.018093938 0.13056379 0.20236490 ## 7 Urban Employed 0.51337023 0.023637331 0.46658553 0.55992181 ## 8 Urban &lt;NA&gt; 0.27956945 0.022085422 0.23799131 0.32531045 De la salida anterior se puede observar que, el 5% de los hombres que viven en la ruralidad están desempleados mientras que el 4% de los que viven en la zona urbana están en esta misma condición. Ahora bien, el 52% de los hombres que viven en la ruralidad trabajan mientras que el 51% de los que viven en la zona rural también están empleados. Si se hace este mismo ejercicio para las mujeres se obtiene: prop_ZonaM_Ocupacion &lt;- sub_Mujer %&gt;% group_by(Zone, Employment) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) %&gt;% data.frame() prop_ZonaM_Ocupacion ## Zone Employment prop prop_se prop_low prop_upp ## 1 Rural Unemployed 0.01017065 0.005540256 0.003443802 0.02964628 ## 2 Rural Inactive 0.44719272 0.035247218 0.378871481 0.51756811 ## 3 Rural Employed 0.23999716 0.039151859 0.171118101 0.32570701 ## 4 Rural &lt;NA&gt; 0.30263948 0.030765644 0.245379430 0.36676711 ## 5 Urban Unemployed 0.02109678 0.005964137 0.012019202 0.03677508 ## 6 Urban Inactive 0.36445938 0.021442387 0.323143427 0.40787461 ## 7 Urban Employed 0.38455672 0.019452094 0.346831628 0.42372325 ## 8 Urban &lt;NA&gt; 0.22988711 0.013850398 0.203613820 0.25845036 Para las mujeres se puede observar que, el 1% de las mujeres que viven en la ruralidad están desempleados mientras que el 2% de las que viven en la zona urbana están en esta misma condición. Ahora bien, el 24% de las mujeres que viven en la ruralidad trabajan mientras que el 38% de las que viven en la zona rural también están empleados. Otro parámetro que es de interés es estimar en encuestas de hogares la cantidad de personas menores y mayores de edad en los hogares. A continuación, ejemplificamos la estimación de menores y mayores a 18 años cruzado por pobreza: diseno %&gt;% group_by(edad_18, pobreza) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_18 pobreza Prop Prop_se Prop_low Prop_upp ## 1 &lt; 18 anios 0 0.4984504 0.03729355 0.4251710 0.5717964 ## 2 &lt; 18 anios 1 0.5015496 0.03729355 0.4282036 0.5748290 ## 3 &gt;= 18 anios 0 0.6646140 0.02978353 0.6033275 0.7208132 ## 4 &gt;= 18 anios 1 0.3353860 0.02978353 0.2791868 0.3966725 De la anterior salida se puede observar que, el 50% de los menores de edad y el 33% de los mayores de edad están en estado de pobreza. Al observar los intervalos de confianza para los menores de edad en estado de pobreza se puede observar que, dicha estimación puede llegar, con una confianza del 95% a 57% mientras que a los mayores de edad puede llegar a 39%. Ahora, si se hace este mismo ejercicio, pero esta vez cruzando con la variable que indica empleo se obtiene: diseno %&gt;% group_by(edad_18, desempleo) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_18 desempleo Prop Prop_se Prop_low Prop_upp ## 1 &lt; 18 anios 0 0.166704172 0.014856561 0.139321648 0.19822898 ## 2 &lt; 18 anios 1 0.003729693 0.001969183 0.001309174 0.01057808 ## 3 &lt; 18 anios NA 0.829566135 0.015009188 0.797760089 0.85726505 ## 4 &gt;= 18 anios 0 0.955234872 0.007552778 0.937660285 0.96802386 ## 5 &gt;= 18 anios 1 0.044765128 0.007552778 0.031976144 0.06233972 De la tabla anterior se puede observar que, el 0.3% de los menores de edad y el 4% de los mayores de edad están desempleados. Adicionalmente, con una confianza del 95% y basados en la muestra se puede observar que el desempleo en menores de edad puede llegar a 0.7% y para los mayores llega a un 5%. Por otro lado, si el objetivo ahora es estimar la cantidad de menores de edad en la zona rural se realiza de la siguiente manera: sub_Rural %&gt;% group_by(edad_18) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_18 Prop Prop_se Prop_low Prop_upp ## 1 &lt; 18 anios 0.3711613 0.03021982 0.3128746 0.4334566 ## 2 &gt;= 18 anios 0.6288387 0.03021982 0.5665434 0.6871254 De la anterior tabla se puede observar que, el 37% de las personas que viven en la zona rural de la base de ejemplo son menores de edad con un intervalo de confianza al 95% comprendido entre 31% y 43%. Como se mencionó al inicio del capítulo, es posible categorizar una variable de tipo cuantitativo como por ejemplo la edad y cruzarla con la variable que categoriza la empleabilidad. A continuación, se estima la edad de las mujeres por rango. sub_Mujer %&gt;% mutate(edad_rango = case_when( Age&gt;= 18 &amp; Age &lt;=35 ~ &quot;18 - 35&quot;, TRUE ~ &quot;Otro&quot;)) %&gt;% group_by(edad_rango, Employment) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_rango Employment Prop Prop_se Prop_low Prop_upp ## 1 18 - 35 Unemployed 0.02893412 0.009142347 0.015403014 0.05370358 ## 2 18 - 35 Inactive 0.51653851 0.037905184 0.441673039 0.59066889 ## 3 18 - 35 Employed 0.45452737 0.035685710 0.385232560 0.52562948 ## 4 Otro Unemployed 0.01015164 0.004026104 0.004617517 0.02217073 ## 5 Otro Inactive 0.35271022 0.020725430 0.312834830 0.39474850 ## 6 Otro Employed 0.25483870 0.021700305 0.214292062 0.30012671 ## 7 Otro &lt;NA&gt; 0.38229944 0.022313379 0.339191706 0.42734277 De la anterior tabla se puede observar, entre otros que, las mujeres con edades entre 18 y 35 años el 2% están desempleadas y el 45% están empleadas. Análisis similares se pueden hacer para los demás rangos de edades. Este mismo ejercicio se puede realizar para los hombres y hacer los mismos análisis. A continuación, se muestra el código computacional: sub_Hombre %&gt;% mutate(edad_rango = case_when( Age&gt;= 18 &amp; Age &lt;=35 ~ &quot;18 - 35&quot;,TRUE ~ &quot;Otro&quot;)) %&gt;% group_by(edad_rango, Employment) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_rango Employment Prop Prop_se Prop_low Prop_upp ## 1 18 - 35 Unemployed 0.09637042 0.018215667 0.06584071 0.13895080 ## 2 18 - 35 Inactive 0.08939940 0.016438321 0.06175556 0.12773290 ## 3 18 - 35 Employed 0.81423018 0.022991735 0.76436394 0.85553799 ## 4 Otro Unemployed 0.02606667 0.007175709 0.01506262 0.04474457 ## 5 Otro Inactive 0.15344056 0.019883462 0.11805657 0.19706023 ## 6 Otro Employed 0.38849664 0.020270309 0.34919327 0.42930563 ## 7 Otro &lt;NA&gt; 0.43199614 0.021111842 0.39076987 0.47418649 "],["tablas-cruzadas..html", "5.3 Tablas cruzadas.", " 5.3 Tablas cruzadas. Una tabla de contingencia o tablas cruzadas es una herramienta muy utilizada en el análisis de encuestas de hogares puesto que, está conformada por al menos dos filas y dos columnas y representa información de variables categóricos en términos de conteos de frecuencia. Estas tablas tienen el objetivo de representar de manera resumida, la relación entre diferentes variables categóricas. una tabla de contingencia se asume como un arreglo bidimensional de \\(r=1,\\ldots,R\\) filas y \\(c=1,\\ldots,C\\) columnas. Cabe resaltar que, Las tablas cruzadas o de contingencia no se limitan a dos dimensiones, también se pueden incluir una tercera variable o más, es decir, \\(l=1,\\ldots,L\\) subtablas basadas en las categorías de una tercera variable. Para efectos de ilustración y facilitación de los ejemplos y conceptos teóricos, en esta sección de trabajarán, en su mayoría con tablas \\(2\\times2\\). Gráficamente, estas tablas se construyen con frecuencias no estimadas como se muestra a continuación: Variable 2 Variable 1 Marginal fila 0 1 0 \\(n_{00}\\) \\(n_{01}\\) \\(n_{0+}\\) 1 \\(n_{10}\\) \\(n_{11}\\) \\(n_{1+}\\) Marginal columna \\(n_{+0}\\) \\(n_{+1}\\) \\(n_{++}\\) A continuación, se muestra la tabla de doble entrada con las frecuencias estimadas o ponderadas: Variable 2 Variable 1 Marginal fila 0 1 0 \\(\\hat{N}_{00}\\) \\(\\hat{N}_{01}\\) \\(\\hat{N}_{0+}\\) 1 \\(n_{10}\\) \\(n_{11}\\) \\(n_{1+}\\) Marginal columna \\(n_{+0}\\) \\(n_{+1}\\) \\(n_{++}\\) donde, por ejemplo, la frecuencia ponderada o estimada en la celda (0, 1) está dada por \\(\\hat{N}_{01}={ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i\\in\\left(0,1\\right)}^{n_{h\\alpha}}}\\omega_{h\\alpha i}\\). Las proporciones estimadas a partir de estas frecuencias muestrales ponderadas, se obtienen de la siguiente manera \\(p_{rc}=\\frac{\\hat{N}_{rc}}{\\hat{N}_{++}}\\). Estimación de proporciones para variables binarias La estimación de una sola proporción, \\(\\pi\\), para una variable de respuesta binaria requiere solo una extensión directa del estimador de razón mostrado en secciones anteriores. Como lo menciona Heeringa, S. G. (2017) Al recodificar las categorías de respuesta originales en una sola variable indicadora \\(y_{i}\\) con valores posibles de 1 y 0 (por ejemplo, sí = 1, no = 0), el estimador de la media de la razón estima la proporción o prevalencia, \\(\\pi\\), de “1” en la población está dada por: \\[ p = \\frac{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i\\in\\left(0,1\\right)}^{n_{h\\alpha}}}\\omega_{h\\alpha i}I\\left(y_{i}=1\\right)}{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i\\in\\left(0,1\\right)}^{n_{h\\alpha}}}\\omega_{h\\alpha i}} = \\frac{\\hat{N}_{1}}{\\hat{N}} \\] Aplicando Linealización de Taylor (TSL) al estimador de razón de \\(\\pi\\) genera el siguiente estimador para la varianza: \\[ v\\left(p\\right) \\dot{=} \\frac{V\\left(\\hat{N}_{1}\\right)+p^{2}V\\left(\\hat{N}\\right)-2\\,p\\,cov\\left(\\hat{N}_{1},\\hat{N}\\right)}{\\hat{N}^{2}} \\] Como es bien sabido en la literatura especializada, cuando la proporción de interés estimada está cerca de 0 o 1, los límites del intervalo de confianza estándar basados en el diseño de muestreo pueden ser menores que 0 o superiores a 1. Lo cual no tendría interpretación por la naturaleza del parámetro. Es por lo anterior que, para solventar este problema se puede realizar cálculos alternativos de IC basados en el diseño de muestreo para las proporciones como lo proponen Wilson modificado (Rust y Hsu, 2007; Dean y Pagano, 2015). El intervalo de confianza utilizando la transformación \\(Logit\\left(p\\right)\\) está dado por: \\[ IC\\left[logit\\left(p\\right)\\right] = \\left\\{ ln\\left(\\frac{p}{1-p}\\right)\\pm\\frac{t_{1-\\alpha/2,\\,gl}se\\left(p\\right)}{p\\left(1-p\\right)}\\right\\} \\] Por tanto, el intervalo de confianza para \\(p\\) sería: \\[ IC\\left(p\\right) = \\left\\{ \\frac{exp\\left[ln\\left(\\frac{p}{1-p}\\right)\\pm\\frac{t_{1-\\alpha/2,\\,gl}se\\left(p\\right)}{p\\left(1-p\\right)}\\right]}{1+exp\\left[ln\\left(\\frac{p}{1-p}\\right)\\pm\\frac{t_{1-\\alpha/2,\\,gl}se\\left(p\\right)}{p\\left(1-p\\right)}\\right]}\\right\\} \\] Ahora bien, si se el interés es estimar proporciones para variables multinomiales. El estimador es el siguiente: \\[ p_{k} = \\frac{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i=1}^{n_{h\\alpha}}}\\omega_{h\\alpha i}I\\left(y_{i}=k\\right)}{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i=1}^{n_{h\\alpha}}}\\omega_{h\\alpha i}} = \\frac{\\hat{N}_{k}}{\\hat{N}} \\] A continuación, siguiendo con la base de ejemplo, se estima la proporción de hombres y mujeres en pobreza y no pobreza junto con su error estándar e intervalos de confianza. prop_sexo_zona &lt;- diseno %&gt;% group_by(pobreza,Sex) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() prop_sexo_zona ## pobreza Sex prop prop_se prop_low prop_upp ## 1 0 Female 0.5291800 0.01242026 0.5045356 0.5536829 ## 2 0 Male 0.4708200 0.01242026 0.4463171 0.4954644 ## 3 1 Female 0.5236123 0.01586237 0.4921512 0.5548870 ## 4 1 Male 0.4763877 0.01586237 0.4451130 0.5078488 Como se puede observar, el 52.3% de las mujeres y el 47.6% son pobres. Generando intervalos de confianza al 95% de (49.2%, 55.5%) para las mujeres y (44.5%, 50.7%) para los hombres. En la librería survey existe una alternativa para estimar tablas de contingencias y es utilizando la función svyby como se muestra a continuación: tab_Sex_Pobr &lt;- svyby(formula = ~Sex, by = ~pobreza, design = diseno, FUN = svymean) tab_Sex_Pobr ## pobreza SexFemale SexMale se.SexFemale se.SexMale ## 0 0 0.5291800 0.4708200 0.01242026 0.01242026 ## 1 1 0.5236123 0.4763877 0.01586237 0.01586237 Como se pudo observar, los argumentos que requiere la función son definir la variable a la cual se desea estimar (formula), las categorías por la cual se desea estimar (by), el diseño muestral (desing) y el parámetro que se desea estimar (FUN). Para la estimación de los intervalos de confianza se utiliza la función confint como sigue: Para la estimación de los intervalos de confianza utilizar la función confint. confint(tab_Sex_Pobr) %&gt;% as.data.frame() ## 2.5 % 97.5 % ## 0:SexFemale 0.5048367 0.5535232 ## 1:SexFemale 0.4925226 0.5547019 ## 0:SexMale 0.4464768 0.4951633 ## 1:SexMale 0.4452981 0.5074774 Los cuales coinciden con los generados anteriormente usando la funicón group_by. Otro análisis de interés relacionado con tablas de doble entrada en encuestas de hogares es estimar el porcentaje de desempleados por sexo. tab_Sex_Ocupa &lt;- svyby(formula = ~Sex, by = ~Employment, design = diseno, FUN = svymean) tab_Sex_Ocupa ## Employment SexFemale SexMale se.SexFemale se.SexMale ## Unemployed Unemployed 0.2726730 0.7273270 0.05351318 0.05351318 ## Inactive Inactive 0.7703406 0.2296594 0.02340005 0.02340005 ## Employed Employed 0.4051575 0.5948425 0.01851986 0.01851986 De la anterior salida se puede observar que, el 27.2% de las mujeres y el 72.7% de los hombres están desempleados con errores estándares para estas estimaciones de 5.3% para mujeres y hombres. cuyos intervalos de confianza se calculan a continuación: confint(tab_Sex_Ocupa) %&gt;% as.data.frame() ## 2.5 % 97.5 % ## Unemployed:SexFemale 0.1677891 0.3775570 ## Inactive:SexFemale 0.7244773 0.8162038 ## Employed:SexFemale 0.3688592 0.4414557 ## Unemployed:SexMale 0.6224430 0.8322109 ## Inactive:SexMale 0.1837962 0.2755227 ## Employed:SexMale 0.5585443 0.6311408 Si ahora el objetivo es estimar la pobreza, pero por las distintas regiones que se tienen en la base de datos. Primero, dado que la variable pobreza es de tipo numérica, es necesario convertirla en factor y luego realizar la estimación con la función svyby. tab_region_pobreza &lt;- svyby(formula = ~as.factor(pobreza), by = ~Region, design = diseno, FUN = svymean) tab_region_pobreza ## Region as.factor(pobreza)0 as.factor(pobreza)1 ## Norte Norte 0.6410318 0.3589682 ## Sur Sur 0.6561536 0.3438464 ## Centro Centro 0.6346152 0.3653848 ## Occidente Occidente 0.5991839 0.4008161 ## Oriente Oriente 0.5482079 0.4517921 ## se.as.factor(pobreza)0 se.as.factor(pobreza)1 ## Norte 0.05547660 0.05547660 ## Sur 0.04348901 0.04348901 ## Centro 0.07858599 0.07858599 ## Occidente 0.04670473 0.04670473 ## Oriente 0.08849644 0.08849644 De lo anterior se puede concluir que, en la región Norte, el 35% de las personas están en estado de pobreza mientras que en el sur es el 34%. La pobreza más alta se tiene en la región oriente con un 45% de pobres. Los errores estándares de las estimaciones. Prueba de independencia \\(\\chi^{2}\\) Esta prueba es una de las más utilizadas para determinar si no existe asociación o independencia entre dos variables de tipo cualitativa. En otras palabras, que dos variables sean independientes significa que una no depende de la otra, ni viceversa. A modo de ejemplificar la técnica, para una tabla de \\(2\\times2\\), la prueba \\(\\chi^{2}\\) de personas se define como: \\[ \\chi^{2} = n_{++}\\sum_{r}\\sum_{c}\\frac{\\left(p_{rc}-\\hat{\\pi}_{rc}\\right)^{2}}{\\hat{\\pi}_{rc}} \\] donde, \\(\\hat{\\pi}_{rc}=\\frac{n_{r+}}{n_{++}}\\,\\frac{n_{+c}}{n_{++}}\\,p_{r+}\\,p_{+c}\\). Para realizar la prueba de independencia \\(\\chi^{2}\\) en R, se utilizará la función svychisq del paquete srvyr. Esta función requiere que se definan las variables de interés (formula) y requiere que se le defina el diseño muestral (desing). Ahora, para ejemplificar el uso de esta función tomaremos la base de datos de ejemplo y se probará si la pobreza es independiente del sexo. A continuación, se presentan los códigos computacionales: svychisq(formula = ~Sex + pobreza, design = diseno, statistic=&quot;F&quot;) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 0.056464, ndf = 1, ddf = 119, p-value = 0.8126 Dado que el p-valor es superior al nivel de significancia 5% se puede concluir que, con una confianza del 95% y basado en la muestra, la pobreza no depende del sexo de las personas. En este mismo sentido, si se desea saber si el desempleo está relacionado con el sexo, se realiza la prueba de hipótesis \\(\\chi^{2}\\) como sigue: svychisq(formula = ~Sex + Employment, design = diseno, statistic=&quot;F&quot;) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 62.251, ndf = 1.6865, ddf = 200.6978, p-value &lt; 2.2e-16 Concluyendo que, con una confianza del 95% y basado en la muestra se rechaza la hipótesis nula, es decir, no se puede afirmar que las variables sexo y desempleo sean independiente. Si en el análisis ahora se quiere verificar que la pobreza de las personas es independiente de las regiones establecidas en la base de datos, se realiza de la siguiente manera: svychisq(formula = ~Region + pobreza, design = diseno, statistic=&quot;F&quot;) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 0.48794, ndf = 3.0082, ddf = 357.9731, p-value = 0.6914 Concluyendo que, con una confianza del 95% y basado en la muestra hay independencia entre la pobreza y la región. Lo anterior implica que, no existe relación lineal entre las personas en estado de pobreza por región. Razón de odds Como lo menciona Monroy, L. G. D. (2018) La traducción más aproximada del término odds es “la ventaja”, en términos de probabilidades es la posibilidad de que un evento ocurra con relación a que no ocurra, es decir, es un número que expresa cuánto más probable es que se produzca un evento frente a que no se produzca. También se puede utilizar para cuantificar la asociación entre los niveles de una variable y un factor categórico (Heeringa, S. G. 2017). Suponga que se desea calcular la siguiente razón de odds. \\[ \\frac{\\frac{P(Sex = Female \\mid pobreza = 0 )}{P(Sex = Female \\mid pobreza = 1 )}}{ \\frac{P(Sex = Male \\mid pobreza = 1 )}{P(Sex = Male \\mid pobreza = 0 )} } \\] El procedimiento para realizarlo en R sería, primero estimar las proporciones de la tabla cruzada entre las variables sexo y pobreza: tab_Sex_Pobr &lt;- svymean(x = ~interaction (Sex, pobreza), design = diseno, se=T, na.rm=T, ci=T, keep.vars=T) tab_Sex_Pobr %&gt;% as.data.frame() ## mean SE ## interaction(Sex, pobreza)Female.0 0.3218703 0.01782709 ## interaction(Sex, pobreza)Male.0 0.2863733 0.01768068 ## interaction(Sex, pobreza)Female.1 0.2051285 0.01659697 ## interaction(Sex, pobreza)Male.1 0.1866279 0.01778801 Luego, se realiza el contraste dividiendo cada uno de los elementos de la expresión mostrada anteriormente: svycontrast(stat = tab_Sex_Pobr, contrasts = quote((`interaction(Sex, pobreza)Female.0`/`interaction(Sex, pobreza)Female.1`) /(`interaction(Sex, pobreza)Male.0`/ `interaction(Sex, pobreza)Male.1`))) ## nlcon SE ## contrast 1.0226 0.0961 Obtiendo que, se estima que el odds de las mujeres que no están en estado de pobreza es 1.02 comparandolo con el odds de los hombres. En otras palabras, se estima que las probabilidades de que las mujeres no estén en estado de pobreza sin tener en cuenta ninguna otra variable de la encuesta es cera de 2% mayor que las probabilidades de los hombres. Diferencia de proporciones en tablas de contingencias Como lo menciona Heeringa, S. G. (2017) las estimaciones de las proporciones de las filas en las tablas de doble entrada son, de hecho, estimaciones de subpoblaciones en las que la subpoblación se define por los niveles de la variable factorial. Ahora bien, si el interés se centra en estimar diferencias de las proporciones de las categorías entre dos niveles de una variable factorial, se pueden utilizando contrastes. A manera de ejemplo, se requiere estimar ahora, el contraste de proporciones de mujeres en estado de pobreza versus los hombres en esta misma condición (\\(\\hat{p}_F - \\hat{p}_M\\)). Para ellos, primero, estimemos la proporción de hombres y mujeres en estado de pobreza como se ha mostrado en capítulos anteriores: (tab_sex_pobreza &lt;- svyby(formula = ~pobreza, by = ~Sex, design = diseno , svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## Sex pobreza se ci_l ci_u ## Female Female 0.3892389 0.03159581 0.3273123 0.4511656 ## Male Male 0.3945612 0.03662762 0.3227724 0.4663501 Ahora bien, para calcular la estimación de la diferencia de proporciones junto con sus errores estándares, se realizarán los siguientes pasos: Paso 1: Calcular la diferencia de estimaciones 0.3892 - 0.3946 ## [1] -0.0054 Con la función vcov se obtiene la matriz de covarianzas: library(kableExtra) vcov(tab_sex_pobreza)%&gt;% data.frame() %&gt;% kable(digits = 10, format.args = list(scientific = FALSE)) Female Male Female 0.0009982953 0.0009182927 Male 0.0009182927 0.0013415823 Paso 2: El cálculo del error estándar es: sqrt(0.0009983 + 0.0013416 - 2*0.0009183) ## [1] 0.02243435 Ahora bien, aplicando la función svycontrast se puede obtener la estimación de la diferencia de proporciones anterior: svycontrast(tab_sex_pobreza, list(diff_Sex = c(1, -1))) %&gt;% data.frame() ## contrast diff_Sex ## diff_Sex -0.005322297 0.02243418 De lo que se concluye que, la diferencia entre las proporciones de mujeres y hombres en estado de pobreza es -0.005 (-0.5%) con una desviación estándar de 0.022. Otro ejercicio de interés en un análisis de encuestas de hogares es verificar la diferencia del desempleo por sexo. Al igual que el ejemplo anterior, se inicia con la estimación del porcentaje de desempleados por sexo: tab_sex_desempleo &lt;- svyby(formula = ~desempleo, by = ~Sex, design = diseno %&gt;% filter(!is.na(desempleo)) , FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) tab_sex_desempleo ## Sex desempleo se ci_l ci_u ## Female Female 0.02168620 0.005580042 0.01074952 0.03262288 ## Male Male 0.06782601 0.012161141 0.04399062 0.09166141 Para calcular la estimación de la diferencia de proporciones junto con sus errores estándares, se realizarán los siguientes pasos: Paso 1: Diferencia de las estimaciones 0.02169 - 0.06783 ## [1] -0.04614 Estimación de la matriz de covarianza: vcov(tab_sex_desempleo) %&gt;% data.frame() %&gt;% kable(digits = 10, format.args = list(scientific = FALSE)) Female Male Female 0.0000311369 0.0000208130 Male 0.0000208130 0.0001478933 Paso 2: Estimación del error estándar. sqrt(0.00003114 + 0.00014789 - 2*0.00002081) ## [1] 0.0117222 Siguiendo el ejemplo anterior, utilizando la función svycontrast se tiene que: svycontrast(tab_sex_desempleo, list(diff_Sex = c(-1, 1))) %&gt;% data.frame() ## contrast diff_Sex ## diff_Sex 0.04613982 0.01172195 de lo que se concluye que, la estimación del contraste es 0.04 (4%) con un error estándar de 0.011. Otro ejercicio que se puede realizar en una encuesta de hogares es ahora estimar la proporción de desempleados por región. Para la realización de este ejercicio, se seguirán los pasos de los dos ejemplos anteriores: tab_region_desempleo &lt;- svyby(formula = ~desempleo, by = ~Region, design = diseno %&gt;% filter(!is.na(desempleo)) , FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) tab_region_desempleo ## Region desempleo se ci_l ci_u ## Norte Norte 0.04877722 0.02002293 0.009532997 0.08802144 ## Sur Sur 0.06563877 0.02375124 0.019087202 0.11219034 ## Centro Centro 0.03873259 0.01240317 0.014422832 0.06304235 ## Occidente Occidente 0.03996523 0.01229650 0.015864529 0.06406592 ## Oriente Oriente 0.02950231 0.01256905 0.004867428 0.05413719 Ahora, el interés es realizar los contrastes siguientes para desempleo: \\(\\hat{p}_{Norte} - \\hat{p}_{Centro} = 0.01004\\), \\(\\hat{p}_{Sur} - \\hat{p}_{Centro} = 0.02691\\) \\(\\hat{p}_{Occidente} - \\hat{p}_{Oriente} = 0.01046\\) Escrita de forma matricial sería: \\[ \\left[\\begin{array}{ccccc} 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right] \\] La matriz de varianzas y covarianzas es: vcov(tab_region_desempleo)%&gt;% data.frame() %&gt;% kable(digits = 10, format.args = list(scientific = FALSE)) Por tanto, la varianza estimada está dada por: sqrt(0.0002981 + 0.0002884 - 2*0) ## [1] 0.02421776 sqrt(0.0001968 + 0.0002884 - 2*0) ## [1] 0.02202726 sqrt(0.0001267 + 0.0004093 - 2*0) ## [1] 0.02315167 Usando la función svycontrast, la estimación de los contrastes sería: svycontrast(tab_region_desempleo, list( Norte_sur = c(1, 0, -1, 0, 0), Sur_centro = c(0, 1, -1, 0, 0), Occidente_Oriente = c(0, 0, 0, 1, -1))) %&gt;% data.frame() ## contrast SE ## Norte_sur 0.01004463 0.02355327 ## Sur_centro 0.02690618 0.02679477 ## Occidente_Oriente 0.01046292 0.01758365 Por último, repitiendo el contraste anterior y los pasos para resolverlo, pero ahora utilizando la variable pobreza se tiene: tab_region_pobreza &lt;- svyby(formula = ~pobreza, by = ~Region, design = diseno %&gt;% filter(!is.na(desempleo)) , FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) tab_region_pobreza ## Region pobreza se ci_l ci_u ## Norte Norte 0.3262813 0.04800361 0.2321959 0.4203666 ## Sur Sur 0.2946736 0.04794292 0.2007072 0.3886400 ## Centro Centro 0.3233923 0.07211854 0.1820426 0.4647421 ## Occidente Occidente 0.3673286 0.04400234 0.2810856 0.4535716 ## Oriente Oriente 0.3870632 0.09160150 0.2075276 0.5665989 El interés se centra en realizar los contrastes siguientes para pobreza: \\(\\hat{p}_{Norte} - \\hat{p}_{Centro}\\), \\(\\hat{p}_{Sur}-\\hat{p}_{Centro}\\) \\(\\hat{p}_{Occidente}-\\hat{p}_{Oriente}\\) Escrita de forma matricial es: \\[ \\left[\\begin{array}{ccccc} 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right] \\] Y, utilizando la función svycontrast se obtiene: svycontrast(tab_region_pobreza, list( Norte_sur = c(1, 0, -1, 0, 0), Sur_centro = c(0, 1, -1, 0, 0), Occidente_Oriente = c(0, 0, 0, 1, -1))) %&gt;% data.frame() ## contrast SE ## Norte_sur 0.002888908 0.08663389 ## Sur_centro -0.028718759 0.08660027 ## Occidente_Oriente -0.019734641 0.10162205 "],["modelos-de-regresión-bajo-diseños-de-muestreo-complejos.html", "Capítulo 6 Modelos de regresión bajo diseños de muestreo complejos", " Capítulo 6 Modelos de regresión bajo diseños de muestreo complejos Un modelo matemático es una relación funcional entre variables. El interés consiste en encontrar modelos que relacionen un conjunto de variables de entrada provenientes de censos, registros administrativos, etc. con una variable de salida proveniente de encuestas de hogares. Normalmente en un proceso se tienen varias salidas, pero en este libro se estudia una variable de salida o respuesta del proceso que se asume condicionada a una, o que depende de los valores de una o más variables de entrada. A modo de contexto histórico (Heringa), los primeros autores en discutir, de manera empírica, el impacto que surten los diseños muestrales complejos en las inferencias relacionadas con modelos de regresión fueron Kish y Frankel (1974). Adicional a lo anterior, Fuller (1975) desarrolló un estimador de varianza tomando como insumos teóricos la linealización para modelos de regresión lineal múltiple con ponderación desigual de las observaciones e introdujo estimadores de varianza para parámetros de regresión estimados bajo diseños de muestreo estratificado y de dos etapas. Ahora bien, como es bien sabido, para el uso de la teoría de modelos de regresión se requieren que se cumplan algunos supuestos estadísticos que en ocasiones no se cumplen. En este sentido, Sha et al. (1977) discutieron las violaciones de dichos supuestos y métodos apropiados para hacer inferencias sobre los parámetros estimados de los modelos de regresión lineal usando datos de encuestas, y presentaron una evaluación empírica del desempeño de los estimadores de varianza basados en TSL. En relación con las distribuciones muestrales Binder (1983) se centró en dichas distribuciones muestrales de estimadores para parámetros de regresión en poblaciones finitas y estimadores de varianza relacionados definidos. Skinner et al. (1989) trabajaron estimadores de las varianzas para los coeficientes de regresión que permitieron diseños de muestras complejos y recomendaron el uso de métodos de linealización u otros métodos para la estimación de la varianza. Avanzando un poco en la línea de tiempo, Fuller (2002) generó un resumen de los métodos de estimación para modelos de regresión que contienen información relacionada con muestras complejas. Por último, Pfeffermann (2011) hizo una discusión sobre los distintos enfoques basados en el ajuste de modelos de regresión lineal a datos de encuestas de muestras complejas, presentando apoyo empírico para el uso de un método “q-weighted”. Un modelo de regresión lineal simple se define como \\(y=\\beta_{0}+\\beta_{1}x+\\varepsilon\\) donde \\(y\\) se define como la variable dependiente, \\(x\\) es la variable independiente y \\(\\beta_{0}\\) y \\(\\beta_{1}\\) los parámetros del modelo. La variable \\(\\varepsilon\\) se conoce como el error aleatorio del modelo y se define como \\(\\varepsilon=y-\\hat{y}=y-\\beta_{0}+\\beta_{1}x\\). Generalizando el modelo anterior, se definen los modelos de regresión lineal múltiples como \\[ y = \\boldsymbol{x}\\boldsymbol{\\beta}+\\varepsilon = \\sum_{j=0}^{p}\\beta_{j}x_{j}+\\varepsilon = \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{p}x_{p}+\\varepsilon \\] Donde \\(x_{0}=1\\). Por otro lado, se define como el valor esperado para la variable dependiente condicionado con las variables independientes \\(x\\) como, \\(E\\left(y\\mid x\\right)=\\hat{\\beta}_{0}+\\hat{\\beta_{1}}x_{1}+\\hat{\\beta}_{2}x_{2}+\\cdots+\\hat{\\beta}_{p}x_{p}\\). Otra manera de escribir el modelo de regresión múltiple es: \\[ y_{i} = x_{i}\\boldsymbol{\\beta}+\\varepsilon_{i} \\] donde, \\(x_{i}=\\left[1\\,x_{1i}\\,\\ldots\\,x_{pi}\\right]\\) y \\(\\boldsymbol{\\beta}^{T}=\\left[\\beta_{0}\\,\\,\\beta_{1}\\,\\,\\ldots\\,\\,\\beta_{p}\\right]\\). El subíndice \\(i\\) hace referencia al elemento muestral o respondiente en el conjunto de datos. Algunas consideraciones para los modelos de regresión lineal son tomadas de Heringa y se describen a continuación: \\(E\\left(\\varepsilon_{i}\\mid x_{i}\\right)=0,\\) lo que significa que el valor esperado de los residuos condicionado a un grupo de covariables es igual a 0. \\(Var\\left(\\varepsilon_{i}\\mid x_{i}\\right)=\\sigma_{y,x}^{2}\\) (homogenidad de varianza) lo que significa que, la varianza de los residuos condicionado a un grupo de covariables es igual constante. \\(\\varepsilon_{i}\\mid x_{i}\\sim N\\left(0,\\,\\sigma_{y,x}^{2}\\right)\\) (Normalidad en los errores) lo que significa que, los residuos condicionados a un grupo de covariables se distribuye normal. Esta propiedad también se extiende a la variable respuesta \\(y_{i}\\). \\(cov\\left(\\varepsilon_{i},\\,\\varepsilon_{j}\\mid x_{i},x_{j}\\right)\\) (independencia en los residuales) los residuales en diferentes sujetos no están correlacionados con los valores dados en sus variables predictoras. Una vez definido el modelo de regresión lineal y sus supuestos, se puede deducir los siguiente: \\[ \\hat{y} = E\\left(y\\mid x\\right) = E\\left(\\boldsymbol{x}\\boldsymbol{\\beta}\\right)+E\\left(\\varepsilon\\right) = \\boldsymbol{x}\\boldsymbol{\\beta}+0 = \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{p}x_{p} \\] y Adicionalmente, \\[ var\\left(y_{i}\\mid x_{i}\\right) = \\sigma_{y,x}^{2} \\] \\[ cov\\left(y_{i},y_{j}\\mid x_{i},x_{j}\\right) = 0 \\] \\[ y_{i} \\sim N\\left(x_{i}\\boldsymbol{\\beta},\\sigma_{y,x}^{2}\\right) \\] "],["estimación-de-los-parámetros-en-un-modelo-de-regresión-con-muestras-complejas..html", "6.1 Estimación de los parámetros en un modelo de regresión con muestras complejas.", " 6.1 Estimación de los parámetros en un modelo de regresión con muestras complejas. Una vez se establecen los supuestos del modelo y las características distribucionales de los errores el paso siguientes es el proceso de estimación de los parámetros. A modo ilustrativo e introductorio al proceso de estimación de los parámetros con información provenientes de muestras complejas, si en lugar de observar una muestra de tamaño \\(n\\) de los \\(N\\) elementos de población se hubiera realizado un censo completo, el parámetro de regresión de población finita \\(\\beta_{1}\\) podría calcularse como sigue (Tellez, et. al 2016): \\[ \\beta_{1} = \\frac{{ \\sum_{i=1}^{N}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}}{\\sum_{i=1}^{N}\\left(X_{i}-\\bar{X}\\right)^{2}} \\] Ahora bien, cuando se desea estimar los parámetros de un modelo de regresión lineal, pero considerando que la información muestral proviene de encuestas con muestras complejas se altera el enfoque estándar que se le da a la estimación de coeficientes de regresión y sus errores estándar. La principal razón por la que los métodos de estimación de parámetros de los coeficientes de regresión cambian es que la información recolectada por medio de una encuesta con muestra compleja generalmente no está distribuida de manera idéntica dado que el diseño muestral así es planeado. En este contexto, al ajustar modelos de regresión con este conjunto de datos, dado que los diseños complejos en su mayoría contienen estratificación, conglomerados, probabilidades de selección desiguales, etc, impiden el uso de estimadores de varianza convencionales que se pueden derivar por máxima verosimilitud puesto que, con esta metodología se asumen que los datos son independientes e idénticamente distribuidos y que provienen de alguna distribución de probabilidad (binomial, Poisson, exponencial, normal, etc.). En su lugar, según Wolter, (2007) se emplean métodos no paramétricos robustos basados en linealización de Taylor o métodos de estimación de la varianza usando replicación (Jackknife, bootstrapping, etc). Con el fin de ilustrar la forma cómo se estiman los parámetros de regresión del modelo en el contexto de encuestas complejas se realizará estimando el parámetro \\(\\beta_{1}\\) y su varianza para una regresión lineal simple. La extensión a la estimación de los parámetros de un modelo de regresión múltiple, algebraicamente es compleja y se sale del contexto de este libro. A continuación, se presenta la estimación del intercepto y su varianza en un modelo de regresión lineal simple: \\[ \\hat{\\beta_{1}} = \\frac{{\\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-\\bar{y}_{\\omega}\\right)\\left(x_{h\\alpha i}-\\bar{x}_{\\omega}\\right)}}{{ \\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(x_{h\\alpha i}-\\bar{x}_{\\omega}\\right)^{2}}} = \\frac{t_{xy}}{t_{x^{2}}} \\] Como se puede observar en la ecuación anterior, el estimador del parámetro es un cociente de totales, por ende, su varianza estimada está dada por: \\[ var\\left(\\hat{\\beta_{1}}\\right) = \\frac{var\\left(t_{xy}\\right)+\\hat{\\beta}_{1}^{2}var\\left(t_{x^{2}}\\right)-2\\hat{\\beta}_{1}cov\\left(t_{xy},t_{x^{2}}\\right)}{\\left(t_{x^{2}}\\right)^{2}} \\] A modo de generalización según Kish y Frankel, (1974) para la estimación de la varianza en un modelo de regresión lineal múltiple los métodos de aproximación requieren totales de muestra ponderados para los cuadrados y productos cruzados de todas las combinaciones \\(y\\) y \\(x = {1 x_{1} … x_{p}}\\). A continuación, se presenta la estimación: \\[\\begin{eqnarray*} var\\left(\\hat{\\beta}\\right)=\\hat{\\Sigma}\\left(\\hat{\\beta}\\right) &amp; = &amp; \\left[\\begin{array}{cccc} var\\left(\\hat{\\beta}_{0}\\right) &amp; cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{1}\\right) &amp; \\cdots &amp; cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{p}\\right)\\\\ cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{1}\\right) &amp; var\\left(\\hat{\\beta}_{1}\\right) &amp; \\cdots &amp; cov\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{p}\\right)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{p}\\right) &amp; cov\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{p}\\right) &amp; \\cdots &amp; var\\left(\\hat{\\beta}_{p}\\right) \\end{array}\\right] \\end{eqnarray*}\\] Para ejemplificar los conceptos trabajados hasta este momento, se tomará la misma base que se ha venido trabajando durante todo el desarrollo de este libro. Se inicia con el cargue de las librerías, la base de datos y la definición del diseño de muestreo: knitr::opts_chunk$set(warning = FALSE, message = FALSE, error = FALSE) options(digits = 4) options(tinytex.verbose = TRUE) library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(jtools) library(broom) Cargue de la base y definición del diseño muestral: data(BigCity, package = &quot;TeachingSampling&quot;) library(tidyverse) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) head(encuesta) HHID Stratum NIh nIh dI PersonID PSU Zone Sex Age MaritalST Income Expenditure Employment Poverty dki dk wk Region CatAge idHH00031 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Male 68 Married 409.9 346.3 Employed NotPoor 8 36 34.50 Norte Más de 60 idHH00031 idStrt001 9 2 4.5 idPer02 PSU0003 Rural Female 56 Married 409.9 346.3 Employed NotPoor 8 36 33.64 Norte 46-60 idHH00031 idStrt001 9 2 4.5 idPer03 PSU0003 Rural Female 24 Married 409.9 346.3 Employed NotPoor 8 36 33.64 Norte 16-30 idHH00031 idStrt001 9 2 4.5 idPer04 PSU0003 Rural Male 26 Married 409.9 346.3 Employed NotPoor 8 36 34.50 Norte 16-30 idHH00031 idStrt001 9 2 4.5 idPer05 PSU0003 Rural Female 3 NA 409.9 346.3 NA NotPoor 8 36 33.64 Norte 0-5 idHH00041 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Female 61 Widowed 823.8 392.2 Employed NotPoor 8 36 33.64 Norte Más de 60 library(srvyr) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) Para efectos de los ejemplos y como se ha hecho en anteriores ocasiones, se divide la muestra en sub-grupos de la encuesta como sigue: sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) En este capítulo se ajustarán los modelos de regresión usando la base de datos de ejemplo que se ha venido trabajando en capítulos anteriores. Puesto que, en modelos de regresión, se utiliza muy frecuente el recurso gráfico. A continuación, se define un tema estándar que la CEPAL tiene para generar sus gráficos el cual se utilizará en este capítulo. Para observar que existe una correlación entre el ingreso y el gasto, las cuales son las variables que se utilizarán para el ajuste de los modelos, se construye un scatterplot usando la librería ggplot. Cabe resaltar que, como la base de datos encuesta, la cual se usa para ejemplificar es una muestra de la base BigCity, analizaremos de manera gráfica si poblacionalmente las dos variables mencionadas anteriormente tienen correlación como se muestra a continuación: library(ggplot2); library(ggpmisc) plot_BigCity &lt;- ggplot(data = BigCity, aes(x = Expenditure, y = Income)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x) + theme_cepal() plot_BigCity + stat_poly_eq(formula = y~x, aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;),size = 3), parse = TRUE) Si bien, existen unas observaciones por fuera de la nube de punto, el comportamiento general de la relación ingresos vs gastos mantiene una tendencia lineal. Una vez hecho el análisis gráfico de las variables a utilizar en los modelos a trabajar, se realizará primero un ajuste del modelo con los datos poblacionales y con esto poder analizar qué tan bueno serán los ajustes que se realizarán posteriormente. A continuación, se muestra el ajuste del modelo con los datos poblacionales: fit &lt;- lm(Income ~ Expenditure, data = BigCity) Ahora bien, para observar los parámetros poblacionales del modelo se utilizará la función modelsummary de la librería modelsummary de la siguiente manera: Tabla 6.1: Modelo BigCity Pob (Intercept) 123.337 Expenditure 1.229 Num.Obs. 150266 R2 0.359 R2 Adj. 0.359 RMSE 461.74 De la anterior salida se puede observar que, el intercepto es igual a 123.337 y el parámetro \\(\\beta_{1}\\) asociado al gasto es 1.229. La demás información relacionada a esta salida se analizará más adelante. Una vez revisada la información poblacional, se utilizará la información obtenida de la muestra para estimar los parámetros y con ello analizar qué tan buenas son las estimaciones. A continuación, se presenta una sintaxis similar a la anterior que permite construir el scatterplot pero para los datos de la muestra. plot_sin &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Income)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x) + theme_cepal() plot_sin + stat_poly_eq(formula = y~x, aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;), size = 5), parse = TRUE) Como se puede observar, los datos de la muestra tienen una tendencia lineal aunque un poco dispersa a medida que crecen los gastos en las familias. Una vez hecho los análisis gráficos se procede a ajustar los modelos de regresión lineal. A modo de comparar el efecto que tiene hacer un correcto uso de los factores de expansión del diseño, primero, se ajustará un modelo sin tener encuesta dichos factores como se muestra a continuación: fit_sinP &lt;- lm(Income ~ Expenditure, data = encuesta) fit_sinP ## ## Call: ## lm(formula = Income ~ Expenditure, data = encuesta) ## ## Coefficients: ## (Intercept) Expenditure ## 121.52 1.22 stargazer(fit_sinP, header = FALSE, title = &quot;Modelo sin factores de expansion&quot;, style = &quot;ajps&quot;) ## ## \\begin{table}[!htbp] \\centering ## \\caption{Modelo sin factores de expansion} ## \\label{} ## \\begin{tabular}{@{\\extracolsep{5pt}}lc} ## \\\\[-1.8ex]\\hline \\\\[-1.8ex] ## \\\\[-1.8ex] &amp; \\textbf{Income} \\\\ ## \\hline \\\\[-1.8ex] ## Expenditure &amp; 1.220$^{***}$ \\\\ ## &amp; (0.025) \\\\ ## Constant &amp; 121.516$^{***}$ \\\\ ## &amp; (11.408) \\\\ ## N &amp; 2605 \\\\ ## R-squared &amp; 0.487 \\\\ ## Adj. R-squared &amp; 0.487 \\\\ ## Residual Std. Error &amp; 345.012 (df = 2603) \\\\ ## F Statistic &amp; 2472.919$^{***}$ (df = 1; 2603) \\\\ ## \\hline \\\\[-1.8ex] ## \\multicolumn{2}{l}{$^{***}$p $&lt;$ .01; $^{**}$p $&lt;$ .05; $^{*}$p $&lt;$ .1} \\\\ ## \\end{tabular} ## \\end{table} Para el modelo ajustado sin factores de expansión, el \\(\\hat{\\beta}_{0}\\) es 121.52 y el \\(\\hat{\\beta}_{1}\\) asociado a la variable gastos es 1.22. Ahora, haciendo un Scatterplot con los datos encuesta pero utilizando los factores de expansión del diseño se debe agregar mapping = aes(weight = wk) en la función geom_smoothcomo sigue: plot_Ponde &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Income)) + geom_point(aes(size = wk)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x, mapping = aes(weight = wk)) + theme_cepal() plot_Ponde + stat_poly_eq(formula = y~x, aes(weight = wk, label = paste(..eq.label..,..rr.label.., sep = &quot;~~~&quot;)), parse = TRUE,size = 5) En este sentido, para ajustar modelos teniendo en cuenta los factores de expansión existen 2 formas, la primera es usando la función lm y la segunda es usando la función svyglm de la librería survey. A continuación. se ajusta el modelo usando la función lm: fit_Ponde &lt;- lm(Income ~ Expenditure, data = encuesta, weights = wk) fit_Ponde ## ## Call: ## lm(formula = Income ~ Expenditure, data = encuesta, weights = wk) ## ## Coefficients: ## (Intercept) Expenditure ## 103.136 1.263 stargazer(fit_Ponde, header = FALSE, title = &quot;Modelo encuesta ponderada&quot;, style = &quot;ajps&quot;) ## ## \\begin{table}[!htbp] \\centering ## \\caption{Modelo encuesta ponderada} ## \\label{} ## \\begin{tabular}{@{\\extracolsep{5pt}}lc} ## \\\\[-1.8ex]\\hline \\\\[-1.8ex] ## \\\\[-1.8ex] &amp; \\textbf{Income} \\\\ ## \\hline \\\\[-1.8ex] ## Expenditure &amp; 1.263$^{***}$ \\\\ ## &amp; (0.024) \\\\ ## Constant &amp; 103.136$^{***}$ \\\\ ## &amp; (11.264) \\\\ ## N &amp; 2605 \\\\ ## R-squared &amp; 0.509 \\\\ ## Adj. R-squared &amp; 0.509 \\\\ ## Residual Std. Error &amp; 2626.691 (df = 2603) \\\\ ## F Statistic &amp; 2702.978$^{***}$ (df = 1; 2603) \\\\ ## \\hline \\\\[-1.8ex] ## \\multicolumn{2}{l}{$^{***}$p $&lt;$ .01; $^{**}$p $&lt;$ .05; $^{*}$p $&lt;$ .1} \\\\ ## \\end{tabular} ## \\end{table} Para el modelo ajustado con factores de expansión usando la función lm, el \\(\\hat{\\beta}_{0}\\) es 103.14 y el \\(\\hat{\\beta}_{1}\\) asociado a la variable gastos es 1.26. Ahora, haciendo el mismo ajuste pero usando la función svyglm: fit_svy &lt;- svyglm(Income ~ Expenditure, design = diseno, family=stats::gaussian()) fit_svy ## Stratified 1 - level Cluster Sampling design (with replacement) ## With (238) clusters. ## Called via srvyr ## Sampling variables: ## - ids: PSU ## - strata: Stratum ## - weights: wk ## ## Call: svyglm(formula = Income ~ Expenditure, design = diseno, family = stats::gaussian()) ## ## Coefficients: ## (Intercept) Expenditure ## 103.136 1.263 ## ## Degrees of Freedom: 2604 Total (i.e. Null); 118 Residual ## Null Deviance: 634600000 ## Residual Deviance: 311300000 AIC: 38280 Obteniendo estimaciones para el \\(\\hat{\\beta}_{0}\\) es 103.14 y el \\(\\hat{\\beta}_{1}\\) asociado a la variable gastos es 1.26. Siendo exactamente las mismas que con la función lm ya que, como se definió en los argumentos de la función, la función de enlace es Gausiana. Por último y a modo de resumen se muestra un gráfico donde se encuentran depositados todos los modelos estimados anteriormente y así poder comparar de manera gráfica su ajuste: df_model &lt;- data.frame( intercept = c(coefficients(fit)[1], coefficients(fit_sinP)[1], coefficients(fit_Ponde)[1], coefficients(fit_svy)[1]), slope = c(coefficients(fit)[2], coefficients(fit_sinP)[2], coefficients(fit_Ponde)[2], coefficients(fit_svy)[2]), Modelo = c(&quot;Población&quot;, &quot;Sin ponderar&quot;, &quot;Ponderado(lm)&quot;, &quot;Ponderado(svyglm)&quot;)) plot_BigCity + geom_abline( data = df_model, mapping = aes( slope = slope, intercept = intercept, linetype = Modelo, color = Modelo ), size = 2 ) "],["diagnóstico-del-modelo.html", "6.2 Diagnóstico del modelo", " 6.2 Diagnóstico del modelo En el análisis de las encuestas de hogares cuando se ajusten modelo estadístico es importante realizar verificaciones de calidad y con esto tener certezas de las conclusiones que se obtienen. La mayoría de textos académicos dan un panorama bastante detallado de los supuestos y consideraciones que se deben tener en cuenta para tener un modelo correctamente definido. A continuación, se enlistan algunas de ellas (Tellez, 2016) Determinar si el modelo proporciona un adecuado ajuste a los datos. Examinar si los errores están normalmente distribuidos. Examinar si los errores tienen varianza constante. Verificar si los errores se pueden asumir no correlacionados. Determinar si alguno de los datos tiene valores con un efecto inusualmente grande sobre el modelo de regresión estimado, estos se conocen como datos influyentes. Determinar si algún punto no sigue la tendencia de la mayoría de los datos cuando se toma en cuenta el modelo, estos puntos se conocen como outliers. En este capítulo se abordarán alguno de los supuestos que se deben tener en cuenta al momento de ajustar un modelo de regresión lineal. Estimación del \\(R^{2}\\) y \\(R_{adj}^{2}\\) Una medida del ajuste del modelo de regresión es el coeficiente de determinación o coeficiente de correlación múltiple (\\(R^{2})\\)). Dicho parámetro estima la proporción de la varianza de la población explicada por la regresión y oscila entre 0 y 1. Entre más cercano esté de uno significa que mayor variabilidad explica y lo contrario ocurrirá si está cerca de cero. Lo anterior, en ocasiones es muy ambigüo puesto que por ejemplo, los físicos pueden obtienen \\(R^{2}\\) altísimos (mayores a 0.98–0.99) mientras que los químicos obtienen \\(R^{2}\\) superiores a 0.90. Sin embargo, los científicos sociales y demás que trabajen con poblaciones humanas encontrarán que su mejor modelo de regresión a menudo explicará solo entre el 20 % y el 40 % de la variación en la variable dependiente (Heringa). A continuación, se presenta como se calcula este coeficiente: \\[ R^{2} = 1-\\frac{SSE}{SST} \\] Donde, SST es la suma de cuadrados totales y SSE es la suma de cuadrados del error. El estimador de este parámetro usando muestras complejas está dado por: \\[ R_{\\omega}^{2} = 1-\\frac{WSSE}{WSST} \\] Donde, WSST es la variabilidad total estimada y WSSE es la suma de cuadrados estimada y se define como: \\[ \\hat{WSSE_{\\omega}} = \\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-x_{h\\alpha i}\\hat{\\beta}\\right)^{2} \\] Por último, \\(R_{adj}^{2}\\) se estima: \\[ R_{adj}^{2} = 1-\\frac{\\left(n-1\\right)}{\\left(n-p\\right)}R_{\\omega}^{2} \\] Para continuar con los modelos ajustados en la sección anterior, se procede a estimar los \\(R^{2}\\) utilizando R. Inicialmente, se procede a estimar los parámetros del modelo utilizando la función svyglm de survey como se mostró anteriormente y también, se ajusta un modelo solo con el intercepto para obtener la estimación de la SST: fit_svy &lt;- svyglm(Income ~ Expenditure, design = diseno) modNul &lt;- svyglm(Income ~ 1, design = diseno) s1 &lt;- summary(fit_svy) s0 &lt;-summary(modNul) WSST&lt;- s0$dispersion WSSE&lt;- s1$dispersion Por tanto, la estimación del \\(R^{2}\\) es: R2 = 1- WSSE/WSST R2 ## variance SE ## [1,] 0.50942 19005 y, para estimar el \\(R_{adj}^{2}\\) se requiere definir el diseño muestral pero incluyendo los q-weigthed (Pffeferman, 2011). A continuación, se muestra los pasos para encontrar los q-weigthed: Ajustar un modelo de regresión a los pesos finales de la encuesta utilizando las variables predictoras en el modelo de regresión de interés. fit_Nul &lt;- lm(wk ~ 1, data = encuesta) Obtener las predicciones de los pesos de la encuesta para cada caso como una función de las variables predictoras en el conjunto de datos qw &lt;- predict(fit_Nul) Dividir los pesos finales de la encuesta por los valores predichos en el paso anterior: encuesta %&lt;&gt;% mutate(wk1 = wk/qw) Usar los nuevos pesos obtenidos para el ajuste de los modelos de regresión: diseno_qwgt &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk1, nest = T) Ahora bien, una vez definido el diseño muestral con los nuevos pesos q-weigthed, se procede a calcular el \\(R_{adj}^{2}\\) como sigue: n = sum(diseno_qwgt$variables$wk) p&lt;- 2 R2Adj = 1-( ( (n-1)/(n-p) )*R2 ) R2Adj ## variance SE ## [1,] 0.49058 19005 Como se puede observar, el \\(R_{adj}^{2}\\) es un poco más bajo que el \\(R^{2}\\) y cercanos al 50% que como se comentó anteriormente, dependiendo del contexto del problema se podrá concluir si es grande o pequeño. Después de realizar la comparación entre las diferentes formas de estimar los coeficientes del modelo se opta por la metodología consolidadas en svyglm: diseno_qwgt %&lt;&gt;% mutate(Age2 = Age^2) mod_svy &lt;- svyglm( Income ~ Expenditure + Zone + Sex + Age2 , design = diseno_qwgt) s1 &lt;- summary(mod_svy) s0 &lt;- summary(modNul) mod_svy Stratified 1 - level Cluster Sampling design (with replacement) With (238) clusters. Called via srvyr Sampling variables: - ids: PSU - strata: Stratum - weights: wk1 Call: svyglm(formula = Income ~ Expenditure + Zone + Sex + Age2, design = diseno_qwgt) Coefficients: (Intercept) Expenditure ZoneUrban SexMale Age2 62.18419 1.22548 63.46000 21.73256 0.00852 Degrees of Freedom: 2604 Total (i.e. Null); 115 Residual Null Deviance: 634600000 Residual Deviance: 3.08e+08 AIC: 38260 stargazer(mod_svy, header = FALSE,single.row = T, title = &quot;Modelo propuesto&quot;, style = &quot;ajps&quot;, omit.stat=c(&quot;bic&quot;, &quot;ll&quot;)) Diagnósticos de los residuales En el diagnóstico de los modelos es crucial el análisis de los residuales. Estos análisis proporcionan, bajo el supuesto que el modelo ajustado es adecuado, una estimación de los errores. Por tanto, un estudio cuidadoso de los residuales deberá ayudar al investigador a concluir si el procedimiento de ajuste no ha violado los supuestos o si, por el contrario, uno o varios de los supuestos no se verifican y hay necesidad de revisar el procedimiento de ajuste. Para realizar el análisis de los residuales, en primera instancia, se definen los residuales de Pearson como sigue (Heeringa) \\[ r_{p_{i}} = \\left(y_{i}-\\mu_{i}\\left(\\hat{\\beta}_{\\omega}\\right)\\right)\\sqrt{\\frac{\\omega_{i}}{V\\left(\\hat{\\mu}_{i}\\right)}} \\] Donde, \\(\\mu_{i}\\) es el valor esperado de \\(y_{i}\\), \\(w_{i}\\) es la ponderación de la encuesta para el i-ésimo individuo del diseño muestral complejo, Por último, \\(V(\\mu_{i})\\) es la función de varianza del resultado. Otra definición que se debe tener en consideración para el análisis de los residuales es el de la matriz hat, la cual se estima como: \\[ H = W^{1/2}X\\left(X&#39;WX\\right)^{-1}X&#39;W^{1/2} \\] donde, \\[ W = diag\\left\\{ \\frac{\\omega_{1}}{V\\left(\\mu_{1}\\right)\\left[g&#39;\\left(\\mu_{1}\\right)\\right]^{2}},...,\\frac{\\omega_{n}}{V\\left(\\mu_{n}\\right)\\left[g&#39;\\left(\\mu_{n}\\right)\\right]^{2}}\\right\\} \\] \\(W\\) es una matriz diagonal de \\(n\\times n\\) y \\(g()\\) es la función de enlace del modelo lineal generalizado. Otras técnicas utilizadas también para el análisis de los modelos consisten en el análisis de observaciones influyentes. Una observación se denomina influyente si al removerlo de la nube de puntos este causa un cambio grande en el ajuste del modelo. Una observación importante para resaltar es que un punto influyente podría o no ser un dato atípico. Para detectar observaciones influyentes es necesario tener claro qué tipo de influencia se quiere detectar. Lo anterior puesto que, por ejemplo, una observación puede ser influyente sobre la estimación de los parámetros, pero no para la estimación de la varianza del error. A continuación, se presentan las distintas técnicas estadísticas para la detección de datos influyentes: Distancia de Cook’s: Diagnostica si la i-ésima observación es influyente en la estimación del modelo, por estar lejos del centro de masa de los datos. Se calcula de la siguiente manera: \\[ c_{i}=\\frac{w_{i}^{*}w_{i}e_{i}^{2}}{p\\phi V\\left(\\hat{\\mu}_{i}\\right)\\left(1-h_{ii}\\right)^{2}}\\boldsymbol{x}_{i}^{t}\\left[\\widehat{Var}\\left(U_{w}\\left(\\hat{\\boldsymbol{B}}_{w}\\right)\\right)\\right]^{-1}\\boldsymbol{x}_{i} \\] donde, \\(w_i^* =\\) Pesos de la encuesta. \\(w_i\\) Elementos por fuera de la diagonal de la matriz hat \\(e_i=\\) residuales \\(p=\\) número de parámetros del Modelo de regresión. \\(\\phi =\\) parámetro de dispersión en el glm \\(\\widehat{Var}\\left(U_{w}\\left(\\hat{\\boldsymbol{B}}_{w}\\right)\\right) =\\) estimación de varianza linealizada de la ecuación de puntuación, que se utiliza para pseudo MLE en modelos lineales generalizados ajustados a datos de encuestas de muestras complejas. Luego del cálculo de la distancia de Cook’s para las observaciones de la muestra, se procede a calcular el siguiente estadístico de prueba para evaluar la importancia de la estadística: \\[ \\frac{\\left(df-p+1\\right)\\times c_{i}}{df} \\doteq F_{\\left(p,df-p\\right)} \\] donde \\(df=\\) grados de liberta basados en el diseño. Por otro lado, la literatura como Tellez (2016), Heeringa considera a las observaciones influyentes cuando \\(c_{i}\\) sean mayores a 2 o 3. \\(D_fBeta_{(i)}\\): Este estadístico mide el cambio en la estimación del vector de coeficientes de regresión cuando la i-ésima observación es eliminada. Se evalúa con la siguiente expresión: \\[ D_fBeta_{(i)} = \\hat{B}-\\hat{B}_{\\left(i\\right)}=\\frac{\\boldsymbol{A}^{-1}\\boldsymbol{X}_{\\left(i\\right)}^{t}\\hat{e}_{i}w_{i}}{1-h_{ii}} \\] Donde \\(\\boldsymbol{A} =\\boldsymbol{X}^{t}\\boldsymbol{WX}\\) \\(\\hat{B}_{(i)}\\) es el vector de parámetros estimados una vez se ha eliminado la i-ésima observación, \\(h_{ii}\\) es el correspondiente elemento de la diagonal de H y \\(\\hat{e}_i\\) es el residual de la i-ésima observación. Otra forma de reescribir este estadístico en términos de la matriz \\(H\\) es: \\[ D_fBetas_{\\left(i\\right)}=\\frac{{c_{ji}e_{i}}\\big/{\\left(1-h_{ii}\\right)}}{\\sqrt{v\\left(\\hat{B}_{j}\\right)}} \\] donde: \\(c_{ji}=\\) es el ji-estimo elemento de \\(\\boldsymbol{A}^{-1}w_{i}^{2}\\boldsymbol{X}_{\\left(i\\right)}\\boldsymbol{X}_{\\left(i\\right)}^{t}\\boldsymbol{A}^{-1}\\) El estimador de \\(v\\left(\\hat{B}_{j}\\right)\\) basado en el Modelo se obtiene como: \\(v_{m}\\left(\\hat{B}_{j}\\right)=\\hat{\\sigma}\\sum_{i=1}^{n}c_{ji}^{2}\\) con \\(\\hat{\\sigma}=\\sum_{i\\in s}w_{i}e^2/ \\left( \\hat{N} - p \\right)\\) y \\(\\hat{N} = \\sum_{i \\in s}w_{i}\\) La i-ésima observación es influyente para \\(B_j\\) si \\(\\mid D_{f}Betas_{\\left(i\\right)j}\\mid\\geq\\frac{z}{\\sqrt{n}}\\) con \\(z=\\) 2 o 3 Como alternativa puede usar \\(t_{0.025,n-p}/\\sqrt(n)\\) donde \\(t_{0.025,n-p}\\) es el percentil \\(97.5\\) Estadístico \\(D_{f}Fits_{\\left(i\\right)}\\): Este estadístico mide el cambio en el ajuste del modelo cuando se elimina el registro i-ésimo. Se calcula de la siguiente manera: \\[ D_{f}Fits_{\\left(i\\right)}= \\frac{h_{ii}e_{i}\\big/\\left(1-h_{ii}\\right)}{\\sqrt{v\\left(\\hat{\\beta}_{j}\\right)}} \\] Donde, \\(\\sqrt{v\\left(\\hat{\\beta}_{j}\\right)}\\) puede ser aproximada por el diseño o el Modelo. La i-ésima observación se considera influyente en el ajuste del Modelo si \\(\\mid DfFits\\left(i\\right)\\mid\\geq z\\sqrt{\\frac{p}{n}}\\) con \\(z =\\) 2 o 3 Por otro lado, un análisis que es de vital importancia en el ajuste de modelos de regresión más específicamente en el análisis de residuales es el de varianza constante en los errores. La principal consecuencia de no tener en cuenta la violación de este supuesto es que los estimadores pierden eficiencia. Si el supuesto de varianza constante no se cumple, los estimadores siguen siendo insesgados y consistentes, pero dejan de ser eficientes, es decir, dejan de ser los mejores en cuanto a que ya no tienen la menor varianza entre todos los estimadores insesgados. Como consecuencia de lo anterior, los intervalos de confianza serán más amplios y las pruebas t y F darán resultados imprecisos (Tellez, 2016). Una de las formas de analizar el supuesto de varianzas constantes en los errores es hacerlo de manera gráfica. Para ello, se grafica los residuos del modelo contra \\(\\hat{y}\\) o los residuos del modelo contra \\(X_{i}\\). Si al realizar estos gráficos se logra evidenciar un patrón (funciones cuadráticas, cúbicas, logarítmicas, etc), se puede decir que la varianza de los errores no es constante. Otro supuesto que se debe revisar en los errores al momento de realizar ajustes es la normalidad en lo errores. Una forma muy común para hacer dicha evaluación es realizar un gráfico cuantil-cuantil normal o QQplot. El QQplot es una gráfica de cuantiles para los residuos observados frente a los calculados a partir de una distribución normal teórica que tiene la misma media y varianza que la distribución de los residuos observados. Por lo tanto, una línea recta de 45° en este gráfico sugeriría que la normalidad es una suposición razonable para los errores aleatorios en el modelo. A manera de ejemplificar los conceptos vistos, se van a utilizar los modelos previamente ajustados. En primero instancia, el análisis del modelo se centrará en los supuestos de normalidad y varianza constante en los errores. Primero, se realizará el análisis de la normalidad en los errores de manera gráfica como se muestra a continuación: par(mfrow = c(2,2)) plot(mod_svy) Como se puedo observar en el QQplot, hay evidencia gráfica de que los errores no se distribuyen según una distribución normal. La librería svydiags está pensada en ayudar en el diagnostico de modelos de regresión lineal, siendo una extensión más para complementar el paquete survey. Con las librerías svydiags se extraen los residuales estandarizados como sigue: library(svydiags) stdresids = as.numeric(svystdres(mod_svy)$stdresids) diseno_qwgt$variables %&lt;&gt;% mutate(stdresids = stdresids) Podemos hacer el análisis de normalidad también por medio del histograma de los residuales estandarizados como sigue: ggplot(data = diseno_qwgt$variables, aes(x = stdresids)) + geom_histogram(aes(y = ..density..), colour = &quot;black&quot;, fill = &quot;blue&quot;, alpha = 0.3) + geom_density(size = 2, colour = &quot;blue&quot;) + geom_function(fun = dnorm, colour = &quot;red&quot;, size = 2) + theme_cepal()+labs(y = &quot;&quot;) y como se puede observar gráficamente los errores no siguen una distribución normal. Por otro lado, el otro análisis que se realiza de manera gráfica es el de varianzas constantes el cual se realizará a continuación: Primero, agreguemos las predicciones a la base de datos para poder realizar las gráficas. library(patchwork) diseno_qwgt$variables %&lt;&gt;% mutate(pred = predict(mod_svy)) g2 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Expenditure, y = stdresids))+ geom_point() + geom_hline(yintercept = 0) + theme_cepal() g3 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Age2, y = stdresids))+ geom_point() + geom_hline(yintercept = 0) + theme_cepal() g4 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Zone, y = stdresids))+ geom_point() + geom_hline(yintercept = 0) + theme_cepal() g5 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Sex, y = stdresids))+ geom_point() + geom_hline(yintercept = 0) + theme_cepal() (g2|g3)/(g4|g5) Como se puede observar en las gráficas de gastos y edad, ambas muestran tendencias y no un comportamiento aleatorio. Por lo anterior, se puede decir que las varianzas no son constantes. Otros de os análisis a realizar es revisar si existen datos influyentes en la base de datos. Para ejemplificar los conceptos definidos, se seguirán con los modelos ajustados en la sección anterior. Una vez ajustados estos modelos y verificados los supuestos, se procede a hacer el cálculo de la distancia de Cook’s usando la función svyCooksDdel paquete svydiags como sigue: library(svydiags) d_cook = data.frame( cook = svyCooksD(mod_svy), id = 1:length(svyCooksD(mod_svy))) table(d_cook$cook&gt;3) ggplot(d_cook, aes(y = cook, x = id)) + geom_point() + theme_bw(20) Como se puede observar, ninguna de las distancias de Cook’s es mayor a 3 por lo que, podemos decir que no existen observaciones influyentes. Ahora bien, se desea observar si hay observaciones influyentes pero utilizando \\(D_{f}Betas_{\\left(i\\right)j}\\) se realiza con la función svydfbetas como se muestra a continuación: d_dfbetas = data.frame(t(svydfbetas(mod_svy)$Dfbetas)) colnames(d_dfbetas) &lt;- paste0(&quot;Beta_&quot;, 1:5) d_dfbetas %&gt;% slice(1:10L) Beta_1 Beta_2 Beta_3 Beta_4 Beta_5 0.0005544 -0.0002176 0.0020984 -0.0045432 -0.0077196 -0.0005745 -0.0001378 0.0013896 0.0026485 -0.0030881 -0.0008522 -0.0000546 0.0008822 0.0022298 0.0007935 -0.0003870 -0.0000551 0.0011523 -0.0031478 0.0006928 -0.0008850 -0.0000401 0.0007854 0.0021273 0.0014197 0.0009195 0.0005560 -0.0036343 -0.0062969 0.0098007 0.0027031 0.0003880 -0.0031253 -0.0075726 -0.0027665 0.0010931 0.0002906 -0.0028323 0.0076881 -0.0042777 0.0030304 0.0003560 -0.0030245 -0.0077961 -0.0051035 -0.0003135 0.0003715 0.0012078 -0.0036874 -0.0039914 Una vez calculado los \\(D_{f}Betas_{\\left(i\\right)j}\\) se procede a acomodar la salida con para verificar cuáles observaciones son influyentes. Para esto, de calcula el umbral (cutoff) para definir si es o no influyente la observación. Ese umbral es tomado de las salidas de la función svydfbetas. Por último, se genera una variable dicotómica que indique si la observación es o no influyente como se muestra a continuación: d_dfbetas$id &lt;- 1:nrow(d_dfbetas) d_dfbetas &lt;- reshape2::melt(d_dfbetas, id.vars = &quot;id&quot;) cutoff &lt;- svydfbetas(mod_svy)$cutoff d_dfbetas %&lt;&gt;% mutate( Criterio = ifelse(abs(value) &gt; cutoff, &quot;Si&quot;, &quot;No&quot;)) tex_label &lt;- d_dfbetas %&gt;% filter(Criterio == &quot;Si&quot;) %&gt;% arrange(desc(abs(value))) %&gt;% slice(1:10L) tex_label id variable value Criterio 889 Beta_1 0.2781445 Si 890 Beta_2 -0.2592704 Si 891 Beta_1 0.2558846 Si 889 Beta_2 -0.2537377 Si 891 Beta_2 -0.2490529 Si 890 Beta_1 0.2456031 Si 2311 Beta_5 0.2056438 Si 889 Beta_5 -0.1992701 Si 890 Beta_5 -0.1787722 Si 890 Beta_4 0.1615834 Si Como se pudo observar en la salida anterior hay varias observaciones que resultan influyentes dado el criterio del \\(D_{f}Betas_{\\left(i\\right)j}\\). A continuación, y de manera ilustrativa, se grafican los \\(D_{f}Betas_{\\left(i\\right)j}\\) y el umbral con el fin de ver de manera gráfica aquellas observaciones influyentes, teniendo en cuenta que, aquellos puntos rojos en la gráfica representan observaciones influyentes. ggplot(d_dfbetas, aes(y = abs(value), x = id)) + geom_point(aes(col = Criterio)) + geom_text(data = tex_label, angle = 45, vjust = -1, aes(label = id)) + geom_hline(aes(yintercept = cutoff)) + facet_wrap(. ~ variable, nrow = 2) + scale_color_manual( values = c(&quot;Si&quot; = &quot;red&quot;, &quot;No&quot; = &quot;black&quot;)) + theme_cepal() Si el objetivo ahora es detectar observaciones influyentes pero considerando ahora la estadística \\(D_{f}Fits_{\\left(i\\right)}\\), se utiliza la función svydffits y se siguen los mismos pasos mostrados para el estadístico \\(D_{f}Betas_{\\left(i\\right)j}\\): d_dffits = data.frame( dffits = svydffits(mod_svy)$Dffits, id = 1:length(svydffits(mod_svy)$Dffits)) cutoff &lt;- svydffits(mod_svy)$cutoff d_dffits %&lt;&gt;% mutate(C_cutoff = ifelse(abs(dffits) &gt; cutoff, &quot;Si&quot;, &quot;No&quot;)) ggplot(d_dffits, aes(y = abs(dffits), x = id)) + geom_point(aes(col = C_cutoff)) + geom_hline(yintercept = cutoff) + scale_color_manual( values = c(&quot;Si&quot; = &quot;red&quot;, &quot;No&quot; = &quot;black&quot;))+ theme_cepal() Como se puede observar en el gráfico anterior, también hay observaciones influyentes utilizando \\(D_{f}Fits_{\\left(i\\right)}\\), las cuales se muestran en rojo en el gráfico. Un último acercamiento que se trabajará en este texto para la detección de datos influyentes está encaminado al uso de la matriz H. En este sentido, la matriz asociada al Estimador de Pseudo Máxima Verosimilitud (PMLE) de \\(\\hat{\\boldsymbol{B}}\\) es \\(\\boldsymbol{H}=\\boldsymbol{XA}^{-1}\\boldsymbol{X}^{-t}\\boldsymbol{W}\\) cuya diagonal esta dado por \\(h_{ii} = \\boldsymbol{x_{i}^tA}^{-1}\\boldsymbol{x_{i}}^{-t}w_{i}\\). Utilizando la matriz H, una observación puede ser grande y, como resultado, influir en las predicciones, cuando un \\(x_i\\) es considerablemente diferente del promedio ponderado \\(\\bar{x}_w=\\sum_{i\\in s}w_{i}\\boldsymbol{x_{i}}\\big/\\sum_{i\\in s}w_i\\). Según (Tellez, 2016) una observación es considerada grande si es mayor a tres veces el promedio de los \\(h_{ii}\\). A continuación, se muestra el procedimiento en R cuya función a utilizar es svyhat: vec_hat &lt;- svyhat(mod_svy, doplot = FALSE) d_hat = data.frame(hat = vec_hat, id = 1:length(vec_hat)) d_hat %&lt;&gt;% mutate(C_cutoff = ifelse(hat &gt; (3 * mean(hat)),&quot;Si&quot;, &quot;No&quot;)) ggplot(d_hat, aes(y = hat, x = id)) + geom_point(aes(col = C_cutoff)) + geom_hline(yintercept = (3 * mean(d_hat$hat))) + scale_color_manual( values = c(&quot;Si&quot; = &quot;red&quot;, &quot;No&quot; = &quot;black&quot;))+ theme_cepal() Dado que esta última técnica es empírica, se puede observar en el gráfico anterior que hay varias observaciones posiblemente influyentes en el conjunto de datos de la muestra de hogares. "],["inferencia-sobre-los-parámetros-del-modelo.html", "6.3 Inferencia sobre los parámetros del Modelo", " 6.3 Inferencia sobre los parámetros del Modelo Una vez evaluado el correcto ajuste del modelo utilizando las metodologías vistas anteriormente y corroborando las propiedades distribucionales de los errores y por ende, de la variable respuesta \\(y\\), el paso siguiente es verificar si los parámetros estimados son significativos y por ende, las covariables utilizadas para ajustar el modelo aportan significativamente para explicar o predecir a la variable de estudio \\(y\\). Dada las propiedades distribucionales de los \\(\\beta&#39;s\\), un estadístico de prueba natural para evaluar la significancia de dicho parámetro se basa en la distribución “t-student” y se describe a continuación: \\[ t=\\frac{\\hat{\\beta}_{k}-\\beta_{k}}{se\\left(\\hat{\\beta}_{k}\\right)}\\sim t_{n-p} \\] Donde \\(p\\) es el número de parámetros del modelo y \\(n\\) el tamaño de la muestra de la encuesta. En este sentido, el estadístico de prueba anterior evalúa las hipótesis \\(H_{0}:\\beta_{k}=0\\) versus la alternativa \\(H_{1}:\\beta_{k}\\neq0\\). De las propiedades distribucionales de los \\(\\beta\\), se puede construir un intervalo de confianza al \\((1-\\alpha)\\times100\\%\\) para \\(\\beta_{k}\\) está dado por: \\[ \\hat{\\beta}_{k}\\pm t_{1-\\frac{\\alpha}{2},\\,df}\\,se\\left(\\hat{\\beta}_{k}\\right) \\] Donde, los grados de libertad para el intervalo (\\(df\\)) en una encuesta de hogares (muestras complejas) está dado por el número de conglomerados finales de la primera etapa menos el número de estratos de la etapa primaria \\(\\left(df=\\sum_{h}a_{h}-H\\right)\\). Para la aplicación de las temáticas vistas, es decir, realizar la prueba de hipótesis y los intervalos de confianza para los parámetros utilizaremos el modelo que se ha venido trabajando y aplicaremos las funciones summary.svyglm para las pruebas t y confint.svyglm para los intervalos de confianza como sigue: survey:::summary.svyglm(mod_svy) ## ## Call: ## svyglm(formula = Income ~ Expenditure + Zone + Sex + Age2, design = diseno_qwgt) ## ## Survey design: ## Called via srvyr ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 62.18419 62.98928 0.987 0.326 ## Expenditure 1.22548 0.19798 6.190 9.55e-09 *** ## ZoneUrban 63.46000 40.09025 1.583 0.116 ## SexMale 21.73256 15.78081 1.377 0.171 ## Age2 0.00852 0.00557 1.530 0.129 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 118297.1) ## ## Number of Fisher Scoring iterations: 2 survey:::confint.svyglm(mod_svy) 2.5 % 97.5 % (Intercept) -62.5854545 186.9538331 Expenditure 0.8333309 1.6176354 ZoneUrban -15.9510648 142.8710655 SexMale -9.5261885 52.9913044 Age2 -0.0025129 0.0195537 De lo anterior se puede observar que, con una confianza del 95% el único parámetro significativo del modelo es Expenditure y ese mismo resultado lo reflejan los intervalos de confianza. Estimación de una observación Los modelos de regresión lineales, según (Neter et al., 1996)., son utilizado esencialmente con 2 fines, el primero es tratar de explicar la variable respuesta en términos de covariables que pueden encontrarse en la encuesta o en registros administrativos, censos, etc. Adicionalmente, también son usados para predecir valores de la variable en estudio ya sea dentro del intervalo de valores recogidos en la muestra o por fuera de dicho intervalo. Lo primero se ha abordado a lo largo de todo el capítulo y lo segundo se obtiene de la siguiente manera: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=\\boldsymbol{x}_{obs,i}\\hat{\\boldsymbol{\\beta}} \\] De manera explícita, si se ajusta un modelo con 4 covariables la expresión sería: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{1i}+\\hat{\\beta}_{2}x_{2i}+\\hat{\\beta}_{3}x_{3i}+\\hat{\\beta}_{4}x_{4i} \\] La varianza de la estimación se calcula de la siguiente manera: \\[ var\\left(\\hat{E}\\left(y_{i}\\mid x_{obs,i}\\right)\\right) = x&#39;_{obs,i}cov\\left(\\hat{\\beta}\\right)x{}_{obs,i} \\] A continuación, se presenta cómo se realiza la estimación del valor esperado, primero se estiman los parámetros del modelo: term estimate std.error statistic p.value (Intercept) 62.1841893 62.9892815 0.9872186 0.3256080 Expenditure 1.2254831 0.1979760 6.1900606 0.0000000 ZoneUrban 63.4600003 40.0902478 1.5829286 0.1161841 SexMale 21.7325579 15.7808095 1.3771510 0.1711403 Age2 0.0085204 0.0055701 1.5296627 0.1288463 Por lo anterior, la estimación del valor esperado o predicción queda: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=62.2+1.2x_{1i}+63.5x_{2i}+21.7x_{3i}+0.01x_{4i} \\] Para calcular la varianza de la estimación, primero se deben obtener las varianzas de la estimación de los parámetros: vcov(mod_svy) (Intercept) Expenditure ZoneUrban SexMale Age2 (Intercept) 3967.6495831 -11.3101288 275.9857536 312.4533433 -0.1851869 Expenditure -11.3101288 0.0391945 -3.5525825 -0.5004199 0.0005145 ZoneUrban 275.9857536 -3.5525825 1607.2279681 -130.7107596 -0.0559166 SexMale 312.4533433 -0.5004199 -130.7107596 249.0339481 -0.0043208 Age2 -0.1851869 0.0005145 -0.0559166 -0.0043208 0.0000310 Ahora bien, se procede a realizar los cálculos como lo indica la expresión mostrada anteriormente: xobs &lt;- model.matrix(mod_svy) %&gt;% data.frame() %&gt;% slice(1) %&gt;% as.matrix() cov_beta &lt;- vcov(mod_svy) %&gt;% as.matrix() as.numeric(xobs %*% cov_beta %*% t(xobs)) ## [1] 1920.837 Si el objetivo ahora es calcular el intervalo de confianza para la predicción se utiliza la siguiente ecuación: \\[ \\boldsymbol{x}_{obs,i}\\hat{\\beta}\\pm t_{\\left(1-\\frac{\\alpha}{2},n-p\\right)}\\sqrt{var\\left(\\hat{E}\\left(y_{i}\\mid\\boldsymbol{x}_{obs,i}\\right)\\right)} \\] Para realizar los cálculos en R, se utiliza la función confint y predict como sigue: pred &lt;- data.frame(predict(mod_svy, type = &quot;link&quot;)) pred_IC &lt;- data.frame(confint(predict(mod_svy, type = &quot;link&quot;))) colnames(pred_IC) &lt;- c(&quot;Lim_Inf&quot;, &quot;Lim_Sup&quot;) pred_IC Ahora, de manera gráfica las predicciones e intervalos se vería de la siguiente manera: pred &lt;- bind_cols(pred, pred_IC) pred$Expenditure &lt;- encuesta$Expenditure pred %&gt;% slice(1:6L) pd &lt;- position_dodge(width = 0.2) ggplot(pred %&gt;% slice(1:100L), aes(x = Expenditure , y = link)) + geom_errorbar(aes(ymin = Lim_Inf, ymax = Lim_Sup), width = .1, linetype = 1) + geom_point(size = 2, position = pd) + theme_bw() Por último, si el interés es hacer una predicción fuera del rango de valores que fue capturado en la muestra. Para esto, supongamos que se desea predecir: datos_nuevos &lt;- data.frame(Expenditure = 1600, Age2 = 40^2, Sex = &quot;Male&quot;, Zone = &quot;Urban&quot;) La varianza para la predicción se hace siguiendo la siguiente ecuación: \\[ var\\left(\\hat{E}\\left(y_{i}\\mid\\boldsymbol{x}_{obs,i}\\right)\\right)=\\boldsymbol{x}_{obs,i}^{t}cov\\left(\\boldsymbol{\\beta}\\right)\\boldsymbol{x}_{obs,i} + \\hat{\\sigma}^2_{yx} \\] Por tanto, se construye la matriz de observaciones y se calcula la varianza como sigue: x_noObs = matrix(c(1,1600,1,1,40^2),nrow = 1) as.numeric(sqrt(x_noObs%*%cov_beta%*%t(x_noObs))) ## [1] 244.6294 Por último, el intervalo de confianza sigue la siguiente ecuación: \\[ \\boldsymbol{x}_{obs,i}\\hat{\\beta}\\pm t_{\\left(1-\\frac{\\alpha}{2},n-p\\right)}\\sqrt{var\\left(\\hat{E}\\left(y_{i}\\mid\\boldsymbol{x}_{obs,i}\\right)\\right)+\\hat{\\sigma}_{yx}^{2}} \\] En R se hace la predicción de la siguiente manera: predict(mod_svy, newdata = datos_nuevos, type = &quot;link&quot;) ## link SE ## 1 2121.8 244.63 y el intervalo: confint(predict(mod_svy,newdata = datos_nuevos)) 2.5 % 97.5 % 1642.318 2601.247 "],["gráficas-en-r.html", "Capítulo 7 Gráficas en R", " Capítulo 7 Gráficas en R El objetivo de esta capítulo es mostrarle al lector cómo hacer gráficos generales en R. En todo análisis de encuestas, el componente gráfico es fundamental para revisar tendencias en algunas variables de interés. También son muy necesarias las gráficas cuando se el objetivo es chequear algunos supuestos en el ajustes de modelo, por ejemplo, varianza constante en los errores, normalidad, etc. Uno de los paquetes más usados para graficar en R es ggplot2 el cual es un paquete potente y flexible, implementado por Hadley Wickham, para producir gráficos elegantes. El gg en ggplot2 significa Grammar of Graphics, el cual es un concepto gráfico que describe gráficos usando gramática. Como es de costumbre, se inicia este capítulo cargando las librerías y bases de datos: # knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, error = FALSE) options(digits = 4) library(survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(ggplot2) library(patchwork) El cargue de la base de datos se hace a continuación, data(BigCity, package = &quot;TeachingSampling&quot;) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) A continuación, se define el diseño de muestreo: library(srvyr) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) A partir de las variables de la encuesta, para efectos de los ejemplos, se definen las siguientes variables: diseno &lt;- diseno %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0), edad_18 = case_when( Age &lt; 18 ~ &quot;&lt; 18 años&quot;, TRUE ~ &quot;&gt;= 18 años&quot; ) ) Como se mostró en capítulos anteriores, se divide la muestra en sub grupos para ejemplificar los conceptos que se mostrarán en este capítulo: sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) Para crear las gráficas en este texto se utilizará por defecto el tema que la CEPAL tiene asignado por defecto. El tema se define a continuación: theme_cepal &lt;- function(...) { theme_light(10) + theme( axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), legend.position = &quot;bottom&quot;, legend.justification = &quot;left&quot;, legend.direction = &quot;horizontal&quot;, plot.title = element_text(size = 20, hjust = 0.5), ... ) } "],["histogramas-para-graficar-variables-continuas..html", "7.1 Histogramas para graficar variables continuas.", " 7.1 Histogramas para graficar variables continuas. Un histograma es una representación gráfica de los datos de una variable empleando rectángulos (barras) cuya altura es proporcional a la frecuencia de los valores representados y su ancho proporcional a la amplitud de los intervalos de la clase. Como se mencionó anteriormente, las gráficas se realizarán principalmente con la librería ggplot2y nos apoyamos en la librería patchwork para organizar la visual de las gráficas. A continuacuón, se presenta cómo realizar un histograma para la variable ingresos utilizando los factores de expansión de la encuesta. EN primera instancia se define la fuente de información (data), luego se definen la variable a graficar (x) y los pesos de muestreo (weight). Una vez definido los parámetros generales del gráfico se define el tipo de gráfico, que para nuestro caso como es un histograma es geom_histogram. Se definen los títulos que se quiere que tenga el histograma y por último, se aplica el tema de la CEPAL. plot1_Ponde &lt;- ggplot( data = encuesta, aes(x = Income, weight = wk)) + geom_histogram( aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot1_Ponde De forma análoga se define el siguiente histograma, note que en este caso se omitió el parámetro weight. Es decir, se genera un histograma sin pesos de muestreo: plot1_SinPonde &lt;- ggplot(encuesta, aes(x = Income)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() plot1_SinPonde Ahora bien, para efectos de comparación, se grafica la variable ingreso tomada de la población (BigCity) y se muestran los tres histogramas para notar las diferencias que tienen en comparación con el poblacional. plot1_censo &lt;- ggplot(BigCity, aes(x = Income)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 2500) plot1_censo | plot1_Ponde | plot1_SinPonde Por otro lado, repetimos ahora la secuencia de gráficos pero en este caso para la variable Expenditure: plot2_Ponde &lt;- ggplot( data = encuesta, aes(x = Expenditure, weight = wk) ) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot2_Ponde plot2_SinPonde &lt;- ggplot(data = encuesta, aes(x = Expenditure)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() plot2_SinPonde plot2_censo &lt;- ggplot(BigCity, aes(x = Expenditure)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) plot2_censo | plot2_Ponde | plot2_SinPonde Como conclusión, de ambos ejercicios, se puede observar que el histograma que mejor se aproxima al poblacional es aquel que utiliza los pesos de muestreo, aunque el gráfico que no los utiliza se aproxima bien y esto debido a la correcta selección de la muestra. Por otro lado, cuando el interés ahora es realizar comparaciones entre dos o más agrupaciones, es posible hacer uso del parámetro fill, el cual “rellena” las barras del histograma con diferentes colores según sea el grupo. Para este ejemplo, se van a graficar subgrupos por zonas: plot3_Ponde &lt;- ggplot( encuesta, aes(x = Income, weight = wk)) + geom_histogram( aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot3_Ponde Como se pudo observar en la generación del histograma, se utilizó el parámetro position el cual permite que las barras del gráfico sean distingibles. Ahora se graficará la misma variable pero esta vez sin los pesos de muestreo: plot3_SinPonde &lt;- ggplot(encuesta, aes(x = Income)) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() + ylab(&quot;&quot;) plot3_SinPonde Ahora, siguiendo el esquema de comparación anterior, se graficará la variable ingreso usando la información de la población y los subgrupos de zonas definidos anteriormente y por último, se muestran los 3 histogramas para poder compararlos: plot3_censo &lt;- ggplot(BigCity, aes(x = Income)) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) + ylab(&quot;&quot;) plot3_censo | plot3_Ponde | plot3_SinPonde Ahora, repetimos la secuencia de gráficos anteriores pero, para la variable Expenditure: plot4_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot4_Ponde Sin ponderar, plot4_SinPonde &lt;- ggplot( encuesta, aes(x = Expenditure) ) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() + ylab(&quot;&quot;) plot4_SinPonde Poblacional, plot4_censo &lt;- ggplot(BigCity, aes(x = Expenditure)) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) + ylab(&quot;&quot;) plot4_censo | plot4_Ponde | plot4_SinPonde Ahora, repetimos la secuencia de gráficos para la variable Income, pero hacemos las particiones por la variable sexo, Primero, hagamos el histogramas ponderado: plot5_Ponde &lt;- ggplot( encuesta, aes(x = Income, weight = wk) ) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot5_Ponde Sin ponderar, plot5_SinPonde &lt;- ggplot(encuesta, aes(x = Income)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() + ylab(&quot;&quot;) plot5_SinPonde Poblacional, plot5_censo &lt;- ggplot(BigCity, aes(x = Income)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) + ylab(&quot;&quot;) plot5_censo | plot5_Ponde | plot5_SinPonde Ahora, repetimos la secuencia de gráficos para la variable Expenditure desagregada por la variable sexo, primero, ponderado: plot6_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot6_Ponde Sin ponderar, plot6_SinPonde &lt;- ggplot(encuesta, aes(x = Expenditure)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() + ylab(&quot;&quot;) plot6_SinPonde Poblacional, plot6_censo &lt;- ggplot(BigCity, aes(x = Expenditure)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) + ylab(&quot;&quot;) plot6_censo | plot6_Ponde | plot6_SinPonde "],["agregando-densidades-y-graficando-boxplot.html", "7.2 Agregando densidades y graficando Boxplot", " 7.2 Agregando densidades y graficando Boxplot Dadas las cualidades de la librería ggplot2, se pueden agregar nuevas capas a los gráficos, particularmente, a los histogramas antes realizados. La densidad se agrega con el argumento geom_density y se incorpora el parámetro alpha que regula la transparencia del relleno. A continuacuón, se muestra cómo se agregan las densidades: plot1_Ponde + geom_density(fill = &quot;blue&quot;, alpha = 0.3) | plot2_Ponde + geom_density(fill = &quot;blue&quot;, alpha = 0.3) Ahora bien, al aplicar aes(fill = Zone) permite que la densidad sea agregada para cada una de las agrupaciones como se muestra a continución, plot3_Ponde + geom_density(aes(fill = Zone), alpha = 0.3) | plot4_Ponde + geom_density(aes(fill = Zone), alpha = 0.3) En está oportunidad se agrega la desnidad por sexo, plot5_Ponde + geom_density(aes(fill = Sex), alpha = 0.3) | plot6_Ponde + geom_density(aes(fill = Sex), alpha = 0.3) Boxplot El boxplot, diagrama de caja y bigotes, es un gráfico resumen presentado por John Tukey en 1977 que en la actualidad es ampliamente utilizado en la práctica estadística. En este diagrama se visualiza de forma general un conjunto de datos empleando el resumen de cinco números. La forma generada por este gráfico compuesto por un rectángulo (“caja”) y dos brazos (“bigotes”) suministra información sobre la relación ente los cuartiles (Q1, Q2 o mediana y Q3) y los valores mínimo y máximo, sobre la existencia de valores atípicos y la simetría de la distribución. Para realizar este gráfico en ggplot2 se utiliza la función geom_boxplot. A continuación, se presentan los Boxplot para las variables ingresos y gastos respectivamente: plot7_Ponde &lt;- ggplot( data = encuesta, aes(x = Income, weight = wk) ) + geom_boxplot() + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot8_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_boxplot() + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot7_Ponde | plot8_Ponde En los gráficos anteriores se puede observar que la variable ingresos tiene más variabilidad que la variable gastos. En ambos gráficos se observan datos atípicos. Estos diagramas también permiten la comparación entre dos o más niveles de agrupamiento, por ejemplo, por zonas para las variables ingresos y gastos como se muestra a continuación, plot9_Ponde &lt;- ggplot( encuesta, aes(x = Income, weight = wk) ) + geom_boxplot(aes(fill = Zone)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot10_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_boxplot(aes(fill = Zone)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot9_Ponde | plot10_Ponde Observándose, entre otros que, para la variable gasto en la zona rural es donde más datos atípico hay. Ahora, si se desea personalizar los colores del relleno debe hacer uso de la función scale_fill_manualcomo se muestra a continuación: colorZona &lt;- c(Urban = &quot;#48C9B0&quot;, Rural = &quot;#117864&quot;) plot9_Ponde + scale_fill_manual(values = colorZona) | plot10_Ponde + scale_fill_manual(values = colorZona) Para mayores colores, ver la ayuda de la librería. Ahora, si se desea comparar los ingresos y gastos por sexo se procede de la siguiente manera: plot11_Ponde &lt;- ggplot( data = encuesta, aes(x = Income, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot12_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot11_Ponde | plot12_Ponde Definiendo el color del relleno para hombres y mujeres: colorSex &lt;- c(Male = &quot;#5DADE2&quot;, Female = &quot;#2874A6&quot;) plot11_Ponde + scale_fill_manual(values = colorSex) | plot12_Ponde + scale_fill_manual(values = colorSex) Realizando la comparación para más de dos categorías, por ejemplo región, se procede como: plot13_Ponde &lt;- ggplot( data = encuesta, aes(x = Income, weight = wk)) + geom_boxplot(aes(fill = Region)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot14_Ponde &lt;- ggplot( data = encuesta, aes(x = Expenditure, weight = wk)) + geom_boxplot(aes(fill = Region)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot13_Ponde | plot14_Ponde Personalizando los coles cuando hay más de dos categorías, se realiza como se muestra a continuación: colorRegion &lt;- c( Norte = &quot;#D6EAF8&quot;, Sur = &quot;#85C1E9&quot;, Centro = &quot;#3498DB&quot;, Occidente = &quot;#2E86C1&quot;, Oriente = &quot;#21618C&quot; ) plot13_Ponde + scale_fill_manual(values = colorRegion) | plot14_Ponde + scale_fill_manual(values = colorRegion) La función geom_boxplotpermite realizar comparaciones con más de dos variables al tiempo. A continuación se compara los ingresos por sexo en las diferentes zonas. plot15_Ponde &lt;-ggplot(data = encuesta, aes(x = Income, y = Zone, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + scale_fill_manual(values = colorSex) + coord_flip() plot15_Ponde De forma análoga podemos realizar la comparación de los gastos por sexo en las diferentes zonas: plot16_Ponde &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Zone, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + scale_fill_manual(values = colorSex) + coord_flip() plot15_Ponde / plot16_Ponde Se puede extender las comparaciones a variables que tienen más de dos categorías. plot17_Ponde &lt;- ggplot(data = encuesta, aes(x = Income, y = Region, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + scale_fill_manual(values = colorSex) + coord_flip() plot17_Ponde plot18_Ponde &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Region, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + scale_fill_manual(values = colorSex) + coord_flip() plot17_Ponde / plot18_Ponde "],["scaterplot.html", "7.3 Scaterplot", " 7.3 Scaterplot Un diagrama de dispersión o Scaterplot representa cada observación como un punto, posicionado según el valor de dos variables. Además de una posición horizontal y vertical, cada punto también tiene un tamaño, un color y una forma. Estos atributos se denominan estética y son las propiedades que se pueden percibir en el gráfico. Cada estética puede asignarse a una variable o establecerse en un valor constante. Para realizar este tipo de gráfico se usará la función geom_point. Para ejemplificar el uso de esta función, se graficarán las variables ingresos y gastos como se muestra a continuación: plot19_Ponde &lt;- ggplot( data = encuesta, aes( y = Income, x = Expenditure, weight = wk)) + geom_point() + theme_cepal() plot19_Ponde Note, que en este caso el parámetro weight no está aportando información visual al gráfico. El parámetro weight se puede usar para controlar el tamaño de los puntos, y así, tener un mejor panorama del comportamiento de la muestra: plot20_Ponde &lt;- ggplot( data = encuesta, aes(y = Income, x = Expenditure)) + geom_point(aes(size = wk), alpha = 0.3) + theme_cepal() plot20_Ponde Otra forma de usar la variable wk, es asignar la intensidad del color según el valor de la variable: plot21_Ponde &lt;- ggplot( data = encuesta, aes(y = Income, x = Expenditure)) + geom_point(aes(col = wk), alpha = 0.3) + theme_cepal() plot21_Ponde Se puede extender las bondades de los gráfico de ggplot2 para obtener mayor información de las muestra. Por ejemplo, agrupar los datos por Zona. Para lograr esto se introduce el parámetro shape: plot22_Ponde &lt;- ggplot( data = encuesta, aes(y = Income, x = Expenditure, shape = Zone)) + geom_point(aes(size = wk, color = Zone), alpha = 0.3) + labs(size = &quot;Peso&quot;) + scale_color_manual(values = colorZona) + theme_cepal() plot22_Ponde De forma similar se puede obtener el resultado por sexo: plot23_Ponde &lt;- ggplot( data = encuesta, aes( y = Income, x = Expenditure, shape = Sex)) + geom_point(aes( size = wk, color = Sex), alpha = 0.3) + labs(size = &quot;Peso&quot;) + scale_color_manual(values = colorSex) + theme_cepal() plot23_Ponde Un resultado equivalente se obtiene por región: plot24_Ponde &lt;- ggplot( data = encuesta, aes( y = Income, x = Expenditure, shape = Region)) + geom_point(aes( size = wk, color = Region), alpha = 0.3) + labs(size = &quot;Peso&quot;) + scale_color_manual(values = colorRegion) + theme_cepal() plot24_Ponde "],["diagrama-de-barras-para-variables-categoricas.html", "7.4 Diagrama de barras para variables categoricas", " 7.4 Diagrama de barras para variables categoricas Para realizar estos gráfico, en primer lugar, se deben realizar las estimaciones puntuales de los tamaños que se van a graficar: tamano_zona &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise( Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) tamano_zona Zone Nd Nd_se Nd_low Nd_upp Rural 72102 3062 66039 78165 Urban 78164 2847 72526 83802 Ahora, se procede a hacer el gráfico como se mostró en las secciones anteriores: plot25_Ponde &lt;- ggplot( data = tamano_zona, aes( x = Zone, y = Nd, ymax = Nd_upp, ymin = Nd_low, fill = Zone)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(position = position_dodge(width = 0.9), width = 0.3) + theme_bw() plot25_Ponde Como se ha visto en los gráficos anteriores, se pueden extender a variables con muchas más categorías: tamano_pobreza &lt;- diseno %&gt;% group_by(Poverty) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) tamano_pobreza Poverty Nd Nd_se Nd_low Nd_upp NotPoor 91398 4395 82696 100101 Extreme 21519 4949 11719 31319 Relative 37349 3695 30032 44666 El gráfico se obtiene con una sintaxis homologa a la anterior: plot26_Ponde &lt;- ggplot( data = tamano_pobreza, aes( x = Poverty, y = Nd, ymax = Nd_upp, ymin = Nd_low, fill = Poverty)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar( position = position_dodge(width = 0.9), width = 0.3) + theme_bw() plot26_Ponde De forma similar a los gráficos Boxplot, es posible realizar comparaciones entre más dos variables. tamano_ocupacion_pobreza &lt;- diseno %&gt;% group_by(desempleo, Poverty) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% as.data.frame() %&gt;% mutate(desempleo = ifelse(is.na(desempleo),&quot;Ninos&quot;,desempleo)) tamano_ocupacion_pobreza desempleo Poverty Nd Nd_se Nd_low Nd_upp 0 NotPoor 68946 3676.3 61666.8 76226 0 Extreme 11549 2208.8 7175.8 15923 0 Relative 22847 2558.5 17780.5 27913 1 NotPoor 1768 405.4 965.7 2571 1 Extreme 1169 348.1 479.9 1859 1 Relative 1697 457.8 790.7 2604 Ninos NotPoor 20684 1256.6 18195.4 23172 Ninos Extreme 8800 2979.9 2899.7 14701 Ninos Relative 12805 1551.0 9733.9 15876 El gráfico para la tabla anterior queda de la siguiente manera: plot27_Ponde &lt;- ggplot( data = tamano_ocupacion_pobreza, aes( x = Poverty, y = Nd, ymax = Nd_upp, ymin = Nd_low, fill = as.factor(desempleo))) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar( position = position_dodge(width = 0.9), width = 0.3) + theme_bw() plot27_Ponde En estos gráficos también se pueden presentar proporciones, como se muestra a continuación: prop_ZonaH_Pobreza &lt;- sub_Hombre %&gt;% group_by(Zone, Poverty) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) prop_ZonaH_Pobreza ## # A tibble: 6 × 6 ## # Groups: Zone [2] ## Zone Poverty prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural NotPoor 0.549 0.0626 0.424 0.668 ## 2 Rural Extreme 0.198 0.0675 0.0958 0.364 ## 3 Rural Relative 0.254 0.0372 0.187 0.334 ## 4 Urban NotPoor 0.660 0.0366 0.584 0.728 ## 5 Urban Extreme 0.113 0.0245 0.0726 0.171 ## 6 Urban Relative 0.227 0.0260 0.180 0.283 Después de tener la tabla con los valores a presentar en el gráfico, los códigos computacionales para realizar el gráfico es el siguiente: plot28_Ponde &lt;- ggplot( data = prop_ZonaH_Pobreza, aes( x = Poverty, y = prop, ymax = prop_upp, ymin = prop_low, fill = Zone)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar( position = position_dodge(width = 0.9), width = 0.3 ) + scale_fill_manual(values = colorZona) + theme_bw() plot28_Ponde Ahora bien, grafiquemos la proporción de hombres en condición de pobreza por región: prop_RegionH_Pobreza &lt;- sub_Hombre %&gt;% group_by(Region, pobreza) %&gt;% summarise( prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() prop_RegionH_Pobreza Region pobreza prop prop_se prop_low prop_upp Norte 0 0.6315 0.0552 0.5171 0.7327 Norte 1 0.3685 0.0552 0.2673 0.4829 Sur 0 0.6134 0.0567 0.4970 0.7181 Sur 1 0.3866 0.0567 0.2819 0.5030 Centro 0 0.6453 0.0846 0.4666 0.7910 Centro 1 0.3547 0.0846 0.2090 0.5334 Occidente 0 0.6259 0.0439 0.5358 0.7080 Occidente 1 0.3741 0.0439 0.2920 0.4642 Oriente 0 0.5450 0.1012 0.3480 0.7289 Oriente 1 0.4550 0.1012 0.2711 0.6520 El gráfico de barras es el siguiente: plot29_Ponde &lt;- ggplot( data = prop_RegionH_Pobreza, aes( x = Region, y = prop, ymax = prop_upp, ymin = prop_low, fill = as.factor(pobreza))) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar( position = position_dodge(width = 0.9), width = 0.3 ) + theme_bw() plot29_Ponde "],["creando-mapas.html", "7.5 Creando mapas", " 7.5 Creando mapas Los mapas son una herramienta gráfica poderosa para la visualización de datos. Particularmente, para indicadores sociales-demográficos estos son una gran referencia visual para desagregaciones a nivel País, región, departamento, provincia, distrito, municipio, comuna, etc. R posee un sin fin de métodos de programación para representar dichos mapas. Para graficar mapas es necesario contar con información geoespacial, datos que contienen las coordenadas o delimitaciones geográficas de determinado país o región. Sitios web como http://www.diva-gis.org/gdata ofrecen de manera gratuita bases de datos o shapes que contienen los vectores asociados a las geografías correspondientes. Dichos conjuntos de datos poseen observaciones sobre la longitud y latitud lo cuál permite graficar en R un conjunto de puntos cuya unión en el gráfico formarán las formas los polígonos que dan forma a las áreas geográficas. Entre las distintas librería para realizar mapas en R están tmap y ggplot2. A continuación, se ilustra cómo se generan mapas, inicalmente con la librería tmap: Inicialmente, para realizar el mapa hay que contar con el archivo de shepefile el cual se carga de la siguiente manera:: library(sf) library(tmap) shapeBigCity &lt;- read_sf(&quot;Data/shapeBigCity/BigCity.shp&quot;) Una vez cargado el shape, el mapa se genera usando las funciones tm_shape y lo que se desea graficar en el mapa se incluye con la función tm_polygons. Para este ejemplo, solo grafiquemos las regiones en el mapa: tm_shape(shapeBigCity) + tm_polygons(col = &quot;Region&quot;) Si ahora el objetivo es graficar en las regiones el procentaje de probreza para hombres, inicialmente se debe agregar esa información a la base de datos con la que se graficará el mapa como sigue: shape_temp &lt;- tm_shape( shapeBigCity %&gt;% # shapefile left_join( # Agregando una variable prop_RegionH_Pobreza %&gt;% filter(pobreza == 1), # Filtrando el nivel de interés. by = &quot;Region&quot;)) Una vez generado la base de datos, se procede a crear el mapa. En este ejemplo, agregarán unos puntos de corte en el mapas que son definidos en el argumento brks como se muestra a continuación: brks &lt;- c(0, .2, .4, .6, 0.8, 1) shape_temp + tm_polygons( col = &quot;prop&quot;, breaks = brks, title = &quot;pobreza&quot;, palette = &quot;YlOrRd&quot;) A modo de otro ejemplo, se desea graficar ahora los coeficientes de variación de las estimaciones de los ingresos medios obtenidas por el diseño a nivel de región: prom_region &lt;- svyby(~Income, ~Region, diseno, svymean, na.rm = T, covmat = TRUE, vartype = c(&quot;cv&quot;)) prom_region Region Income cv Norte Norte 552.3637 0.1002236 Sur Sur 625.7740 0.0997257 Centro Centro 650.7820 0.0944538 Occidente Occidente 517.0071 0.0894006 Oriente Oriente 541.7543 0.1322830 brks &lt;- c(0, 0.2, 1) shape_temp &lt;- tm_shape( shapeBigCity %&gt;% left_join( prom_region, by = &quot;Region&quot;)) shape_temp + tm_polygons( &quot;cv&quot;, breaks = brks, title = &quot;cv&quot;, palette = c(&quot;#FFFFFF&quot;, &quot;#000000&quot;), ) + tm_layout(asp = 0) Ahora, realizar el mismo ejercicio anterior pero por zona y sexo: prom_region_Sex &lt;- diseno %&gt;% group_by(Region, Zone, Sex, pobreza) %&gt;% summarise(prop = survey_mean(vartype = &quot;cv&quot;)) %&gt;% filter(pobreza == 1, Zone == &quot;Rural&quot;, Sex == &quot;Female&quot;) shape_temp &lt;- tm_shape( shapeBigCity %&gt;% left_join( prom_region_Sex, by = &quot;Region&quot;)) shape_temp + tm_polygons( &quot;prop&quot;, title = &quot;Pobreza&quot;, ) + tm_layout(asp = 0) Como se comentó en la introducción de esta sección, los gráficos también se pueden hacer usando la librería ggplot2. Esta librería se apoya en las librerías biscale y cowplot. El procedimiento en R para hacer los mapas es muy similar al mostrado con la librería tmap y se realiza de la siguiente manera: library(biscale) library(cowplot) temp_shape &lt;- shapeBigCity %&gt;% left_join( prom_region_Sex, by = &quot;Region&quot;) k &lt;- 3 datos.RM.bi &lt;- bi_class(temp_shape, y = prop, x = prop_cv, dim = k, style = &quot;fisher&quot;) map.RM &lt;- ggplot() + geom_sf( data = datos.RM.bi, aes(fill = bi_class, geometry = geometry), colour = &quot;white&quot;, size = 0.1) + bi_scale_fill(pal = &quot;GrPink&quot;, dim = k) + bi_theme() + theme(legend.position = &quot;none&quot;) map.RM Ahora, para crear la leyenda del mapa se hace de la siguiente manera: legend1 &lt;- bi_legend( pal = &quot;GrPink&quot;, dim = k, xlab = &quot;Coeficiente de variaci&lt;U+00F3&gt;n&quot;, ylab = &quot;Pobreza&quot;, size = 8) mapa1 &lt;- ggdraw() + draw_plot(map.RM, 0, 0, 1, scale = 0.7) + draw_plot(legend1, 0.75, 0.4, 0.2, 0.2, scale = 1) + draw_text(&quot;Estimaciones directas de la pobreza en la mujer rural&quot;, vjust = -13, size = 18) mapa1 "],["modelos-lineales-generalizados-en-encuestas-de-hogares.html", "Capítulo 8 Modelos lineales generalizados en encuestas de hogares", " Capítulo 8 Modelos lineales generalizados en encuestas de hogares Los modelos lineales generalizados (MLGs) proporcionan una aproximación unificada a la mayoría de los procedimientos usados en estadística aplicada. El nombre se debe a que ellos generalizan los modelos lineales basados en el supuesto de distribución normal para la variable respuesta. Al igual que los modelos lineales clásicos, tratados en capítulos anteriores, los MLG tienen aplicación en todas las disciplinas del saber. Nelder &amp; Wedderburn (1972) presentaron por primera vez el término en un artículo que, sin lugar a dudas, es uno de los más importantes publicados en el área de estadística, por su gran impacto en la forma como se aplica esta disciplina. Desde entonces, poco a poco los modelos lineales generalizados se han ido conociendo y usando ampliamente. La genialidad de Nelder &amp; Wedderburn (1972) consistió en darse cuenta (y demostrar) que muchos de los métodos estadísticos ampliamente usados en la época, aparentemente desligados unos de otros, tales como la regresión lineal múltiple, el análisis probit, el análisis de datos provenientes de ensayos de dilución usando la distribución binomial (realizados por Fisher), los modelos logit para proporciones, los modelos log-lineales para conteos, los modelos de regresión para datos de sobrevivencia, entre otros, se podían tratar con un marco teórico unificado y que las estimaciones de máxima verosimilitud para los parámetros de esos modelos podían obtenerse por el mismo algoritmo conocido como mínimos cuadrados ponderados iterativos (MCPI). Los desarrollos teóricos en modelos lineales clásicos parten del supuesto que la variable respuesta tiene distribución normal, cuando un fenómeno en estudio genera datos para los cuales no es razonable la suposición de normalidad, como por ejemplo cuando la respuesta es categórica, una proporción o un conteo, obviamente la respuesta no es normal y no es recomendable analizar los datos suponiendo normalidad. Otro supuesto de los modelos lineales clásicos es el de homogeneidad de la varianza, situación que no se verifica cuando la respuesta es, por ejemplo, una variable aleatoria de poisson, distribución donde la media y la varianza son iguales, es decir, en este modelo un cambio en la media necesariamente implica cambio en la varianza. Los modelos lineales generalizados son excelentes para modelar datos en condiciones de no normalidad y varianza no constante. Específicamente, se debería considerar usar los MLGs cuando la variable respuesta es: conteos expresados como proporciones, conteos que no son proporciones, respuestas binarias, tiempos de sobrevida donde la varianza se incrementa con la media. Sin lugar a dudas, en las encuestas de hogares existen variables de tipo conteo, binomiales, etc que meritan su análisis usando modelos lineales generalizados. Es por esto que, este capítulo es de relevancia en este texto. Para ejemplificar los conceptos, inicialmente se cargan las librerías y la base de datos como sigue: options(digits = 4) options(tinytex.verbose = TRUE) library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(broom) library(jtools) library(modelsummary) library(patchwork) Cargue de las bases de datos, encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) data(&quot;BigCity&quot;, package = &quot;TeachingSampling&quot;) Por último, se define el diseño muestral, diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) Se generan nuevas variables en el diseño para ser utilizadas en los ejemplos, diseno &lt;- diseno %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0)) Como se ha definido en secciones y capítulos anteriores, con variables dicotómicas se pueden generar tablas de frecuencias teniendo en cuenta los factores de expansión del diseño. En R se hace usando la función svyby de la siguiente manera. Primero, se define la variable a la que se le requiere hacer la tabla (formula), luego se le indica cuál es la variable clasificadora (by). En este caso se quiere hacer una tabla de pobreza claificada por sexo. En tercer lugar se define la función que se quiere aplicar (FUN), en este caso, se quieren calcular totales por celda, por último, se define el diseño de muestreo (design) tab_pobreza_sexo &lt;- svyby(formula = ~factor(pobreza), by = ~Sex, FUN = svytotal, design = as.svrepdesign(diseno), se=F, na.rm=T, ci=T, keep.var=TRUE) tab_pobreza_sexo Sex factor(pobreza)0 factor(pobreza)1 se1 se2 Female Female 48366 30824 2411 2916 Male Male 43032 28044 2522 3095 Sin embargo para la estimación de tamaños, se puede emplear también la función svytable como sigue: tab &lt;- svytable(formula = ~pobreza + Sex, design = diseno) kable(tab) Female Male 0 48366 43032 1 30824 28044 Al hacer uso de la función svyby pero usando en el argumento FUN= svymean es posible estimar proporciones como se muestra a continuación: tab_pobreza_sexo &lt;- svyby(formula = ~factor(pobreza), by = ~Sex, FUN = svymean, design = as.svrepdesign(diseno), se=F, na.rm=T, ci=T, keep.var=TRUE) tab_pobreza_sexo Sex factor(pobreza)0 factor(pobreza)1 se1 se2 Female Female 0.6108 0.3892 0.0316 0.0316 Male Male 0.6054 0.3946 0.0366 0.0366 También se pueden hacer tablas de doble entrada para la proporción. En forma alternativa es posible usar la función prop.table del paquete base. kable(prop.table(tab, margin = 2)) Female Male 0 0.6108 0.6054 1 0.3892 0.3946 Estas diferentes formas de proceder son de mucha importancia al momento de hacer uso de pruebas de independencia en tablas cruzadas. "],["prueba-de-independencia-f.html", "8.1 Prueba de independencia F", " 8.1 Prueba de independencia F La prueba de independencia F de Fisher permite analizar si dos variables dicotómicas están asociadas cuando la muestra a estudiar es demasiado pequeña y no se cumplen las condiciones para aplicar la prueba \\(\\chi^{2}\\). Para utilizar esta técnica, tengamos en cuenta que la probabilidad estimada se escribe como: \\[ \\hat{\\pi}_{rc}=\\frac{n_{r+}}{n_{++}}\\times\\frac{n_{+c}}{n_{++}} \\] Teniendo en cuenta esta expresión, la estadística \\(\\chi{2}\\) de Pearson se define de la siguiente manera: \\[ \\chi_{pearsom}^{2}=n_{++}\\times\\sum_{r}\\sum_{c}\\left(\\frac{\\left(p_{rc}-\\hat{\\pi}_{rc}\\right)^{2}}{\\hat{\\pi}_{rc}}\\right) \\] y la estadística de razón de verosimilitud se define como: \\[ G^{2}=2\\times n_{++}\\times\\sum_{r}\\sum_{c}p_{cr}\\times\\ln\\left(\\frac{p_{rc}}{\\hat{\\pi}_{rc}}\\right) \\] donde, \\(r\\) es el número de filas y \\(c\\) representa el número de columnas, la prueba tiene \\((R-1)\\times (C-1)\\) grados de libertad. Como lo menciona Heeringa, Fay (1979, 1985) y Fellegi (1980) fueron de los primeros en proponer la corrección del estadístico chi-cuadrado de Pearson basada en un efecto de diseño generalizado (GDEFF, por sus siglas en inglés). Rao y Scott (1984) y más tarde Thomas y Rao (1987) ampliaron la teoría de las correcciones del efecto de diseño generalizado para estas pruebas estadísticas. El método de Rao-Scott requiere el cálculo de efectos de diseño generalizados que son analíticamente más complicados que el enfoque de Fellegi. Las correcciones de Rao-Scott son ahora el estándar en los procedimientos para el análisis de datos de encuestas categóricas en sistemas de software como Stata y SAS. Los estadísticos de prueba Rao-Scott Pearson ajustados por diseño y razón de verosimilitud chi-cuadrado se calculan de la siguiente manera: \\[ \\chi^2_{(R-S)} = \\chi^2_{(Pearson)}\\big/GDEFF \\] y, para la estadística basada en la razón de verosimilitud se calcula como: \\[ G^2_{(R-S)} = G^2\\big/GDEFF \\] donde el efecto generalizado del diseño (\\(GDEFF\\)) de Rao–Scott, está dado por \\[ GDEFF=\\frac{\\sum_{r}\\sum_{c}\\left(1-p_{rc}\\right)d^{2}\\left(p_{rc}\\right)-\\sum_{r}\\left(1-p_{r+}\\right)d^{2}\\left(p_{r+}\\right)-\\sum_{c}\\left(1-p_{+c}\\right)d^{2}\\left(p_{+c}\\right)}{\\left(R-1\\right)\\left(C-1\\right)} \\] Por tanto, la estadística F para independencia basada en la chi-cuadrado de Pearson se calcula como sigue: \\[ F_{R-S,Pearson}=\\chi_{R-S}^{2}\\big/\\left[\\left(R-1\\right)\\left(C-1\\right)\\right]\\sim F_{\\left(R-1\\right)\\left(C-1\\right),\\left(R-1\\right)\\left(C-1\\right)df} \\] y, la estadística F para independencia basada en la razón de verosimilitudes se calcula como sigue: \\[ F_{R-S,LRT}=G_{R-S}^{2}\\big/\\left(C-1\\right)\\sim F_{\\left(C-1\\right),df} \\] donde \\(C\\) es el número de columnas de la tabla cruzada. En R, el cálculo de las estadísticas chi-cuadrado y F se camculan usando la función summary como se muestra a continuación: summary(tab, statistic = &quot;Chisq&quot;) ## Sex ## pobreza Female Male ## 0 48366 43032 ## 1 30824 28044 ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## X-squared = 0.077, df = 1, p-value = 0.8 Basado en la estadística de Pearson, se puede concluir que el estado de pobreza y el sexo no están relacionados con una confianza del 95%. summary(tab, statistic = &quot;F&quot;) ## Sex ## pobreza Female Male ## 0 48366 43032 ## 1 30824 28044 ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 0.056, ndf = 1, ddf = 119, p-value = 0.8 Resultados similares se obtienen con la prueba F de independencia. "],["estadístico-de-wald.html", "8.2 Estadístico de Wald", " 8.2 Estadístico de Wald Este estadístico se aplica cuando ya se ha elegido un modelo estadístico ( regresión lineal simple, regresión logística, entre otros).El estadístico de prueba de Wald \\(\\chi^{2}\\) para la hipótesis nula de independencia de filas y columnas en una tabla de doble entrada se define de la siguiente manera: \\[ Q_{wald}=\\hat{\\boldsymbol{Y}^{t}}\\left(\\boldsymbol{H}\\hat{\\boldsymbol{V}}\\left(\\hat{\\boldsymbol{N}}\\right)\\boldsymbol{H}^{t}\\right)^{-1}\\hat{\\boldsymbol{Y}} \\] donde, \\[ \\hat{\\boldsymbol{Y}}=\\left(\\hat{N}-E\\right) \\] es un vector de \\(R\\times C\\) de diferencias entre los recuentos de celdas observadas y esperadas, esto es, \\(\\hat{N}_{rc}-E_{rc}\\). La matriz \\(\\boldsymbol{H}\\hat{\\boldsymbol{V}}\\left(\\hat{\\boldsymbol{N}}\\right)\\boldsymbol{H}^{t}\\), representa la matriz de varianza y covarianza estimada para el vector de diferencias. En el caso de un diseño de muestra complejo, la matriz de varianza-covarianza de los conteos de frecuencia ponderada, \\(\\hat{V}\\left(\\hat{N}\\right)\\), se estima utilizando métodos de remuestreo o aproximación de Taylor. La matriz \\(\\boldsymbol{H}\\) es la inversa de la matriz \\(\\boldsymbol{J}\\) dada por: \\[ \\boldsymbol{J}=-\\left[\\frac{\\delta^{2}\\ln PL\\left(\\boldsymbol{B}\\right)}{\\delta^{2}\\boldsymbol{B}}\\right] \\mid \\boldsymbol{B}=\\hat{\\boldsymbol{B}} \\] Bajo la hipótesis nula de independencia, el estadístico de wald se distribuye chi cuadrado con \\(\\left(R-1\\right)\\times\\left(C-1\\right)\\) grados de libertad, \\[ Q_{wald}\\sim\\chi_{\\left(R-1\\right)\\times\\left(C-1\\right)}^{2} \\] La transformación F del estadístico de Wald es: \\[ F_{wald}=Q_{wald}\\times\\frac{df-\\left(R-1\\right)\\left(C-1\\right)+1}{\\left(R-1\\right)\\left(C-1\\right)df}\\sim F_{\\left(R-1\\right)\\left(C-1\\right),df-\\left(R-1\\right)\\left(C-1\\right)+1} \\] En R, para calcular el estadístico de Wald se hace similarmente al cálculo de los estadísticos anteriores usando la función summary como sigue: summary(tab, statistic = &quot;Wald&quot;) ## Sex ## pobreza Female Male ## 0 48366 43032 ## 1 30824 28044 ## ## Design-based Wald test of association ## ## data: NextMethod() ## F = 0.056, ndf = 1, ddf = 119, p-value = 0.8 Se puede concluir que, con una confianza del 95% y basado en la muestra no hay relación entre el estado de pobreza y el sexo. En este mismo sentido, el estadístco de Wald ajustado en R se se calcula similarmente al anterior y los resultados fueron similares: summary(tab, statistic = &quot;adjWald&quot;) ## Sex ## pobreza Female Male ## 0 48366 43032 ## 1 30824 28044 ## ## Design-based Wald test of association ## ## data: NextMethod() ## F = 0.056, ndf = 1, ddf = 119, p-value = 0.8 "],["modelo-log-lineal-para-tablas-de-contingencia.html", "8.3 Modelo log lineal para tablas de contingencia", " 8.3 Modelo log lineal para tablas de contingencia El término modelo log-lineal, que básicamente describe el papel de la función de enlace que se utiliza en los modelos lineales generalizados. Iniciaremos esta sección con los modelos log-lineales en tablas de contingencia. El modelo estadístico es el siguiente: \\[ \\log(p_{ijk}) = \\mu + \\lambda_i^X + \\lambda_j^Y + \\lambda_k^Z + \\lambda_{ij}^{XY} , \\] donde: \\(p_{ijk}=\\) la proporción esperada en la celda bajo el modelo. \\(\\mu = \\log(p_{0})=\\frac{1}{\\#\\ de\\ celdas}\\) El modelo log-lineal en R se ajusta utilizando la función svyloglin como sigue: mod1 &lt;- svyloglin(formula = ~pobreza+Sex + pobreza:Sex, design = diseno) s1 &lt;- summary(mod1) s1 ## Loglinear model: svyloglin(formula = ~pobreza + Sex + pobreza:Sex, design = diseno) ## coef se p ## pobreza1 0.219673 0.06778 0.001192 ## Sex1 0.052843 0.01625 0.001145 ## pobreza1:Sex1 0.005583 0.02350 0.812175 En la salida anterior se puede observar que, con una confianza del 95% el estado de pobreza es independiente del sexo, como se ha mostrado con las pruebas anteriores. Ahora bien, puesto que en la salida anterior se pudo observar que la interacción es no significativa, entonces, ajustemos ahora el modelo sin interacción: mod2 &lt;- svyloglin(formula = ~pobreza+Sex, design = diseno) s2 &lt;- summary(mod2) s2 ## Loglinear model: svyloglin(formula = ~pobreza + Sex, design = diseno) ## coef se p ## pobreza1 0.21997 0.06752 0.0011230 ## Sex1 0.05405 0.01577 0.0006076 Por último, mediante un análisis de varianza es posible comparar los dos modelos como sigue: anova(mod1, mod2) ## Analysis of Deviance Table ## Model 1: y ~ pobreza + Sex ## Model 2: y ~ pobreza + Sex + pobreza:Sex ## Deviance= 0.07719 p= 0.8126 ## Score= 0.07719 p= 0.8126 De la anterior salida se puede concluir que, con una confianza del 95%, la interacción no es significativa en el modelo log-lineal ajustado. Modelo de regresión logistica Un modelo de regresion logística es un modelo matemático que puede ser utilizado para describir la relacion entre un conjunto de variables independientes y una variable dicotomica Y. El modelo logístico se describe a continuación: \\[ g(\\pi(x))=logit(\\pi(x)) \\] De aquí, \\[ z = \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right) = B_0 + B_1x_1+\\dots+B_px_p \\] Por tanto, la probabilidad estimada utilizando el modelo logístico es la siguiente: \\[ \\hat{\\pi}\\left(\\boldsymbol{x}\\right)=\\frac{\\exp\\left(\\boldsymbol{X\\hat{B}}\\right)}{1-\\exp\\left(\\boldsymbol{X\\hat{B}}\\right)}=\\frac{\\exp\\left(\\hat{B}_{0}+\\hat{B}_{1}x_{1}+\\cdots+\\hat{B}x_{p}\\right)}{1-\\exp\\left(\\hat{B}_{0}+\\hat{B}_{1}x_{1}+\\cdots+\\hat{B}x_{p}\\right)} \\] \\[ \\pi\\left(x_{i}\\right)=\\frac{\\exp\\left(x_{i}\\boldsymbol{B}\\right)}{1-\\exp\\left(x_{i}\\boldsymbol{B}\\right)} \\] La varianza de los parámetros estimados se calcula como sigue: \\[ var\\left(\\boldsymbol{\\hat{B}}\\right)=\\boldsymbol{J}^{-1}var\\left(S\\left(\\hat{\\boldsymbol{B}}\\right)\\right)\\boldsymbol{J}^{-1} \\] con, \\[ S\\left(B\\right)=\\sum_{h}\\sum_{a}\\sum_{i}w_{hai}\\boldsymbol{D}_{hai}^{t}\\left[\\left(\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)\\left(1-\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)\\right]^{-1}\\left(y_{hai}-\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)=0 \\] y, \\[ D_{hai} = \\frac{\\delta\\left(\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)}{\\delta B_{j}} \\] donde \\(j=0,\\dots,p\\) Prueba de Wald para los parámetros del modelo Para utilizar el estadístico de Wald en en la significancia de los parámetros del modelo se utiliza la razón de verosimilitud. En este caso se contrastan el modelo con todos los parámetros (modelo full) versus el modelo reducido, es decir, el modelo con menos parámetros (modelo reduced), \\[ G=-2\\ln\\left[\\frac{L\\left(\\hat{\\boldsymbol{\\beta}}_{MLE}\\right)_{reduced}}{L\\left(\\hat{\\boldsymbol{\\beta}}_{MLE}\\right)_{full}}\\right] \\] Dado que el modelo tiene enlace logaritmo, para construir los intervalos de confianza se debe aplicar el función exponencial a cada parámetro, \\[ \\hat{\\psi}=\\exp\\left(\\hat{B}_{1}\\right) \\] por ende, el intervalo de confianza es: \\[ CI\\left(\\psi\\right)=\\exp\\left(\\hat{B}_{j}\\pm t_{df,1-\\frac{\\alpha}{2}}se\\left(\\hat{B}_{j}\\right)\\right) \\] A continuación, se muestra el ajuste de un modelo logístico teniendo e cuenta el diseño muestral: mod_loglin &lt;- svyglm(formula = pobreza ~ Sex + Zone + Region, family = binomial, design=diseno) tidy(mod_loglin) term estimate std.error statistic p.value (Intercept) -0.4082 0.2640 -1.5464 0.1248 SexMale 0.0086 0.0915 0.0945 0.9249 ZoneUrban -0.4378 0.2418 -1.8106 0.0729 RegionSur 0.0063 0.3140 0.0201 0.9840 RegionCentro 0.1915 0.4279 0.4476 0.6553 RegionOccidente 0.2319 0.3144 0.7377 0.4622 RegionOriente 0.3699 0.4259 0.8686 0.3869 La función tidy muestra que ninguna de las covariables son significativas con una confianza del 95%. A continuación, se presentan los intervalos de confianza en los cuales se pueden concluir que en todos los parámetros el cero se encuentra dentro del intrevalo: confint(mod_loglin, level = 0.95) 2.5 % 97.5 % (Intercept) -0.9312 0.1148 SexMale -0.1727 0.1900 ZoneUrban -0.9169 0.0413 RegionSur -0.6159 0.6285 RegionCentro -0.6562 1.0392 RegionOccidente -0.3910 0.8549 RegionOriente -0.4738 1.2136 Para verificar de manera gráfica la distribución de los parámetros del modelo, se realizará un gráfico de estos usando la función plot_summs como se muestra a continuación, library(ggstance) plot_summs(mod_loglin, scale = TRUE, plot.distributions = TRUE) Se puede observar en el gráfico que el número cero se encuentra dentro del intervalo de confianza de cada uno de los parámetros, lo que confirma la no significancia al 95% de los parámetros del modelo. Por otro parte, el estadístico de Wald para el cada una de las variables del modelo se calcula a continuación con la función regTermTest para las variables del modelo: regTermTest(model = mod_loglin, ~Sex) ## Wald test for Sex ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno, ## family = binomial) ## F = 0.00893 on 1 and 113 df: p= 0.92 regTermTest(model = mod_loglin, ~Zone) ## Wald test for Zone ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno, ## family = binomial) ## F = 3.278 on 1 and 113 df: p= 0.073 regTermTest(model = mod_loglin, ~Region) ## Wald test for Region ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno, ## family = binomial) ## F = 0.3654 on 4 and 113 df: p= 0.83 Concluyendo que con una confianza del 95% no son significativas en el modelo como se había mencionado anteriormente. Como es tradicional en el ejuste de modelos de regresión ya sea, clásico o generalizado, se pueden realizar ajustes con interacciones. A continuación, se present cómo se ajustan modelos loglineales con interacción: mod_loglin_int &lt;- svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, family=binomial, design=diseno) tab_mod &lt;- tidy(mod_loglin_int) %&gt;% arrange(p.value) tab_mod term estimate std.error statistic p.value ZoneUrban -0.4248 0.2562 -1.6580 0.1002 (Intercept) -0.4289 0.2849 -1.5055 0.1351 SexMale:RegionSur 0.2871 0.2774 1.0348 0.3031 RegionOriente 0.3843 0.4279 0.8980 0.3712 RegionOccidente 0.3342 0.3783 0.8835 0.3790 SexMale:RegionOccidente -0.2302 0.2868 -0.8026 0.4240 RegionCentro 0.2466 0.4560 0.5408 0.5897 SexMale:RegionCentro -0.1162 0.2791 -0.4162 0.6781 RegionSur -0.1325 0.3464 -0.3825 0.7028 SexMale 0.0478 0.1994 0.2399 0.8109 SexMale:RegionOriente -0.0304 0.2878 -0.1057 0.9161 SexMale:ZoneUrban -0.0154 0.1872 -0.0824 0.9345 Observando que la interacción tampoco es significativa con una confianza del 95%. El gráfico de la distribución de los parámetros del modelo con intercepto y sin intercepto se presenta a continuación: plot_summs(mod_loglin_int, mod_loglin, scale = TRUE, plot.distributions = TRUE) El estadístico de Wald sobre los parámetros del modelo con intercepto son: regTermTest(model = mod_loglin_int, ~Sex) ## Wald test for Sex ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 0.05753 on 1 and 108 df: p= 0.81 regTermTest(model = mod_loglin_int, ~Zone) ## Wald test for Zone ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 2.749 on 1 and 108 df: p= 0.1 regTermTest(model = mod_loglin_int, ~Region) ## Wald test for Region ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 0.8999 on 4 and 108 df: p= 0.47 regTermTest(model = mod_loglin_int, ~Sex:Zone) ## Wald test for Sex:Zone ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 0.006789 on 1 and 108 df: p= 0.93 regTermTest(model = mod_loglin_int, ~Sex:Region) ## Wald test for Sex:Region ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 1.058 on 4 and 108 df: p= 0.38 Observándose que con una confianza del 95% ninguno de los parámetros del modelo es significativo. Ahora bien, como se ha explicado a los largo de los capítulos relacionado con modelos, se pueden ajustar modelos usando Q_Weighting. A continuación, se presenta cómo se ajusta el modelo usando estos pesos: fit_wgt &lt;- lm(wk ~ Sex + Zone + Region , data = encuesta) wgt_hat &lt;- predict(fit_wgt) encuesta %&lt;&gt;% mutate(wk2 = wk/wgt_hat) diseno_qwgt &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk2, nest = T ) Defiendo la variable pobreza dentro de la base de datos. diseno_qwgt &lt;- diseno_qwgt %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0)) Ajustando el modelo se tiene: mod_loglin_qwgt &lt;- svyglm(formula = pobreza ~ Sex + Zone + Region, family=quasibinomial, design=diseno_qwgt) tab_mod &lt;- tidy(mod_loglin_qwgt) tab_mod term estimate std.error statistic p.value (Intercept) -0.4644 0.2630 -1.7656 0.0802 SexMale 0.0241 0.0883 0.2726 0.7857 ZoneUrban -0.3445 0.2311 -1.4903 0.1389 RegionSur -0.0041 0.3116 -0.0130 0.9896 RegionCentro 0.1613 0.4270 0.3778 0.7063 RegionOccidente 0.2424 0.3147 0.7705 0.4426 RegionOriente 0.3937 0.4319 0.9115 0.3639 Concluyendo que, con una confianza del 95% y basado en la muestra, ninguno de los parámetros del modelo es significativo. Lo cual se puede corroborar con el gráfico de la distribución de los parámetros del modelo con los pesos ajustados y sin los pesos: plot_summs(mod_loglin, mod_loglin_qwgt, scale = TRUE, plot.distributions = TRUE) El Estadístico de Wald sobre los parámetros del modelo se obtiene de manera similar a lo visto anteriormente: regTermTest(model = mod_loglin_qwgt, ~Sex) ## Wald test for Sex ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno_qwgt, ## family = quasibinomial) ## F = 0.0743 on 1 and 113 df: p= 0.79 regTermTest(model = mod_loglin_qwgt, ~Zone) ## Wald test for Zone ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno_qwgt, ## family = quasibinomial) ## F = 2.221 on 1 and 113 df: p= 0.14 regTermTest(model = mod_loglin_qwgt, ~Region) ## Wald test for Region ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno_qwgt, ## family = quasibinomial) ## F = 0.4156 on 4 and 113 df: p= 0.8 Concluyendo también que ninguna de las variables son significativas. "],["referencias.html", "Capítulo 9 Referencias", " Capítulo 9 Referencias Sarndal, C., Swensson, B. &amp;Wretman, J. (1992), Model Assisted Survey Sampling, Springer, New York. Rojas, H. A. G. (2016). Estrategias de muestreo: diseño de encuestas y estimación de parámetros. Ediciones de la U. Santana Sepúlveda, S., &amp; Mateos Farfán, E. (2014). El arte de programar en R: un lenguaje para la estadística. Lumley, T. (2011). Complex surveys: a guide to analysis using R. John Wiley &amp; Sons. Bache, S. M., Wickham, H., Henry, L., &amp; Henry, M. L. (2022). Package ‘magrittr’. Tellez Piñerez, C. F., &amp; Lemus Polanía, D. F. (2015). Estadística Descriptiva y Probabilidad con aplicaciones en R. Fundación Universitaria Los Libertadore. Groves, R. M., Fowler Jr, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., &amp; Tourangeau, R. (2011). Survey methodology. John Wiley &amp; Sons. Tille, Y. &amp; Ardilly, P. (2006), Sampling Methods: Exercises and Solutions, Springer. Gambino, J. G., &amp; do Nascimento Silva, P. L. (2009). Sampling and estimation in household surveys. In Handbook of Statistics (Vol. 29, pp. 407-439). Elsevier. Cochran, W. G. (1977) Sampling Techniques. John Wiley and Sons. Gutiérrez, H. A. (2017) TeachingSampling. R package. Wickham, H., Chang, W., &amp; Wickham, M. H. (2016). Package ‘ggplot2’. Create elegant data visualisations using the grammar of graphics. Version, 2(1), 1-189. Lumley, T. (2020). Package ‘survey’. Available at the following link: https://cran. r-project. org. Hansen, M. H., &amp; Steinberg, J. (1956). Control of errors in surveys. Biometrics, 12(4), 462-474. Heeringa, S. G., West, B. T., &amp; Berglund, P. A. (2017). Applied survey data analysis. chapman and hall/CRC. Valliant, R., Dever, J.A., and Kreuter, F., Practical Tools for Designing and Weighting Survey Samples, Springer, New York, 2013. Valliant, R., Dorfman, A.H., and Royall, R.M., Finite Population Sampling and Inference: A Prediction Approach, John Wiley &amp; Sons, New York, 2000. Loomis, D., Richardson, D.B., and Elliott, L., Poisson regression analysis of ungrouped data, Occupational and Environmental Medicine, 62, 325–329, 2005. Kovar, J.G., Rao, J.N.K., and Wu, C.F.J., Bootstrap and other methods to measure errors in survey estimates, Canadian Journal of Statistics, 16(Suppl.), 25–45, 1988. Binder, D.A. and Kovacevic, M.S., Estimating some measures of income inequality from survey data: An application of the estimating equations approach, Survey Methodology, 21(2), 137–145, 1995. Kovacevic, M. S., &amp; Binder, D. A. (1997). Variance estimation for measures of income inequality and polarization-the estimating equations approach. Journal of Official Statistics, 13(1), 41. Bautista, J. (1998), Diseños de muestreo estadístico, Universidad Nacional de Colombia. Monroy, L. G. D., Rivera, M. A. M., &amp; Dávila, L. R. L. (2018). Análisis estadístico de datos categóricos. Universidad Nacional de Colombia. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
