[["index.html", "Análisis de encuestas con R Prefacio", " Análisis de encuestas con R Andrés Gutiérrez1, Cristian Téllez2, Stalyn Guerrero3 2025-09-24 Prefacio La versión online de este libro está licenciada bajo una Licencia Internacional de Creative Commons para compartir con atribución no comercial 4.0. Este libro es el resultado de un compendio de las experiencias internacionales prácticas adquiridas por el autor como Experto Regional en Estadísticas Sociales de la CEPAL. Experto Regional en Estadísticas Sociales - Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ Profesor - Universidad Santo Tomás - cristiantellez@usta.edu.co↩︎ Consultor - Comisión Económica para América Latina y el Caribe (CEPAL), guerrerostalyn@gmail.com↩︎ "],["introducción.html", "Capítulo 1 Introducción", " Capítulo 1 Introducción FALTA ESTO: BADEHOG en la Cepal Las encuestas de hogares se han consolidado como una de las principales fuentes de información estadística para el diseño, la evaluación y el seguimiento de políticas públicas. Su relevancia se extiende más allá del ámbito nacional: constituyen un insumo indispensable para el monitoreo de los Objetivos de Desarrollo Sostenible (ODS) en el marco de la Agenda 2030. Gracias a la amplitud temática que abarcan —educación, salud, empleo, ingresos, condiciones de vida y pobreza, entre otros—, las encuestas de hogares permiten elaborar indicadores que reflejan la situación de las poblaciones y orientan la formulación de estrategias de desarrollo inclusivas y sostenibles. Sin embargo, la utilidad de estas encuestas no depende únicamente de la calidad del operativo de campo. La producción de estadísticas oficiales exige un compromiso adicional: garantizar que los resultados sean precisos, confiables y comparables. En este sentido, las oficinas nacionales de estadística y los investigadores deben adoptar métodos robustos que respeten los Principios Fundamentales de las Estadísticas Oficiales de las Naciones Unidas, particularmente aquellos relacionados con la responsabilidad y transparencia (Principio 3) y la prevención del uso indebido de la información (Principio 4). Estos principios enfatizan la necesidad de documentar y difundir con claridad las fuentes, los métodos y los procedimientos utilizados, así como de pronunciarse cuando se detectan interpretaciones incorrectas de los datos. En las últimas décadas, la revolución informática y el acceso cada vez mayor a software estadístico han multiplicado las posibilidades de análisis. Hoy en día, los datos de encuestas de hogares se utilizan tanto en aplicaciones descriptivas (como la estimación de medias poblacionales, tasas de pobreza o distribuciones de frecuencia) como en aplicaciones analíticas o inferenciales (pruebas de hipótesis, construcción de modelos de regresión, estimaciones de bienestar multidimensional). Sin embargo, este avance trae consigo un riesgo: los programas estadísticos tradicionales, cuando se emplean sin tener en cuenta la complejidad del diseño muestral, pueden producir estimaciones sesgadas e intervalos de confianza engañosos, induciendo a conclusiones erróneas que pueden repercutir en la toma de decisiones públicas. El desafío central reside en que las encuestas de hogares rara vez utilizan un muestreo aleatorio simple. Por el contrario, se basan en diseños de muestreo complejos que incorporan procedimientos como: Estratificación, donde la población se divide en subgrupos homogéneos (por ejemplo, región, área urbana/rural o nivel socioeconómico), a partir de los cuales se seleccionan muestras independientes. Este procedimiento asegura representatividad y mejora la precisión de las estimaciones. Conglomeración, que consiste en agrupar las unidades de observación en unidades primarias de muestreo (manzanas, sectores censales, áreas de enumeración). En una primera etapa se seleccionan los conglomerados, y en etapas subsiguientes los hogares dentro de estos. Probabilidades desiguales de selección, diseñadas para garantizar la inclusión adecuada de subpoblaciones de interés, especialmente aquellas pequeñas o con alta heterogeneidad. Ajustes de ponderación, mediante los cuales los factores de expansión iniciales se corrigen para compensar problemas de cobertura, no respuesta o para calibrar los resultados frente a totales poblacionales conocidos. Estas características convierten al análisis de encuestas en una tarea especializada, que requiere técnicas específicas para obtener estimaciones no sesgadas y una correcta cuantificación de la incertidumbre. En este marco, el presente libro tiene como objetivo principal ofrecer al lector una guía práctica y accesible para el análisis de encuestas complejas, con un énfasis particular en las dinámicas de los hogares. Se propone como un material tanto de formación como de consulta, que combina la explicación conceptual con la aplicación práctica en el software estadístico R. La elección de R responde a tres razones fundamentales: (i) es un software de código abierto, disponible gratuitamente para cualquier usuario o institución; (ii) cuenta con un ecosistema robusto de paquetes especializados, entre ellos survey, que facilita el trabajo con diseños muestrales complejos; y (iii) es ampliamente adoptado por la comunidad académica y los institutos de estadística, lo que lo convierte en una herramienta idónea para la enseñanza, la replicación y la transparencia en los procesos analíticos. El lector encontrará a lo largo del libro no solo los fundamentos teóricos, sino también ejemplos reproducibles, tablas y gráficos generados íntegramente en R, acompañados del código necesario para su replicación. De esta manera, se busca que cada usuario pueda adaptar los procedimientos a sus propios datos y necesidades. La estructura del libro se organiza en capítulos progresivos: El Capítulo 2 introduce los conceptos esenciales sobre encuestas complejas y su importancia en el análisis de hogares. Los Capítulos 3 y 4 desarrollan la teoría de las variables aleatorias discretas y continuas en el contexto del muestreo probabilístico, sentando las bases para el análisis inferencial. El Capítulo 5 aborda la estimación de modelos de regresión lineal, mientras que el Capítulo 6 introduce el ajuste de modelos de regresión logística, ambos pilares del análisis aplicado en encuestas de hogares. Los Capítulos 7 y 8 amplían el enfoque hacia modelos lineales generalizados y multiniveles, necesarios para capturar relaciones más complejas y estructuras jerárquicas de los datos. El Capítulo 9 se centra en la imputación de datos, una herramienta indispensable para enfrentar el aumento de la no respuesta, fenómeno que se intensificó tras la pandemia. Finalmente, el Capítulo 10 explora las técnicas de visualización y comunicación de resultados, un aspecto crucial para garantizar que los hallazgos lleguen de manera clara y efectiva a audiencias técnicas y no técnicas. Con esta estructura, el libro pretende dotar al lector de un conjunto integral de herramientas que abarcan desde los fundamentos conceptuales hasta las aplicaciones más avanzadas. Más allá de ser un manual técnico, busca contribuir a una cultura estadística orientada a la calidad, la transparencia y la utilidad de la información derivada de encuestas de hogares. En síntesis, este libro se propone como una guía para investigadores, estudiantes y profesionales que deseen comprender y aplicar los métodos de análisis de encuestas complejas en R, y como un aporte al fortalecimiento de las capacidades estadísticas en la región. Al integrar teoría, práctica y principios internacionales, aspira a fomentar análisis más rigurosos y decisiones más informadas, contribuyendo así al avance del conocimiento y al logro de los objetivos de desarrollo en nuestras sociedades. "],["conceptos-básicos-en-encuestas-de-hogares.html", "Capítulo 2 Conceptos básicos en encuestas de hogares", " Capítulo 2 Conceptos básicos en encuestas de hogares El análisis riguroso de encuestas de hogares parte de una comprensión clara de sus fundamentos conceptuales y metodológicos. Como señalan Sarndal, Swensson &amp; Wretman (1992) y Gutiérrez (2016), el punto de partida es la definición de la población objetivo, el universo de estudio y el marco muestral, que sirven de base para el diseño de la encuesta y la selección de la muestra. Estos elementos son esenciales para garantizar la validez de las inferencias y la representatividad de los resultados. En este contexto, un aspecto crucial es la consideración del diseño muestral. Para obtener conclusiones válidas sobre la población, es necesario adoptar un enfoque de inferencia basada en el diseño, el cual reconoce que la muestra no surge de una selección aleatoria simple, sino de un plan probabilístico bien definido. En dicho plan, cada unidad de la población tiene una probabilidad conocida y distinta de cero de ser incluida en la muestra, lo que constituye la garantía fundamental de que los resultados puedan generalizarse a la población de referencia. Bajo este enfoque, las estimaciones son insesgadas (o prácticamente insesgadas) en relación con el esquema de muestreo, sin depender de supuestos sobre la distribución de la variable de interés. Esto otorga a la inferencia basada en el diseño un carácter robusto y ampliamente aceptado en el análisis de encuestas de hogares. Un elemento central de este proceso son los pesos de muestreo, que indican cuántas unidades de la población está representando cada unidad seleccionada. Dichos pesos permiten ajustar las estimaciones a las particularidades del diseño, de modo que los resultados reflejen con fidelidad la estructura poblacional. Además, un diseño bien documentado facilita el análisis estadístico, respalda la interpretación rigurosa de los datos y posibilita obtener conclusiones significativas sobre fenómenos complejos. En contraste, ignorar el diseño muestral y aplicar métodos de análisis que asumen una muestra aleatoria simple puede llevar a estimaciones sesgadas, errores de inferencia y conclusiones equivocadas. Por ello, el análisis de encuestas de hogares no puede desligarse de su diseño de selección: este no es un detalle técnico accesorio, sino el núcleo mismo que sustenta la validez de la información producida. "],["universo-de-estudio-y-población-objetivo.html", "2.1 Universo de estudio y población objetivo", " 2.1 Universo de estudio y población objetivo El término encuesta se encuentra directamente relacionado con una población finita compuesta de individuos a los cuales es necesario entrevistar. El universo de estudio lo constituye el total de individuos o elementos que poseen dichas características a ser estudiadas. Ahora bien, conjunto de unidades de interés sobre los cuales se tendrán resultados recibe el nombre de población objetivo. Por ejemplo, la Encuesta Nacional de Empleo y Desempleo de Ecuador define su población objetivo como todas las personas mayores de 10 años residentes en viviendas particulares en Ecuador. "],["unidades-de-análisis.html", "2.2 Unidades de análisis", " 2.2 Unidades de análisis Corresponden a los diferentes niveles de desagregación establecidos para consolidar el diseño probabilístico y sobre los que se presentan los resultados de interés. En México, la Encuesta Nacional de Ingresos y Gastos de los Hogares define como unidades de análisis el ámbito al que pertenece la vivienda, urbano alto, complemento urbano y rural. La Gran Encuesta Integrada de Hogres de Colombia tiene cobertura nacional y sus unidades de análisis están definidas por 13 grandes ciudades junto con sus áreas metropolitanas. "],["unidades-de-muestreo.html", "2.3 Unidades de muestreo", " 2.3 Unidades de muestreo El diseño de una encuesta de hogares en América Latina plantea la necesidad de seleccionar en varias etapas ciertas unidades de muestreo que sirven como medio para seleccionar finalmente a los hogares que participarán de la muestra. La Pesquisa Nacional por Amostra de Domicilios en Brasil se realiza por medio de una muestra de viviendas en tres etapas, cada etapa se define como una unidad de muestreo. Por ejemplo, las unidades de muestreo en PNAD son: Las unidades primarias de muestreo (UPM) son los municipios, Las unidades secundarias de muestreo (USM) son los sectores censales, que conforman una malla territorial conformada en el último Censo Demográfico. Las últimas unidades en ser seleccionadas son las viviendas. "],["marcos-de-muestreo.html", "2.4 Marcos de muestreo", " 2.4 Marcos de muestreo Para realizar el proceso de selección sistemática de los hogares es necesario contar con un marco de muestreo que sirva de link entre los hogares y las unidades de muestreo y que permita tener acceso a la población de interés. En este sentido, el marco muestral es el conjunto en el cual se identifican a todos los elementos que componen la población objeto de estudio, de la cual se selecciona la muestra. Los marcos de muestreo más utilizados en encuestas complejas son de áreas geográficas que vinculan directamente a los hogares o personas. A modo de ejemplo, la Encuesta Nacional de Hogares de Costa Rica utiliza un marco muestral construido a partir de los censos nacionales de población y vivienda de 2011. Dicho marco corresponde a uno de áreas en donde sus unidades son superficies geográficas asociadas con las viviendas. Este marco permite la definición de UPM con 150 viviendas en las zonas urbanas y 100 viviendas en las zonas rurales. Este marco está conformado por 10461 UPM (64.5% urbanas y 35.5% rurales). "],["motivación.html", "2.5 Motivación", " 2.5 Motivación Desde que se popularizaron las encuestas de hogares en 1940, se han hecho evidentes algunas tendencias vinculadas a los avances tecnológicos tanto en las agencias estadísticas como en la sociedad, las cuales se han acelerado con la introducción del computador. Gambino &amp; Silva (2009) El muestreo surge como respuesta a la necesidad de obtener información estadística precisa sobre una población objetivo, sin recurrir a un censo completo. Como señala Gutiérrez (2016), el muestreo consiste en investigaciones parciales sobre la población que permiten inferir resultados al conjunto total. En las últimas décadas, esta metodología se ha consolidado en distintos campos —especialmente en el sector gubernamental, con la producción de estadísticas oficiales que facilitan el seguimiento de políticas públicas y de los Objetivos de Desarrollo Sostenible—, pero también en el ámbito académico, privado y de comunicaciones. En el marco de este libro, el foco se centra en el análisis de encuestas de hogares. Para que el lector disponga de ejemplos prácticos, en este capítulo se empleará la base de datos BigCity, que contiene información socioeconómica de 150 266 personas de una ciudad en un año específico. Entre sus variables destacan: HHID: Identificador del hogar. PersonID: Identificador de la persona dentro del hogar. Stratum: Estrato geográfico (119 en total). PSU: Unidades primarias de muestreo (1664 en total). Zone: Zona urbana o rural. Sex: Sexo del entrevistado. Income: Ingreso mensual per cápita. Expenditure: Gasto mensual per cápita. Employment: Situación laboral. Poverty: Condición de pobreza según ingresos. 2.5.1 La importancia de considerar el diseño muestral Al analizar datos de encuestas de hogares, ignorar el diseño de muestreo compromete la representatividad, la precisión y la credibilidad de los resultados, lo que puede conducir a decisiones erróneas. Korn y Graubard (1995) muestran cómo las estimaciones ponderadas y no ponderadas pueden diferir sustancialmente, lo que resalta la necesidad de aplicar siempre métodos consistentes con el diseño. Como se mencionó, las encuestas de hogares se caracterizan por: Diseños de muestreo complejos (estratificación, conglomeración y probabilidades desiguales de selección), que buscan mejorar la eficiencia y la precisión. Pesos de muestreo para cada unidad, que permiten representar adecuadamente a la población. 2.5.1.1 Ejemplo ilustrativo Supóngase un país con dos regiones: la Región A (100 habitantes, ingreso promedio de $10 000) y la Región B (900 habitantes, ingreso promedio de $2 000). El ingreso promedio verdadero es: \\[ \\theta = \\frac{(100 \\times 10,000) + (900 \\times 2,000)}{100 + 900} = 2,800 \\] Si se encuestan 50 personas en cada región y se ignora el diseño de muestreo, asignando el mismo peso a todas las observaciones, se obtiene: \\[ \\hat{\\theta} = \\frac{(50 \\times 10,000) + (50 \\times 2,000)}{100} = 6,000 \\] El sesgo es evidente: se sobreestima el ingreso nacional, pues la Región A (10 % de la población) influye tanto como la Región B (90 %). En cambio, si se aplican pesos proporcionales al tamaño poblacional (2 para la Región A y 18 para la Región B), la estimación corregida es: \\[ \\hat{\\theta} = \\frac{(2 \\times 50 \\times 10,000) + (18 \\times 50 \\times 2,000)}{(2 \\times 50) + (18 \\times 50)} = 2,800 \\] Lo que reproduce el valor verdadero y elimina el sesgo. 2.5.1.2 Conglomeración y precisión Otra característica crítica es la conglomeración. La mayoría de encuestas selecciona unidades primarias de muestreo (UPM) como sectores censales o áreas de enumeración, y dentro de ellas, submuestras de hogares. Este diseño reduce costos, pero afecta la precisión: si los hogares dentro de un conglomerado son muy similares, la información adicional que aporta cada uno disminuye. Por ejemplo, si una encuesta selecciona 100 conglomerados y dentro de cada uno 10 hogares, se obtiene una muestra de 1 000 hogares. Si todos los hogares de un mismo conglomerado comparten la misma característica (p. ej., acceso a electricidad), la muestra ofrece la misma precisión que una muestra aleatoria simple de solo 100 hogares. Analizar ingenuamente los 1 000 hogares como si fueran independientes conduce a subestimar gravemente los errores estándar. 2.5.1.3 Recomendaciones prácticas Para un análisis adecuado es indispensable que las bases de datos de encuestas incluyan: Identificadores de estratos y UPM. Pesos de muestreo a nivel de hogar o persona. Cuando no se dispone de esta información, se recomienda que al menos se proporcionen pesos replicados, junto con instrucciones claras para calcular estimaciones y errores estándar. "],["parámetros-y-estimadores.html", "2.6 Parámetros y Estimadores", " 2.6 Parámetros y Estimadores Al analizar datos de encuestas, el primer paso es definir el parámetro de interés, un valor numérico fijo que describe una característica de toda la población (\\(U\\)). Ejemplos comunes incluyen el total y la media de la población. Dado que no es práctico observar a toda la población, se utilizan encuestas por muestreo para inferir estos parámetros a partir de una muestra (\\(s\\)). El enfoque de inferencia y estimación basado en el diseño se basa en la estructura de probabilidad del plan de muestreo. Las propiedades estadísticas de los estimadores, como la precisión y el sesgo, se evalúan en relación con la distribución de aleatorización que el diseño de muestreo genera. En el muestreo probabilístico, cada unidad de la población tiene una probabilidad de inclusión conocida. Estas probabilidades son la base para el cálculo de los pesos básicos de muestreo, que a su vez se utilizan para estimar los parámetros poblacionales a través de sumas ponderadas de los datos de la encuesta. Cuando se aplican correctamente, las estimaciones son insesgadas, lo que significa que en promedio coinciden con el valor real de la población si la encuesta se repitiera bajo las mismas condiciones. 2.6.1 Ajustes Adicionales de los Pesos A pesar de lo anterior, los pesos básicos de muestreo suelen necesitar ajustes adicionales para mejorar la precisión y robustez de las estimaciones. Los ajustes más comunes son: Ajuste por no respuesta: Los pesos de las unidades que sí respondieron se incrementan para representar también a las unidades seleccionadas que no participaron, lo cual ayuda a reducir el sesgo y a aumentar la fiabilidad de los resultados. Calibración: Los pesos se modifican para asegurar que las sumas ponderadas de variables de control (como edad o sexo, obtenidas de censos o proyecciones demográficas) se alineen con los valores poblacionales conocidos. Esto es una herramienta útil para identificar problemas de cobertura o no respuesta y reforzar la validez de las estimaciones. 2.6.2 Importancia de los Ajustes La calibración, en particular, es crucial porque permite comparar las estimaciones iniciales con los valores de referencia antes del ajuste. Esta comparación es una herramienta valiosa para detectar posibles problemas de cobertura o de no respuesta, lo que refuerza la validez de los resultados finales. "],["estimación-de-totales.html", "2.7 Estimación de totales", " 2.7 Estimación de totales La estimación de totales en encuestas constituye un paso central en el análisis estadístico aplicado a poblaciones finitas. Gran parte de los indicadores de interés para la formulación de políticas públicas, como el número de personas en situación de pobreza, el total de ocupados o el gasto agregado de los hogares, se derivan de un total poblacional. Por esta razón, comprender cómo se definen y estiman los totales resulta fundamental para garantizar la calidad y pertinencia de la información producida. En términos formales, si \\(y_k\\) denota el valor de una variable de interés para la unidad \\(k \\in U\\), el total poblacional se define como \\[ Y = \\sum_{U} y_k, \\] y su media poblacional como \\[ \\bar{Y} = \\frac{Y}{N}. \\] Dado que en la práctica solo se observa una muestra \\(s \\subset U\\), es necesario recurrir a estimadores que incorporen el diseño de muestreo. El estimador de Horvitz–Thompson (HT) es el más utilizado bajo el enfoque de diseño y se expresa como \\[ \\hat{Y}_{HT} = \\sum_{s} d_k y_k, \\qquad \\bar{y}_{HT} = \\frac{\\hat{Y}_{HT}}{\\hat{N}_{HT}}, \\quad \\hat{N}_{HT} = \\sum_{s} d_k, \\] donde \\(d_k = 1/\\pi_k\\) son los pesos básicos de diseño y \\(\\pi_k = \\Pr(k \\in s)\\) son las probabilidades de inclusión de primer orden. En la práctica, los pesos de diseño suelen modificarse para reflejar procesos adicionales como el ajuste por no respuesta o la calibración a totales poblacionales conocidos, obteniendo así los pesos ajustados \\(w_k\\). El reemplazo de \\(d_k\\) por \\(w_k\\) permite mejorar la precisión y reducir sesgos en las estimaciones, especialmente cuando existen fuentes auxiliares de información confiables. No obstante, toda estimación a partir de una muestra conlleva incertidumbre. Incluso cuando el estimador es insesgado, los resultados variarán de una muestra a otra debido al azar del diseño. Esta variabilidad se cuantifica mediante la varianza de muestreo, el error estándar (\\(se\\)) o el coeficiente de variación (\\(cv\\)). Estos indicadores son herramientas indispensables para evaluar la confiabilidad de los totales estimados y, por tanto, para interpretar de manera adecuada la información estadística. Bajo el enfoque de diseño, la varianza insesgada del estimador de Horvitz–Thompson puede expresarse como: \\[ \\hat{V}_p(\\hat{Y}_{HT}) = \\sum_{k \\in s} \\sum_{l \\in s} \\bigl( d_k d_l - d_{kl} \\bigr) y_k y_l, \\] donde \\(d_{kl} = 1/\\pi_{kl}\\) y \\(\\pi_{kl} = \\Pr(k,l \\in s)\\) representan las probabilidades conjuntas de inclusión. Esta expresión requiere que el diseño de muestreo cumpla \\(\\pi_{kl} &gt; 0\\) para todo par de unidades \\(k,l \\in U\\). En síntesis, la estimación de totales es la piedra angular sobre la cual se construyen indicadores más complejos. Su estudio permite entender tanto la lógica de los ponderadores como la necesidad de medir y comunicar la precisión de las estimaciones, lo que constituye un elemento esencial en la producción de estadísticas de calidad. 2.7.1 Ejemplo ilustrativo Para comprender de manera más tangible la importancia de considerar el diseño muestral en la estimación de totales y sus varianzas, analicemos un ejemplo sencillo. Supóngase una población de tamaño \\(N=6\\) y una muestra aleatoria simple de tamaño \\(n=3\\), seleccionada sin reemplazo, en la que se observan los valores \\((y\\_1=10, y\\_2=14, y\\_3=18)\\). Bajo este diseño, la varianza estimada del estimador de Horvitz–Thompson se calcula como \\[ \\hat{V}_{SRS}(\\hat{Y}_{HT}) = \\frac{N^2}{n}\\left(1-\\frac{n}{N}\\right)S_{(y_s)}^2 \\tag{9-5} \\] donde \\(S\\_{(y\\_s)}^2\\) corresponde a la varianza muestral de los valores observados. Sustituyendo en la expresión, se obtiene \\[ \\hat{V}_p(\\hat{Y}_{HT}) = \\frac{36}{3}\\left(1-\\frac{3}{6}\\right)16 = 96. \\] En contraste, si se ignora el diseño de muestreo, un analista inexperto podría calcular erróneamente la varianza mediante la fórmula simplificada: \\[ \\frac{N^2}{n}S_{(y_s)}^2 = 192, \\] lo que conduciría a una sobreestimación de la varianza por no considerar las características del diseño de selección. La estimación del total poblacional es \\(\\hat{Y}\\_{HT}=84\\). El error estándar correcto, calculado según el diseño de muestreo, es \\[ \\sqrt{\\hat{V}_p(\\hat{Y}_{HT})} = \\sqrt{96} \\approx 9.80. \\] En cambio, si la varianza se estimara usando un método ingenuo que ignore el diseño de muestreo, el intervalo de confianza resultante sería más amplio y desalineado, lo que podría conducir a inferencias erróneas. Este ejemplo evidencia claramente la relevancia de incorporar el diseño de muestreo al estimar varianzas, errores estándar e intervalos de confianza. Si bien la fórmula general para la estimación de la varianza \\(\\hat{V}*p\\) es aplicable a distintos diseños de muestreo, en la práctica rara vez se utiliza porque las probabilidades de inclusión de segundo orden \\(\\pi*{kl}\\) y los pesos pareados \\(d\\_{kl}\\) suelen ser desconocidos para los usuarios secundarios. Incluso los propios productores de datos evitan calcular estos valores, pues existen métodos más simples y eficientes para la estimación de varianzas, como el método linealizado, la replicación y el bootstrap, que permiten cuantificar la incertidumbre sin necesidad de contar con información tan detallada. "],["efecto-del-diseño-deff.html", "2.8 Efecto del diseño (DEFF)", " 2.8 Efecto del diseño (DEFF) De acuerdo con Kish (1965, p. 258), el efecto del diseño (DEFF) se define como la relación entre la varianza de un estimador calculado bajo un diseño de muestreo complejo y la varianza del mismo estimador cuando se emplea un muestreo aleatorio simple (MAS) con igual tamaño de muestra. Su estimación se formula como: \\[\\widehat{\\text{DEFF}} = \\frac{\\widehat{V}_{p}(\\hat{\\theta})}{\\widehat{V}_{\\text{SRS}}(\\hat{\\theta})}\\] donde \\(\\widehat{V}*{p}(\\hat{\\theta})\\) corresponde a la varianza estimada de \\(\\hat{\\theta}\\) bajo el diseño complejo \\(p(s)\\), mientras que \\(\\widehat{V}*{\\text{SRS}}(\\hat{\\theta})\\) representa la varianza estimada del mismo estimador bajo un MAS de igual tamaño. Este indicador permite cuantificar cuánto aumenta la varianza debido a la conglomeración y otras características propias de los diseños complejos en comparación con un muestreo simple. Según Naciones Unidas (2008, p. 49), el DEFF puede entenderse de tres maneras: como el factor de incremento de la varianza frente al MAS, como una medida de la pérdida relativa de precisión o como una indicación del aumento en el tamaño de la muestra que sería necesario en un diseño complejo para alcanzar el mismo nivel de varianza que en un MAS. Según Park et al. (2003), el efecto del diseño de una encuesta puede desglosarse en tres factores multiplicativos: Ponderación desigual: La presencia de pesos muestrales no uniformes suele incrementar ligeramente la varianza; por ello, el uso de pesos iguales resulta ventajoso y explica por qué los diseños auto-ponderados son preferidos en encuestas de hogares. Estratificación: Cuando se aplica correctamente, puede disminuir la varianza, aunque en la práctica su efecto reductor suele ser moderado. Muestreo en varias etapas: Generalmente incrementa la varianza, ya que las unidades dentro de un mismo conglomerado tienden a ser más homogéneas entre sí que en comparación con las de otros conglomerados. Al analizar encuestas, el DEFF se convierte en un indicador fundamental para medir la calidad de las estimaciones y orientar el diseño de estudios futuros. Un valor elevado evidencia que el diseño complejo introduce ineficiencias que incrementan la varianza y reducen la precisión de los resultados. Por el contrario, un valor cercano a uno indica que el diseño tiene un efecto mínimo sobre la varianza. Esta información permite a los investigadores identificar si es necesario ajustar la ponderación, optimizar la estratificación o modificar el tamaño del submuestreo para aumentar la eficiencia en levantamientos posteriores. La interpretación de un DEFF alto debe hacerse con precaución, ya que no siempre implica que el diseño muestral sea inadecuado. Es fundamental considerar el contexto de la encuesta: un valor superior a tres podría parecer alarmante, pero a menudo se debe a limitaciones prácticas como restricciones presupuestales, dificultades logísticas o la necesidad de garantizar la participación de los entrevistados. En ciertos levantamientos de hogares, puede ser indispensable seleccionar solo una fracción de los individuos elegibles en cada hogar. Además, problemas de cobertura o tasas de no respuesta pueden aumentar la variabilidad de los pesos muestrales y, en consecuencia, elevar los valores del DEFF. "],["muestreo-aleatorio-simple-en-dos-etapas-estratificado.html", "2.9 Muestreo aleatorio simple en dos etapas estratificado", " 2.9 Muestreo aleatorio simple en dos etapas estratificado Con la finalidad de mantener un equilibrio entre los costos económicos y las propiedades estadísticas de la estrategia de muestreo se puede aprovechar la homogeneidad dentro de los conglomerados y, así, no tener que realizar censos dentro de cada Unidad Primaria de Muestreo (UPM) sino, proceder a seleccionar una sub-muestra dentro del conglomerado seleccionado. Los diseños de muestreo en las encuestas de hogares se caracterizan por ser diseños complejos los cuales involucran, entre otras, más de una etapa en la selección de las unidades de observación, estratos y estimadores complejos. En su mayoría, las unidades primarias de muestreo son seleccionadas dentro de los estrato. Ahora bien, según la teoría de muestreo (Cochran, W. G., 1977) se asume que el muestreo en cada estrato respeta el principio de la independencia. Esto es, las estimaciones del total, así como el cálculo y estimación de la varianza son el resultado de añadir o sumar para cada estrato la respectiva cantidad. Dentro de cada estrato \\(U_h\\) con \\(h=1,\\ldots, H\\) existen \\(N_{Ih}\\) unidades primarias de muestreo, de las cuales se selecciona una muestra \\(s_{Ih}\\) de tamaño \\(n_{Ih}\\) mediante un diseño de muestreo aleatorio simple. Suponga, además que el sub-muestreo dentro de cada unidad primaria seleccionada es también aleatorio simple. En este sentido, para cada unidad primaria de muestreo seleccionada \\(i\\in s_{Ih}\\) de tamaño \\(N_i\\) se selecciona una muestra \\(s_i\\) de elementos de tamaño \\(n_i\\). Como es ampliamente conocido, el proceso de estimación de un parámetro particular, por ejemplo, la media de los ingresos consiste en multiplicar la observación obtenida en la muestra por su respectivo factor de expansión y dividirlo sobre la suma de los factores de expansión de acuerdo con el nivel de desagregación que se quiera estimar. Sin embargo, cuando el diseño es complejo como es el caso de las encuestas de hogares, la estimación de la varianza se torna un poco difícil de realizar utilizando ecuaciones cerradas. Para estos casos y como lo recomienda la literatura especializada (Hansen, M. H., &amp; Steinberg, J., 1956)), se procede a utilizar la técnica del último conglomerado. Esta técnica consiste en aproximar la varianza sólo teniendo en cuenta la varianza de los estimadores en la primera etapa. Para esto se debe suponer que el diseño de muestreo fue realizado con reemplazo. Para poder utilizar los principios de estimación del último conglomerado en las encuestas de hogares se definen las siguientes cantidades: \\(d_{I_i} = \\dfrac{N_{Ih}}{n_{Ih}}\\), que es el factor de expansión de la \\(i\\)-ésima UPM en el estrato \\(h\\). \\(d_{k|i} = \\dfrac{N_{i}}{n_{i}}\\), que es el factor de expansión del \\(k\\)-ésimo hogar para la \\(i\\)-ésima UPM. \\(d_k = d_{I_i} \\times d_{k|i} = \\dfrac{N_{Ih}}{n_{Ih}} \\times \\dfrac{N_{i}}{n_{i}}\\), que es el factor de expansión final del \\(k\\)-ésimo elemento para toda la población \\(U\\). "],["práctica-en-r.html", "2.10 Práctica en R", " 2.10 Práctica en R En esta sección se utilizarán las funciones estudiadas en el capítulo anterior para la manipulación de la base de datos de ejemplo. Inicialmente, se cargarán las librerías ggplot2 que permitirá generar gráficos de alta calidad en R, TeachingSampling que permite tomar muestras probabilísticas utilizando los diseños de muestreo usuales, survey y srvyr que permitirán definir los diseños muestrales y por último dplyr que permite la manipulación de las bases de datos. library(ggplot2) library(TeachingSampling) library(dplyr) library(survey) library(srvyr) Una vez cargada las librerías, se procede a calcular la cantidad de personas en la base de datos, el total de ingresos y total de gastos para cada UPM dentro de cada estrato: data(&#39;BigCity&#39;) FrameI &lt;- BigCity %&gt;% group_by(PSU) %&gt;% summarise(Stratum = unique(Stratum), Persons = n(), Income = sum(Income), Expenditure = sum(Expenditure)) attach(FrameI) head(FrameI, 10) PSU Stratum Persons Income Expenditure PSU0001 idStrt001 118 70911.72 44231.78 PSU0002 idStrt001 136 68886.60 38381.90 PSU0003 idStrt001 96 37213.10 19494.78 PSU0004 idStrt001 88 36926.46 24030.74 PSU0005 idStrt001 110 57493.88 31142.36 PSU0006 idStrt001 116 75272.06 43473.28 PSU0007 idStrt001 68 33027.84 21832.66 PSU0008 idStrt001 136 64293.02 47660.02 PSU0009 idStrt001 122 33156.14 23292.16 PSU0010 idStrt002 70 65253.78 37114.76 Ahora bien, para calcular los tamaños poblacionales de los estratos (NIh) y los tamaños de muestra dentro de cada estrato (nIh), se realiza de la siguiente manera: sizes = FrameI %&gt;% group_by(Stratum) %&gt;% summarise(NIh = n(), nIh = 2, dI = NIh/nIh) NIh &lt;- sizes$NIh nIh &lt;- sizes$nIh head(sizes, 10) Stratum NIh nIh dI idStrt001 9 2 4.5 idStrt002 11 2 5.5 idStrt003 7 2 3.5 idStrt004 13 2 6.5 idStrt005 11 2 5.5 idStrt006 5 2 2.5 idStrt007 14 2 7.0 idStrt008 7 2 3.5 idStrt009 8 2 4.0 idStrt010 8 2 4.0 Si se desea extraer una muestra probabilística bajo un diseño aleatorio simple estratificado, se procede a utilizar la función S.STSI de la librería TeachingSampling como se muestra a continuación: samI &lt;- S.STSI(Stratum, NIh, nIh) UI &lt;- levels(as.factor(FrameI$PSU)) sampleI &lt;- UI[samI] Ahora bien, con la función left_join se procede a pegar los tamaños muestrales a aquellas UPM’s que fueron seleccionadas en la muestra: FrameII &lt;- left_join(sizes, BigCity[which(BigCity$PSU %in% sampleI), ]) attach(FrameII) Una vez se tiene la base de datos con la muestra de UMP’s. se selecciona aquellas variables que son de inetrés para el estudio como sigue a continuación: head(FrameII, 10) %&gt;% select(Stratum:Zone) Stratum NIh nIh dI HHID PersonID PSU Zone idStrt001 9 2 4.5 idHH00082 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idHH00082 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idHH00082 idPer03 PSU0007 Rural idStrt001 9 2 4.5 idHH00082 idPer04 PSU0007 Rural idStrt001 9 2 4.5 idHH00083 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idHH00083 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idHH00084 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idHH00084 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idHH00084 idPer03 PSU0007 Rural idStrt001 9 2 4.5 idHH00084 idPer04 PSU0007 Rural Luego de tener la información muestral de la primera etapa en la base FrameII se procede a calcular los tamaños de muestra dentro de cada UPM’s. En este caso, a modo de ejemplo, se tomará el 10% del tamaño de la UPM y se utilizará la función ceiling la cual aproxima al siguiente entero. HHdb &lt;- FrameII %&gt;% group_by(PSU) %&gt;% summarise(Ni = length(unique(HHID))) Ni &lt;- as.numeric(HHdb$Ni) ni &lt;- ceiling(Ni * 0.1) sum(ni) ## [1] 693 Teniendo el vector de tamaños de muestra para cada UMP, se procede a realizar la selección mediante un muestreo aleatorio simple con la función S.SI de la librería TeachingSampling. A modo ilustrativo, la selección en la segunda etapa del diseño se realizará, inicialmente para la primera UPM. Posterior a eso, se realizará un ciclo “for” para hacerlo con las demás UPM’s. Para la primera UPM se realiza de la siguiente manera: sam = S.SI(Ni[1], ni[1]) clusterII = FrameII[which(FrameII$PSU == sampleI[1]),] sam.HH &lt;- data.frame(HHID = unique(clusterII$HHID)[sam]) clusterHH &lt;- left_join(sam.HH, clusterII, by = &quot;HHID&quot;) clusterHH$dki &lt;- Ni[1] / ni[1] clusterHH$dk &lt;- clusterHH$dI * clusterHH$dki sam_data = clusterHH head(sam_data, 10) %&gt;% select(Stratum:Zone) Stratum NIh nIh dI PersonID PSU Zone idStrt001 9 2 4.5 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idPer03 PSU0007 Rural idStrt001 9 2 4.5 idPer04 PSU0007 Rural idStrt001 9 2 4.5 idPer05 PSU0007 Rural idStrt001 9 2 4.5 idPer06 PSU0007 Rural idStrt001 9 2 4.5 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idPer03 PSU0007 Rural idStrt001 9 2 4.5 idPer01 PSU0007 Rural Para las demás UPM’s seleccionadas en la etapa 1, for (i in 2:length(Ni)) { sam = S.SI(Ni[i], ni[i]) clusterII = FrameII[which(FrameII$PSU == sampleI[i]),] sam.HH &lt;- data.frame(HHID = unique(clusterII$HHID)[sam]) clusterHH &lt;- left_join(sam.HH, clusterII, by = &quot;HHID&quot;) clusterHH$dki &lt;- Ni[i] / ni[i] clusterHH$dk &lt;- clusterHH$dI * clusterHH$dki data1 = clusterHH sam_data = rbind(sam_data, data1) } encuesta &lt;- sam_data attach(encuesta) Una vez se obtiene la muestra (como se mostró anteriormente), el paso siguiente es definir el diseño utilizado y guardarlo como un objeto en R para posteriormente poderlo utilizar y realizar el proceso de estimación de parámetros y cálculo de indicadores. Para realizar esta tarea, se utilizará el paquete srvyr el cual ya fue definido en el capítulo anterior. Para este ejemplo, el diseño de muestreo utilizado fue un estratificado-multietápico en el cual, los estratos correspondieron a la variable Stratum, las UPM’s correspondieron a la variable PSU, los factores de expansión están en la variable dk y por último, se le indica a la función as_survey_design que las UPM’s están dentro de los estrato con el argumento nest = T. A continuación, se presenta el código computacional: diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = dk, nest = T ) Ya definido el diseño de muestreo como un objeto de R se puede empezar a extraer información del mismo. Por ejemplo, se pueden extraer los pesos de muestreo de dicho diseño con la función weights y luego sumarlos para revisar hasta cuánto me está expandiendo mi muestra. El código es el siguiente: sum(weights(diseno)) ## [1] 144033.6 Como se puede observar, el tamaño poblacional estimado utilizando el diseño propuesto es de \\(140579.2\\). Sin embargo, el tamaño poblacional de la base BigCity es de \\(150266\\). Es normal que esto suceda pero debe ser corregido puesto que la suma de los factores de expansión debe sumar el total de la población. La solución para esto es calibrar los pesos de muestreo que se abordará a continuación. "],["calibrando-con-r.html", "2.11 Calibrando con R", " 2.11 Calibrando con R La calibración es un ajuste que se realiza a los pesos de muestreo con el propósito de que las estimaciones de algunas variables de control reproduzcan de forma perfecta los totales poblacionales de estas variables (Sarndal, 2003). Esta propiedad de consistencia es deseable en un sistema de ponderadores. En este sentido, cuando los estudios por muestreo están afectados por la ausencia de respuesta, como en muchos casos pasa en las encuestas de hogares, es deseable tener las siguientes propiedades en la estructura inferencial que sustenta el muestreo: Sesgo pequeño o nulo. Errores estándares pequeños. Un sistema de ponderación que reproduzca la información auxiliar disponible. Un sistema de ponderación que sea eficiente al momento de estimar cualquier característica de interés en un estudio multipropósito. La calibración es usualmente el último paso en el ajuste de los ponderadores. Hace uso de información auxiliar que reduce la varianza y corrige los problemas de cobertura que no pudieron ser corregidos en los pasos previos. Puesto que el estimador de calibración depende exclusivamente de la información auxiliar disponible, esta información puede aparecer en diversas formas: Puede estar de forma explícita en el marco de unidades. \\(x_k \\ (\\forall \\ k \\in U)\\) Puede ser un agregado poblacional proveniente de un censo o de registros administrativos. \\(t_x = \\sum_U x_k\\) Puede ser una estimación poblacional \\(\\hat{t}_x = \\sum_s w_kx_k\\) muy confiable. Particularmente, en encuestas de hogares, existen conteos de personas disponibles a nivel de desagregaciones de interés. Por ejemplo, número de personas por edad, raza y género que se permite utilizar como información auxiliar para calibrar las estimaciones. La necesidad de calibrar en las encuestas de hogares es porque no todos los grupos de personas se cubren apropiadamente desde el diseño de muestreo. Además, las estimaciones del número de personas en estos subgrupos son menores a las proyecciones que se tienen desde los censos. Por último, al ajustar los pesos para que sumen exactamente la cifra de los conteos censales, se reduce el sesgo de subcobertura. Para ejemplificar el estimador de calibración en R usando la base de datos de ejemplo se utilizarán la función calibrate del paquete survey. En primer lugar, para poder calibrar se requiere construir la información poblacional a la cual se desea calibrar. En este ejemplo se calibrará a nivel de zona y sexo. Por tanto, los totales se obtienen como sigue: library(survey) totales &lt;- colSums( model.matrix(~ -1 + Zone:Sex, BigCity)) En la salida anterior se puede observar que, por ejemplo, en la zona rural hay 37238 mujeres mientras que en la urbana hay 41952. De igual manera se puede leer para el caso de los hombres. Una vez obtenido estos totales, se procede a utilizar la función calibrate para calibrar los pesos de muestreo como sigue: diseno_cal &lt;- calibrate( diseno, ~ -1 + Zone:Sex, totales, calfun = &quot;linear&quot;) Luego de que se hayan calibrado los pesos se puede observar que, al sumar los pesos calibrados estos reproducen el total poblacional de la base de ejemplo. sum(weights(diseno_cal)) ## [1] 150266 encuesta$wk &lt;- weights(diseno_cal) Dado que uno de los principios de los pesos calibrados es que dichos pesos no sean muy diferentes a los pesos originales que provienen del diseño de muestreo, se puede observar a continuación, la distribución de los pesos, sin calibrar y calibrados respectivamente. par(mfrow = c(1,2)) hist(encuesta$dk) hist(encuesta$wk) plot(encuesta$dk,encuesta$wk) Region &lt;- as.numeric( gsub(pattern = &quot;\\\\D&quot;, replacement = &quot;&quot;, x = encuesta$Stratum)) encuesta$Region &lt;- cut(Region, breaks = 5, labels = c(&quot;Norte&quot;,&quot;Sur&quot;,&quot;Centro&quot;,&quot;Occidente&quot;,&quot;Oriente&quot;)) encuesta %&lt;&gt;% mutate( CatAge = case_when( Age &lt;= 5 ~ &quot;0-5&quot;, Age &lt;= 15 ~ &quot;6-15&quot;, Age &lt;= 30 ~ &quot;16-30&quot;, Age &lt;= 45 ~ &quot;31-45&quot;, Age &lt;= 60 ~ &quot;46-60&quot;, TRUE ~ &quot;Más de 60&quot; ), CatAge = factor( CatAge, levels = c(&quot;0-5&quot;, &quot;6-15&quot;, &quot;16-30&quot;, &quot;31-45&quot;, &quot;46-60&quot;, &quot;Más de 60&quot;), ordered = TRUE ) ) saveRDS(object = encuesta, file = &quot;../Curso Tellez/Data/encuesta.rds&quot;) "],["manejando-una-base-de-encuestas-de-hogares-con-r.html", "Capítulo 3 Manejando una base de encuestas de hogares con R ", " Capítulo 3 Manejando una base de encuestas de hogares con R "],["fundamentos-básicos-de-r-y-rstudio.html", "3.1 Fundamentos básicos de R y Rstudio", " 3.1 Fundamentos básicos de R y Rstudio R fue creado en 1992 en Nueva Zelanda por Ross Ihaka y Robert Gentleman. A manera introductoria, R es un software diseñado para realizar análisis estadístico tanto sencillos como complejos. Este software ha ganado popularidad en el gremio estadístico y no estadístico, puesto que su manejo es sencillo y, además, es de libre uso (puede descargarse en https://www.r-project.org), es decir, no requiere de ninguna licencia para su utilización. Como lo mencionan Santana Sepúlveda y Mateos Farfán (2014), R es un lenguaje de programación de libre distribución, bajo Licencia GNU, que se mantiene en un ambiente para el cómputo estadístico y gráfico. Este software está diseñado para utilizarse en distintos sistemas operativos como Windows, MacOS o Linux. El concepto de ambiente se enfoca en caracterizarlo como un sistema totalmente planificado y coherente, a diferencia de otros softwares de análisis de datos que suelen consistir en acumulaciones de herramientas específicas y poco flexibles. Ahora bien, R es un lenguaje de programación, por lo que su interfaz puede resultar poco amigable para quienes inician en el lenguaje. Para facilitar su uso, se creó RStudio, un Entorno de Desarrollo Integrado (IDE, por sus siglas en inglés). Esto significa que RStudio es un programa que permite manejar R de manera más cómoda y visual, optimizando la experiencia de análisis y programación. R es un software libre y de código abierto que ha ganado gran popularidad en el procesamiento de encuestas y la investigación social, convirtiéndose en una herramienta de elección para aplicar los desarrollos científicos y metodológicos más recientes en el análisis de datos de encuestas (R Core Team, 2024). Su carácter abierto permite que investigadores de todo el mundo aporten funciones y paquetes propios al Comprehensive R Archive Network (CRAN), poniéndolos a disposición de la comunidad académica y profesional. Entre sus recursos más destacados se encuentra el paquete samplesize4surveys (Gutiérrez, 2020), que facilita el cálculo de tamaños de muestra para individuos y hogares en encuestas repetidas, de panel y rotacionales. Asimismo, los paquetes sampling (Tillé y Matei, 2016) y TeachingSampling (Gutiérrez, 2015) ofrecen soporte para seleccionar muestras probabilísticas a partir de marcos de muestreo bajo diferentes diseños y algoritmos. Para el análisis de datos de encuestas de hogares, el paquete survey (Lumley, 2024) permite especificar el diseño muestral mediante la función svydesign() y obtener estimaciones correctas de errores estándar. El paquete convey (Pessoa et al., 2024) complementa este proceso al facilitar el cálculo de medidas de desigualdad. En el ámbito del modelado de regresiones, svydiags (Valliant, 2024) incluye herramientas de diagnóstico como análisis de residuos, valores de apalancamiento, factores de inflación de varianza y pruebas de colinealidad, mientras que PracTools (Valliant et al., 2025) proporciona utilidades para el cálculo del tamaño de muestra, el diseño de muestreo, la estimación de efectos de diseño y el análisis de componentes de varianza en esquemas multietápicos. "],["algunas-librerías-de-interés.html", "3.2 Algunas librerías de interés", " 3.2 Algunas librerías de interés Puesto que R es un lenguaje colaborativo el cual permite que la comunidad vaya haciendo aportes al desarrollo de funciones dentro de paquetes o librerías. Alguna de las librerías más usadas para el análisis de bases de datos son las siguientes: dplyr, dplyr es la evolución del paquete plyr, enfocada en herramientas para trabajar con marcos de datos (de ahí la d en el nombre). Según Hadley Wickham, las siguientes son las tres propiedades principales de la librería: Identificar las herramientas de manipulación de datos más importantes necesarias para el análisis de datos y hacerlas fáciles de usar desde R. Proporcionar un rendimiento ultrarrápido para los datos en memoria escribiendo piezas clave en C++. Utilizar la misma interfaz para trabajar con datos sin importar dónde estén almacenados, ya sea en un marco de datos, una tabla de datos o una base de datos.Esta librería permite manejar eficientemente las bases de datos. tidyverse, es una colección de paquetes disponibles en R y orientados a la manipulación, importación, exploración y visualización de datos y que se utiliza exhaustivamente en ciencia de datos. El uso de tidyverse permite facilitar el trabajo estadístico y la generación de trabajos reproducibles. Está compuesto de los siguientes paquetes: readr, dplyr, ggplot2, tibble, tidyr, purr, stringr, forcats readstata13, este paquete permite leer y escribir todos los formatos de archivo de Stata (versión 17 y anteriores) en un marco de datos R. Se admiten las versiones de formato de archivo de datos 102 a 119. para leer las bases de datos de STATA. Además, el paquete admite muchas características del formato Stata dta, como conjuntos de etiquetas en diferentes idiomas o calendarios comerciales. survey, este paquete ha sido elaborado por el Profesor Thomas Lumley (Lumley, T. 2011) y nos proporciona funciones en R útiles para analizar datos provenientes de encuestas complejas.Alguno de los parámetros que se pueden estimar usando este paquete son medias, totales, razones, cuantiles, tablas de contingencias, modelos de regresión, modelos loglineales, entre otros. srvyr, este paquete permite utilizar el operador pipe operators en las consultas que se realizan con el paquete survey. ggplot2, es un paquete de visualización de datos para el lenguaje R que implementa lo que se conoce como la Gramática de los Gráficos, que no es más que una representación esquemática y en capas de lo que se dibuja en dichos gráficos, como lo pueden ser los marcos y los ejes, el texto de los mismos, los títulos, así como, por supuesto, los datos o la información que se grafica, el tipo de gráfico que se utiliza, los colores, los símbolos y tamaños, entre otros. TeachingSampling, este paquete permite al usuario extraer muestras probabilísticas y hacer inferencias a partir de una población finita basada en varios diseños de muestreo. Entre los diseño empleados en esta librería están: Muestreo Aleatorio Simple (MAS), Muestreo Bernoullí, Muestreo Sistemático, PiPT, PPT, estre otros. samplesize4surveys, este paquete permite calcular el tamaño de muestra requerido para la estimación de totales, medias y proporciones bajo diseños de muestreo complejos. Antes de poder utilizar las diferentes funciones que cada librería tiene, es necesario descargarlas de antemano de la web. El comando install.packages permite realizar esta tarea. Note que algunas librerías pueden depender de otras, así que para poder utilizarlas es necesario instalar también las dependencias. install.packages(&quot;dplyr&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;readstata13&quot;) install.packages(&quot;survey&quot;) install.packages(&quot;srvyr&quot;) install.packages(&quot;ggplot2&quot;) install.packages(&quot;TeachingSampling&quot;) install.packages(&quot;samplesize4surveys&quot;) Una vez instaladas las librerías hay que informarle al software que vamos a utilizarlas con el comando library. Recuerde que es necesario haber instalado las librerías para poder utilizarlas. rm(list = ls()) library(&quot;dplyr&quot;) library(&quot;tidyverse&quot;) library(&quot;readstata13&quot;) library(&quot;survey&quot;) library(&quot;srvyr&quot;) library(&quot;ggplot2&quot;) library(&quot;TeachingSampling&quot;) library(&quot;samplesize4surveys&quot;) "],["cración-de-proyectos-en-r.html", "3.3 Cración de proyectos en R", " 3.3 Cración de proyectos en R Una vez se descargan e instalan las librerías o paquetes en R el paso siguientes es crear proyectos. Un proyecto de R se define como un archivo que contiene los archivos de origen y contenido asociados con el trabajo que se está realizando. Adicionalmente, contiene información que permite la compilación de cada archivo de R a utilizar, mantiene la información para integrarse con sistemas de control de código fuente y ayuda a organizar la aplicación en componentes lógicos. Ahora bien, por una cultura de buenas practicas de programación, se recomienda crear un proyecto en el cual se tenga disponible toda la información a trabajar. A continuación, se muestran los pasos para crear un proyecto dentro de RStrudio. Paso 1: Abrir RStudio. Paso 2: ir a file -&gt; New Project Paso 3: Tipos de proyecto. Para este ejemplo se tomará New Directory Tipos de proyectos Algo a tener en cuenta en este paso es que en New Directory RStudio brinda una variedad de opciones dependiendo las características del procesamiento que desea realizar. Ahora bien, si se cuenta con algunos código previamente desarrollados y se desea continuar con ese proyecto, se debe tomar la opción Existing Directory . Por último, Si se cuenta con cuenta en Git y se desea tener una copia de seguridad, se debe emplear la opción Version Control. Paso 4: Seleccionar el tipo de proyecto. Seleccionar el tipo de proyecto Paso 5: Diligenciar el nombre del proyecto y la carpeta de destino. Nombre de proyecto Al realizar esto pasos permite que todas rutinas creadas dentro del proyecto estén ancladas a la carpeta del proyecto. "],["lectura-de-las-bases-de-datos-y-manipulación.html", "3.4 Lectura de las bases de datos y manipulación", " 3.4 Lectura de las bases de datos y manipulación Es muy usual que al trabajar proyectos en R sea necesario importar bases de datos con información relevante para un estudio en particular. En Colombia, por ejemplo, en la Encuesta de Calidad de Vida (ECV, por sus siglas) es necesario, una vez se realiza el trabajo de campo, importar la información recolectada para poder ajustar los factores de expansión y posteriormente estimar los parámetros. Los formatos de bases de datos que R permite importar son diversos, entre ellos se tienen xlsx, csv, txt, STATA, etc. Particularmente, para la lectura de bases de datos provenientes de STATA 13 se realiza con la función read.dta13. Una vez leída la base de datos en el formato mencionado anteriormente se procede a transformar en el formato .RDS el cual es un formato más eficiente y propio de R. Para ejemplificar los procedimientos en R se utilizará la base de datos de Pesquisa Nacional por Amostra de Domicílios 2015 de Brasil la cual está en formato .dta el cual se lee en R con la función read.dta13. Posteriormente se transformará al formato .rds con la función saveRDS el cual es un formato propio de R y por último se cargar esta base. Lo pasos anteriores se realiza como sigue: Primero se carga la base en formato dta con la librería read.dta13 y se guarda en formato rds con la función saveRDS ` data1 &lt;- read.dta13(&quot;Z:/BC/BRA_2015N.dta&quot;) saveRDS(data1, &quot;../data/BRA_2015N.rds&quot;) Una vez guardada la base en nuestros archivos de trabajo, se procede a cargar la base a R con la función readRDS para poder utilizar toda la información que en ella se contiene. data2 &lt;- readRDS(&quot;Data/BRA_2015N.rds&quot;) Una vez cargada la base de datos en R ésta se puede empezar a manipular según las necesidades de cada investigador. En este sentido, una de las primeras revisiones que se realizan al cargar las bases de datos es revisar su dimensión, es decir, chequear la cantidad de filas y columnas que tenga la base. Lo anterior se puede hacer con la función nrow. Dicha función identifica el número de registros (unidades efectivamente medidas) en la base de datos y la función ncol muestra el número de variables en la base de datos. Los códigos computacionales son los siguientes: nrow(data2) ## [1] 356904 ncol(data2) ## [1] 109 Una forma resumida de revisar la cantidad de filas y columnas que tiene la base de datos es usar la función dim. Esta función nos devuelve un vector indicado en su primera componente la cantidad de fila y en su segundo la cantidad de columnas como se muestra a continuación: dim(data2) ## [1] 356904 109 Es usual que en las encuestas de hogares las bases de datos sean muy extensas, es decir, contengan una cantidad importante de variables medidas (filas) y por lo general, el tamaño de la muestra de estos estudios con grandes. Es por lo anterior que, para poder visualizar dichas bases una vez cargadas en R, es necesario hacerlo de manera externa. Esto es, abrir una pestaña diferente en R y hacer la navegación de la base como un texto plano. Lo anterior se realiza con la función View como se muestra a continuación: View(data2) Visor de bases de datos de RStudio Otro chequeo importante que se debe realizar al momento de cargar una base de datos en R es el reconocimiento de las variables que incluye. Esto se puede hacer utilizando la función names la cual identifica las variables de la base de datos. names(data2) La función names solo devuelve un vector un vector con los nombres de las variables que contiene la base. Sin embargo, si se quiere profundizar en qué información contiene cada variable, La función str muestra de manera compacta la estructura de un objeto y sus componentes. Para nuestra base se utilizaría de la siguiente manera: str(data2) Como se puede observar en la salida anterior, por ejemplo, la variable id_hogar es de tipo Entero al igual que id_pers mientras que cotiza_ee es un factor con 2 niveles. Como se observa, esta función es muy útil al momento de querer tener un panorama amplio del contenido y clase de cada variable en una base de datos, particularmente, en una encuesta de hogares en donde se tiene, por la misma estructura del estudio, muchas clases o tipos de variables medidas. "],["el-operador-pipe.html", "3.5 El operador pipe", " 3.5 El operador pipe El software estadístico R es un lenguaje de programación creado por estadísticos para estadísticos. Una de las contribuciones recientes es el desarrollo de los pipelines que permiten de una forma intuitiva generar consultas y objetos desde una base de datos. El operador pipe, %&gt;%, viene del paquete magrittr (Bache, S. et al., 2022) y está cargado automáticamente en los paquetes del Tidyverse. El objetivo del operador pipe es ayudar a escribir código de una manera que sea más fácil de leer y entender. En este sentido, el operador %&gt;% permite “encadenar” operaciones en el sentido que el resultado de una operación anterior se convierta en el input de la siguiente operación. A continuación, ejemplificaremos el uso del %&gt;% en la base de datos de Brasil haciendo un conteo del total de elementos que contiene la base de datos utilizando la función count. data2 %&gt;% count() ## n ## 1 356904 Otra operación que se puede realizar en R es re-codificar los niveles de los factores que en muchas ocasiones son necesarios en las encuestas de hogares. El siguiente código permite generar los nombres de los estados en Brasil. data2$estados &lt;- factor(data2$uf, levels = c(11:17, 21:29, 31:33, 35, 41:43, 50:53), labels = c(&quot;Rondonia&quot;, &quot;Acre&quot;, &quot;Amazonas&quot;, &quot;Roraima&quot;, &quot;Para&quot;, &quot;Amapa&quot;, &quot;Tocantins&quot;, &quot;Maranhao&quot;, &quot;Piaui&quot;, &quot;Ceara&quot;, &quot;RioGrandeNorte&quot;, &quot;Paraiba&quot;, &quot;Pernambuco&quot;, &quot;Alagoas&quot;, &quot;Sergipe&quot;, &quot;Bahia&quot;, &quot;MinasGerais&quot;, &quot;EspirituSanto&quot;, &quot;RioJaneiro&quot;, &quot;SaoPaulo&quot;, &quot;Parana&quot;, &quot;SantaCatarina&quot;, &quot;RioGrandeSur&quot;, &quot;MatoGrossoSur&quot;, &quot;MatoGrosso&quot;, &quot;Goias&quot;, &quot;DistritoFederal&quot;)) Adicionalmente, para efectos de visualización en tablas y gráficos es conviene codificar los nombres de las variables. Para este ejemplo, se codificarán de la siguiente manera: data2$deptos &lt;- factor(data2$uf, levels = c(11:17, 21:29, 31:33, 35, 41:43, 50:53), labels = c(&quot;RO&quot;, &quot;AC&quot;, &quot;AM&quot;, &quot;RR&quot;, &quot;PA&quot;, &quot;AP&quot;, &quot;TO&quot;, &quot;MA&quot;, &quot;PI&quot;, &quot;CE&quot;, &quot;RN&quot;, &quot;PB&quot;, &quot;PE&quot;, &quot;AL&quot;, &quot;SE&quot;, &quot;BA&quot;, &quot;MG&quot;, &quot;ES&quot;, &quot;RJ&quot;, &quot;SP&quot;, &quot;PR&quot;, &quot;SC&quot;, &quot;RS&quot;, &quot;MS&quot;, &quot;MT&quot;, &quot;GO&quot;, &quot;DF&quot;)) Por otro lado, existe una gama amplia de funciones que se pueden utilizar con el operador %&gt;%, A continuación, se enlistan una serie de funciones muy útiles al momento de hacer análisis con bases de datos provenientes de encuestas de hogares: filter: mantiene un criterio de filtro sobre alguna variable o mezcla de variables. select: selecciona columnas por nombres. arrange: ordena las filas de la base de datos. mutate: añade nuevas variables a la base de datos. summarise: reduce variables a valores y los presenta en una tabla. group_by: ejecuta funciones y agrupa el resultado por las variables de interés. Ejemplificando alguna de las funciones mostradas anteriormente, una de las primeras consultas que se realizan en las encuestas de hogares es saber el número de encuestas (personas) realizadas y que están contenida en la base de datos. Usando %&gt;% se realiza de la siguiente manera: data2 %&gt;% count() ## n ## 1 356904 Otro de los ejercicios que se hacen usualmente con las encuestas de hogares está relacionado con saber la cantidad de hogares que hay en el país de estudio. Una de las formas más sencillas de hacer esta revisión es usar la función filter. Las encuestas de hogares muchas veces recopilan información a nivel de viviendas, hogares y personas. Particularmente, las bases de datos que están disponibles en BADEHOG están a nivel de persona. Ahora bien, para saber la cantidad de hogares que se encuestaron basta con filtrar por hogar porque sólo hay un jefe de hogar por hogar, como se muestra a continuación: datahogar1 &lt;- data2 %&gt;% filter(parentco == 1) datahogar2 &lt;- data2 %&gt;% filter(paren_ee == &quot;Jefe&quot;) Por otro lado, si el interés ahora es filtrar la base de datos por la ubicación de la persona en el área rural y urbana se realiza de la siguiente manera: dataurbano &lt;- data2 %&gt;% filter(area_ee == &quot;Area urbana&quot;) datarural &lt;- data2 %&gt;% filter(area_ee == &quot;Area rural&quot;) En este mismo sentido, si el objetivo ahora es filtrar la base de datos por algunos ingresos particulares mensuales por personas, por ejemplo, altos o bajos, se realiza de la siguiente manera: dataingreso1 &lt;- data2 %&gt;% filter(ingcorte %in% c(50, 100)) dataingreso2 &lt;- data2 %&gt;% filter(ingcorte %in% c(1000, 2000)) Otra función muy útil en el análisis en encuestas de hogares es la función select la cual, como se mencionó anteriormente permite seleccionar un grupo de variables de interés a analizar. Si por ejemplo, se desea seleccionar de la base de ejemplo solo las variables identificación del hogar (id_hogar), unidades primarias de muestreo (_upm), factores de expansión (_feh) y estratos muestrales ( _estrato) se realiza de la siguiente manera: datared &lt;- data2 %&gt;% select(`id_hogar`, `_upm`, `_feh`, `_estrato`) datablue &lt;- data2 %&gt;% select(id_pers, edad, sexo, ingcorte) La función select no solo sirve para seleccionar variables de una base de datos, también se puede utilizar para eliminar algunas variables de la base de datos que ya no son de interés para el análisis o que simplemente se generaron en la manipulación de la base de datos como variables puentes para realizar algunos cálculos de interés. Por ejemplo, si se desea eliminar de la base de datos de ejemplo las variables identificación del hogar (id_hogar) e identificación de las personas (id_pers) se realiza introduciendo un signo “menos” (-) delante del nombre de la variable como sigue: datagrey &lt;- data2 %&gt;% select(-id_hogar, -id_pers) Por otro lado, si el objetivo ahora en análisis de las encuestas de hogares es ordenar las filas de la base por alguna variable en particular, se utiliza en R la función arrange para realizar esta operación. A continuación, se ejemplifica con la base de datos de ejemplo, cómo se ordena la base de acuerdo con la variable ingcorte: datadog &lt;- datablue %&gt;% arrange(ingcorte) datadog %&gt;% head() ## id_pers edad sexo ingcorte ## 1 1 38 Mujer 0 ## 2 2 12 Mujer 0 ## 3 1 26 Hombre 0 ## 4 2 29 Mujer 0 ## 5 1 50 Hombre 0 ## 6 1 53 Mujer 0 Es posible utilizar la función arrange para hacer ordenamientos más complicados. Por ejemplo, ordenar por más de una variable. A modo de ejemplo, ordenemos la base de datos datablue de acuerdo con las variables sexo y edad datablue %&gt;% arrange(sexo, edad) %&gt;% head() ## id_pers edad sexo ingcorte ## 1 6 0 Hombre 660.4400 ## 2 6 0 Hombre 162.5000 ## 3 3 0 Hombre 381.6667 ## 4 5 0 Hombre 320.0000 ## 5 6 0 Hombre 375.0000 ## 6 4 0 Hombre 1425.0000 También es posible utilizar la función arrange junto con la opción desc() para que el ordenamiento sea descendente. datablue %&gt;% arrange(desc(edad)) %&gt;% head() ## id_pers edad sexo ingcorte ## 1 2 115 Mujer 103.0000 ## 2 4 110 Mujer 1156.5300 ## 3 2 107 Hombre 415.5904 ## 4 1 107 Mujer 1754.4600 ## 5 3 105 Mujer 380.7904 ## 6 2 105 Mujer 898.3200 "],["funciones-mutate-summarise-y-group_by-en-encuestas-de-hogares.html", "3.6 Funciones mutate, summarise y group_by en encuestas de hogares", " 3.6 Funciones mutate, summarise y group_by en encuestas de hogares Las funciones mutate, summarise y group_by están cargadas en el paquete tidyverse y son muy importantes al momento de realizar análisis en encuestas de hogares. En primer lugar, la función mutate permite computar transformaciones de variables en una base de datos. Usualmente, en las encuestas de hogares es necesario crear nuevas variables, por ejemplo, si el hogar está en estado de pobreza extrema o no la cual se calcula a partir de los ingresos del hogar, la función mutate proporciona una interface clara para realizar este tipo de operaciones. A modo de ejemplo, utilizaremos la base de ejemplo para crear una nueva variable llamada ingreso2 la cual es el doble de los ingresos por persona dentro de un hogar. Los códigos computacionales se muestran a continuación: datablue2 &lt;- datablue %&gt;% mutate(ingreso2 = 2 * ingcorte) datablue2 %&gt;% head() ## id_pers edad sexo ingcorte ingreso2 ## 1 1 23 Hombre 800.0 1600.0 ## 2 1 23 Mujer 1150.0 2300.0 ## 3 1 35 Mujer 904.4 1808.8 ## 4 2 34 Hombre 904.4 1808.8 ## 5 3 11 Mujer 904.4 1808.8 ## 6 4 7 Mujer 904.4 1808.8 No solo se puede crear una nueva variable, si es necesario, se pueden crear más de una variable en la base de datos. Cabe recalcar que la función mutate reconoce sistemáticamente las variables que van siendo creadas de manera ordenada. A continuación, se presenta cómo crear más de una nueva variable en la base de datos: datacat &lt;- datablue %&gt;% mutate(ingreso2 = 2 * ingcorte, ingreso4 = 2 * ingreso2) datacat %&gt;% head() ## id_pers edad sexo ingcorte ingreso2 ingreso4 ## 1 1 23 Hombre 800.0 1600.0 3200.0 ## 2 1 23 Mujer 1150.0 2300.0 4600.0 ## 3 1 35 Mujer 904.4 1808.8 3617.6 ## 4 2 34 Hombre 904.4 1808.8 3617.6 ## 5 3 11 Mujer 904.4 1808.8 3617.6 ## 6 4 7 Mujer 904.4 1808.8 3617.6 Ahora bien, la función summarise funciona de forma similar a la función mutate, excepto que en lugar de añadir nuevas columnas crea un nuevo data frame. Como se mencionó anteriormente esta función sirve para resumir o “colapsar filas”. Toma un grupo de valores como input y devuelve un solo valor; por ejemplo, hallar la media de los ingresos, percentiles o medidas de dispersión. Por otro lado, la función group_by permite agrupar información de acuerdo con una(s) variable(s) de interés. El siguiente código permite generar el número de encuestas efectivas en cada uno de los estados de Brasil. El comando group_by agrupa los datos por estados, el comando summarise hace los cálculos requeridos y el comando arrange ordena los resultados data2 %&gt;% group_by(estados) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) %&gt;% head() ## # A tibble: 6 × 2 ## estados n ## &lt;fct&gt; &lt;int&gt; ## 1 SaoPaulo 40008 ## 2 MinasGerais 32933 ## 3 RioGrandeSur 26259 ## 4 Bahia 26155 ## 5 RioJaneiro 25858 ## 6 Para 22489 Hay otro tipos de análisis que se quieren realizar en encuestas de hogares, por ejemplo, generar el número de encuestas efectivas discriminado por el sexo del respondiente. A continuación, se presenta el código computacional: data2 %&gt;% group_by(sexo) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## # A tibble: 2 × 2 ## sexo n ## &lt;fct&gt; &lt;int&gt; ## 1 Mujer 183681 ## 2 Hombre 173223 Si ahora se desea realizar la consulta del número de encuestas efectivas por área geográfica, se realiza de la siguiente manera: data2 %&gt;% group_by(area_ee) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## # A tibble: 2 × 2 ## area_ee n ## &lt;fct&gt; &lt;int&gt; ## 1 Area urbana 304564 ## 2 Area rural 52340 Otras consultas que se realizan de manera frecuente en encuestas de hogares es reporta el número efectivo de encuestas clasificado por parentezco (jefe de hogar, hijos, conyugues, etc) data2 %&gt;% group_by(paren_ee) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## # A tibble: 6 × 2 ## paren_ee n ## &lt;fct&gt; &lt;int&gt; ## 1 Hijos 126206 ## 2 Jefe 117939 ## 3 Cónyuge 73725 ## 4 Otros parientes 36508 ## 5 Otros no parientes 2342 ## 6 Servicio doméstico 184 "],["medidas-descriptivos-y-reflexiones.html", "3.7 Medidas descriptivos y reflexiones", " 3.7 Medidas descriptivos y reflexiones En estadística, según Tellez Piñerez, C. F., &amp; Lemus Polanía, D. F. (2015) las medidas descriptivas permiten la presentación y caracterización de un conjunto de datos con el fin de poder describir apropiadamente las diversas características presentes en la información de la muestra. Involucra cualquier labor o actividad para resumir y describir los datos univariados o multivariados sin tratar de hacer inferencia más allá de los mismos. Este tipo de análisis son primordiales en cualquier encuesta de hogares dado que, permiten tener una idea inicial del comportamiento de la población en ciertas variables de estudio. A continuación, se presentan las funciones básicas en R para realizar análisis descriptivo. Media: mean() Mediana: median() Varianza: var() Desviación estándar: sd() Percentiles: quantile() Algunas medidas descriptivas: summary() Covarianza: cov( , ) Correlación: cor( , ) Ahora bien, para continuar con lo análisis de las encuestas de hogares es necesario que el lector tenga claro algunos conceptos básicos en el muestreo probabilístico. A continuación, se dan unas definiciones básicas: ¿Qué es una encuesta? Según Groves, R. M., et al (2011) una encuesta es un método sistemático para recopilar información de una muestra de elementos con el propósito de construir descriptores cuantitativos de los parámetros de la población. ¿Qué es una muestra? La definición más básica de una muestra es un subconjunto de la población. Esta definición es muy general dado que, no es específico de si la muestra es representativa de una población o no. ¿Qué es una muestra representativa? Según Gutiérrez (2016) una muestra representativa es un modelo reducido de la población y de aquí se desprende un argumento de validez sobre la muestra. En pocas palabras, se desea que la muestra representativa tenga la cantidad de información suficiente para poder hacer una inferencia adecuada a la población. ¿Está bien sacar conclusiones sobre una muestra? Si la muestra es representativa, las conclusiones que se obtienen de la población utilizando las técnicas de muestreo adecuadas, son correctas. Sin embargo, si se toma una muestra no representativa, no es correcto realizar inferencias dado que estas no representan la realidad de la población. "],["algunas-reflexiones-generales.html", "3.8 Algunas reflexiones generales", " 3.8 Algunas reflexiones generales Como se mencionó anteriormente, antes de realizar los análisis en las encuestas de hogares es necesario hacernos algunas preguntas que nos permiten dar claridad de los análisis que se desean hacer. A continuación, se presentan las preguntas: Si calculamos el promedio de los ingresos en una encuesta, ¿qué significa esa cifra? Esta cifra representa los ingresos medios que reportaron las personas entrevistadas en el estudio. En ningún momento se puede hablar de que este valor representa a la población a la cual queremos hacer inferencia. Para poder realizar las conclusiones a nivel poblacional se deben utilizar los factores de expansión que se obtuvieron empleando el diseño muestral. Si calculamos el total de los ingresos en una encuesta, ¿qué significa esa cifra? Similar a lo anterior, significa los ingresos totales que reportaron los entrevistados en el estudio. Se recalca que, bajo ninguna circunstancia se puede inferir que este valor muestral representa a la población de estudio. ¿Qué necesitamos para que la inferencia sea precisa y exacta? Se requiere de un buen diseño muestral, que la muestra que se recolecte sea representativa de la población en estudio y que el tamaño de muestra sea suficiente para poder inferir en todas las desagregaciones, tanto geográficas como temáticas que se plantearon en el diseño muestral. ¿Qué es el principio de representatividad? La representatividad es la característica más importante de una muestra probabilística, y se define como la capacidad que tiene una muestra de poder representar a la población a la cual se desea hacer inferencia. En este sentido, el muestreo adquiere todo su sentido en cuanto se garantice que las características que se quieren medir en la población quedan reflejadas adecuadamente en la muestra. Cabe resaltar que, una muestra representativa no es aquella que se parece a la población, de tal forma que las categorías aparecen con las mismas proporciones que en la población dado que, en algunas ocasiones es fundamental sobre-representar algunas categorías o incluso seleccionar unidades con probabilidades desiguales para poderlas medir con precisión (Tillé, 2006) ¿Qué es el factor de expansión? Según Guitiérrez (2016) el factor de expansión es el número de elementos menos uno de la población (no incluidos en la muestra) representados por el elemento incluido. También se conoce como el inverso de la probabilidad de inclusión. Dadas las definiciones hechas anteriormente, una encuesta de hogares requiere el análisis de todas las variables que dispuestas en la encuesta. Este proceso debe ser llevado a cabo por separado para asegurar la calidad y consistencia de los datos recolectados. Sin embargo, no vamos a adentrarnos en el análisis de las variables en la muestra, porque los datos muestrales no son de interés para el investigador. El interés se centra en lo que suceda a nivel poblacional y este análisis se debe abordar desde la teoría del muestreo. "],["observación-importante.html", "3.9 ¡Observación importante!", " 3.9 ¡Observación importante! Los siguientes resultados no tienen interpretación poblacional y se realizan con el único propósito de ilustrar el manejo de las bases de datos de las encuestas. "],["medias-y-totales.html", "3.10 Medias y totales", " 3.10 Medias y totales La función summarise permite conocer el total de los ingresos en la base de datos y la media de los ingresos sobre los respondientes. data2 %&gt;% summarise(total.ing = sum(ingcorte), media.ing = mean(ingcorte)) ## total.ing media.ing ## 1 422286293 1183.193 También se puede calcular medias de manera agrupada. Particularmente, si se desea calcular la media de los ingresos por área se hace de la siguiente manera: data2 %&gt;% group_by(area_ee) %&gt;% summarise(n = n(), media = mean(ingcorte)) ## # A tibble: 2 × 3 ## area_ee n media ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Area urbana 304564 1278. ## 2 Area rural 52340 634. Si ahora el análisis de los ingresos se desea hacer por sexo se realiza de la siguiente manera: data2 %&gt;% group_by(sexo) %&gt;% summarise(n = n(), media = mean(ingcorte)) ## # A tibble: 2 × 3 ## sexo n media ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Hombre 173223 1192. ## 2 Mujer 183681 1174. "],["medianas-y-percentiles.html", "3.11 Medianas y percentiles", " 3.11 Medianas y percentiles La función summarise también permite conocer algunas medidas de localización de los ingresos en la base de datos. data2 %&gt;% summarise(mediana = median(ingcorte), decil1 = quantile(ingcorte, 0.1), decil9 = quantile(ingcorte, 0.9), rangodecil = decil9 - decil1) ## mediana decil1 decil9 rangodecil ## 1 732.8571 244.8872 2308.5 2063.613 "],["varianza-desviación-estándar-y-rangos.html", "3.12 Varianza, desviación estándar y rangos", " 3.12 Varianza, desviación estándar y rangos Utilizando la función summarise podemos conocer también el comportamiento variacional de los ingresos sobre los respondientes. data2 %&gt;% summarise(varianza = var(ingcorte), desv = sd(ingcorte)) ## varianza desv ## 1 3407496 1845.94 data2 %&gt;% summarise(mini = min(ingcorte), maxi = max(ingcorte), rango = maxi - mini, rangoiq = IQR(ingcorte)) ## mini maxi rango rangoiq ## 1 0 171000 171000 869.8312 Ahora bien, si se desea realizar el cálculo de la media, la desviación estándar y el rango de los ingresos por hogares, se realiza de la siguiente manera: data2 %&gt;% filter(paren_ee == &quot;Jefe&quot;) %&gt;% group_by(sexoj) %&gt;% summarise(n = n(), media = mean(ingcorte), desv = sd(ingcorte), rangoiq = IQR(ingcorte)) ## # A tibble: 2 × 5 ## sexoj n media desv rangoiq ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jefe hombre 70154 1456. 2325. 1026. ## 2 Jefa mujer 47785 1334. 2076. 943. y por condicción de ocupación se realizaría: data2 %&gt;% group_by(condact) %&gt;% summarise(n = n(), media = mean(ingcorte), desv = sd(ingcorte), rangoiq = IQR(ingcorte)) ## # A tibble: 4 × 5 ## condact n media desv rangoiq ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1 22937 764. 1136. 524. ## 2 1 165325 1458. 2191. 1028. ## 3 2 17896 695. 949. 497. ## 4 3 150746 1003. 1527. 706. a nivel de hogar: data2 %&gt;% filter(paren_ee == &quot;Jefe&quot;) %&gt;% group_by(condact) %&gt;% summarise(n = n(), media = mean(ingcorte), desv = sd(ingcorte), rangoiq = IQR(ingcorte)) ## # A tibble: 3 × 5 ## condact n media desv rangoiq ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 77852 1526. 2459. 1096. ## 2 2 4469 535. 778. 441. ## 3 3 35618 1256. 1730. 880. Si se desea hacer un descriptivo a nivel de hogar para el ingreso se realizaría de la siguiente manera: data2 %&gt;% filter(paren_ee == &quot;Jefe&quot;) %&gt;% group_by(pobreza) %&gt;% summarise(n = n(), media = mean(ingcorte), desv = sd(ingcorte), rangoiq = IQR(ingcorte)) ## # A tibble: 3 × 5 ## pobreza n media desv rangoiq ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Pobreza extrema 3918 79.9 52.7 88.9 ## 2 Pobreza no extrema 13688 269. 62.5 107. ## 3 Fuera de la pobreza 100333 1614. 2355. 1055. "],["análisis-de-las-variables-continuas-en-encuestas-de-hogares.html", "Capítulo 4 Análisis de las variables continuas en encuestas de hogares", " Capítulo 4 Análisis de las variables continuas en encuestas de hogares Los desarrollos estadísticos están en permanente evolución, surgiendo nuevas metodologías y desarrollando nuevos enfoques en el análisis de encuestas. Estos desarrollos parten de la academia, luego son adoptados por las empresas (privadas o estatales) y entidades estatales, las cuales crean la necesidad que estos desarrollos sean incluidos en software estadísticos licenciados, proceso que puede llevar mucho tiempo. Algunos investigadores para acortar los tiempos y poner al servicio de la comunidad sus descubrimientos y desarrollos, hacen la implementación de sus metodologías en paquetes estadísticos de código abierto como R o Python. Teniendo R un mayor número de desarrollos en el procesamiento de las encuestas. Como se ha venido mencionando anteriormente, dentro del software R se disponen de múltiples librerías para el procesamiento de encuestas, estas varían dependiendo del enfoque de programación desarrollado por el autor o la necesidad que se busque suplir. Como es el objetivo de este libro y como se ha venido trabajando en los capítulos anteriores nos centraremos en las libreria survey y srvyr. Se incluiran más librerías de acuerdo a las necesidades que se presenten. "],["lectura-de-bases-de-datos-y-definición-del-diseño-muestral.html", "4.1 Lectura de bases de datos y definición del diseño muestral", " 4.1 Lectura de bases de datos y definición del diseño muestral Las bases de datos (tablas de datos) pueden estar disponibles en una variedad de formatos (.xlsx, .dat, .csv, .sav, .txt, etc.). Sin embargo, por experiencia es recomendable leer cualquiera de estos formatos y proceder inmediatamente a guardarlos en un archivo de extensión .rds, que es nativa de R. Los archivos .rds permiten almacenar cualquier objeto o información en R, como marcos de datos, vectores, matrices, listas, entre otros. Se caracterizan por su flexibilidad y compatibilidad completa con R, lo que facilita su reutilización en análisis posteriores. Por otro lado, existen otros tipos de archivos propios de R, como .Rdata. La diferencia principal es que los archivos .rds contienen un solo objeto, mientras que .Rdata puede almacenar múltiples objetos. Por ello, se recomienda trabajar preferentemente con archivos .rds. Para ejemplificar la sintaxis que se utilizará en R, se tomará la misma base del capítulo anterior, que contiene una muestra de 2,427 registros y proviene de un muestreo complejo. A continuación, se muestra cómo cargar un archivo con extensión .rds: library(tidyverse) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) head(encuesta) ## HHID Stratum NIh nIh dI PersonID PSU Zone Sex Age MaritalST ## 1 idHH00031 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Male 68 Married ## 2 idHH00031 idStrt001 9 2 4.5 idPer02 PSU0003 Rural Female 56 Married ## 3 idHH00031 idStrt001 9 2 4.5 idPer03 PSU0003 Rural Female 24 Married ## 4 idHH00031 idStrt001 9 2 4.5 idPer04 PSU0003 Rural Male 26 Married ## 5 idHH00031 idStrt001 9 2 4.5 idPer05 PSU0003 Rural Female 3 &lt;NA&gt; ## 6 idHH00041 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Female 61 Widowed ## Income Expenditure Employment Poverty dki dk wk Region CatAge ## 1 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte Más de 60 ## 2 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 46-60 ## 3 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 16-30 ## 4 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte 16-30 ## 5 409.87 346.34 &lt;NA&gt; NotPoor 8 36 33.63761 Norte 0-5 ## 6 823.75 392.24 Employed NotPoor 8 36 33.63761 Norte Más de 60 Según Naciones Unidas (2005, sec. 7.8), es fundamental que la estructura de los diseños de muestreo complejos se tenga en cuenta en el proceso de inferencia al estimar estadísticas oficiales basadas en encuestas de hogares. Ignorar este aspecto puede generar estimaciones sesgadas y errores de muestreo subestimados. En este contexto, los programas estadísticos ofrecen funcionalidades clave para el manejo de datos provenientes de este tipo de diseños (Heeringa, West y Berglund, 2017, Apéndice A). De manera general, herramientas estadísticas como R, Stata, SAS y SPSS cuentan con módulos y bibliotecas que optimizan la estimación de varianzas en muestras complejas, incorporando métodos de replicación para varianzas basadas en el diseño. Mientras que R es de acceso libre, los otros programas requieren licencias de pago. Estas plataformas permiten calcular estadísticas descriptivas (medias, totales, proporciones, percentiles, razones) y ajustar modelos de regresión considerando la estructura del diseño de la encuesta. Los programas especializados generan automáticamente el efecto del diseño, facilitando la interpretación de la variabilidad de las estimaciones. El uso de estos paquetes implica que el usuario suministre información clave del diseño muestral, como factores de expansión, estratificación e identificadores de conglomerados. A continuación, se ofrece un resumen general, aunque no exhaustivo, de las funciones y posibilidades que brindan los principales programas estadísticos para el análisis de encuestas complejas. Una vez cargada la muestra de hogares en R, el siguiente paso es definir el diseño muestral del cual proviene dicha muestra. Para ello se utilizará el paquete srvyr, que como se mencionó anteriormente, surge como un complemento de survey. Estas librerías permiten definir objetos tipo survey.design, a los que se aplican las funciones de estimación y análisis de encuestas, y que pueden ser combinadas con la programación de tubería (%&gt;%) del paquete tidyverse. A manera de ejemplificar los conceptos mencionados, a continuación se define en R el diseño de muestreo del cual proviene la muestra contenida en el objeto encuesta: options(survey.lonely.psu = &quot;adjust&quot;) library(srvyr) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T) En el código anterior se puede observar que, en primera instancia se debe definir la base de datos en la cual se encuentra la muestra seleccionada. Seguido de eso, se debe definir el tipo de objeto en R con el cual se trabajará, para nuestro caso, será un objeto survey_design el cual se define usando la función as_survey_design. ahora bien, una vez definido el tipo de objeto se procede a definir los parámetros del diseño definido. Para este caso fue un diseño de muestreo estratificado y en varias etapas. Estos argumentos se definen dentro de la función as_survey_design como sigue. Para definir los estratos de utiliza el argumento strata y se define en qué columna están los estratos en mi base de datos. Ahora bien, para definir las UPM´s, en el argumento ids se definen la columna donde se encuntran los conglomerados seleccionados en la primera etapa. También, se definen los pesos de muestreo en el argumento weights y, por último, con el argumento nest=T se define que las UPM´s están dentro de los estratos. "],["análisis-gráfico-histogramas-y-boxplot.html", "4.2 Análisis gráfico: Histogramas y Boxplot", " 4.2 Análisis gráfico: Histogramas y Boxplot Una vez cargada la muestra a R y definido el diseño muestral del cual proviene se pueden hacer los primeros análisis. Como es natural, se inician con análisis gráficos. A continuación, se muestran los códigos computacionales con los cuales se hacen histogramas en R para la variable ingresos teniendo en cuenta el diseño muestral y los factores de expansión haciendo uso la función svyhist de la librería survey. library(survey) library(srvyr) svyhist( ~ Income , diseno, main = &quot;Ingresos por hogar&quot;, col = &quot;grey80&quot;, xlab = &quot;Ingreso&quot;, probability = FALSE ) Como se pudo observar en el código anterior, para generar un histograma teniendo en cuenta el diseño muestral se usó la función svyhist. En primer lugar, se definió la variable a graficar, que para nuestro caso fue Income. Seguido, se define el diseño muestral utilizado en la encuesta. Luego, se definen los argumentos relacionados con la estética del gráfico como lo son: el título principal (main), el color (col) y el título horizontal (xlab). Finalmente, se define si el histograma es de frecuencias o probabilidades con el argumento probability. Para este ejemplo, se tomó la opción probability = False indicando que es un histograma de frecuencias. Una pregunta que surge de manera natural es ¿cuál es la diferencia entre los histogramas sin usar los factores de expansión y utilizándolo? A continuación, se generan 3 histogramas, en el primero se grafica la variable ingreso utilizando los factores de expansión, en el segundo se grafica la misma variable sin usar los factores de expansión y en el tercero, se hace el gráfico poblacional. library(survey) data(&quot;BigCity&quot;, package = &quot;TeachingSampling&quot;) par(mfrow = c(1,3)) svyhist(~ Income, diseno, main = &quot;Ponderado&quot;, col = &quot;green&quot;, breaks = 50) hist( encuesta$Income, main = &quot;Sin ponderar&quot;, col = &quot;red&quot;, prob = TRUE, breaks = 50) hist(BigCity$Income, main = &quot;Poblacional&quot;, col = &quot;purple&quot;, prob = TRUE, xlim = c(0, 2500), breaks = 500) Uno de los análisis gráficos más comunes que se realizan ene encuestas de hogares están relacionados con subgrupos geográficos como lo son las zonas (urbano - rural) o también realizar desagregaciones temáticas como lo son por sexo (hombre mujer). A continuación, se muestra la sintaxis en R como se realizan histogramas para hombres y mujeres mayores de 18 años: sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) par(mfrow = c(1,2)) svyhist( ~ Income , design = subset(sub_Mujer, Age &gt;= 18), main = &quot;Mujer&quot;, breaks = 30, col = &quot;grey80&quot;, xlab = &quot;Ingreso&quot;) svyhist( ~ Income , design = subset(sub_Hombre, Age &gt;= 18), main = &quot;Hombre&quot;, breaks = 30, col = &quot;grey80&quot;, xlab = &quot;Ingreso&quot;) Como se puede observar, los argumentos utilizando para realizar los gráficos son los mismo que se utilizaron y ejemplificaron anteriormente. Cabe notar que la función subset permite hacer un subconjunto de la población, que para nuetro caso son aquellos hombres y mujeres mayores o iguales a 18 años. Si el objetivo ahora es realizar análisis de localización y variablidad, por ejemplo, graficar Bloxplot teniendo en cuenta los factores de expansión, a continuación, se muestran las sintaxis de como realizarlo en R. sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) par(mfrow = c(1,2)) svyboxplot( Income~1 , sub_Urbano, col = &quot;grey80&quot;, ylab = &quot;Ingreso&quot;, xlab = &quot;Urbano&quot;) svyboxplot( Income ~ 1 , sub_Rural, col = &quot;grey80&quot;, ylab = &quot;Ingreso&quot;, xlab = &quot;Rural&quot;) Los argumentos usados en la función svyboxplot para generar el gráfico son muy similares a los usados en la función svyhist. Algo a recalcar el los argumentos de esta función es que el símbolo “Income ~ 1” hace referencia a que todas las personas pertenecen a un solo grupo que puede ser urbano o rural dependiendo del caso y por eso se requiere indicarle a R esa restricción, lo cual se hace con el símbolo “~1”. "],["estimación-de-totales-medias-y-razones.html", "4.3 Estimación de totales, medias y razones", " 4.3 Estimación de totales, medias y razones Al trabajar con encuestas de hogares, el análisis de datos numéricos implica con frecuencia calcular estadísticas descriptivas como medias, totales y razones, ya que estas permiten sintetizar las principales características de la población y sirven de base para la toma de decisiones. Dichas estimaciones pueden calcularse para la población en su conjunto o para subgrupos específicos, según los objetivos de la investigación. Tal como destacan Heeringa, West y Berglund (2017), el cálculo de totales y medias poblacionales, junto con sus varianzas, ha sido esencial para el desarrollo de la teoría del muestreo probabilístico y la interpretación adecuada de los resultados de encuestas de hogares. 4.3.1 Estimación puntual Una vez exploradas las tendencias de las variables continuas mediante análisis gráfico, el siguiente paso consiste en obtener las estimaciones puntuales de los parámetros medidos. Estas estimaciones pueden calcularse de forma general o desagregada por niveles de análisis, dependiendo de las necesidades de la investigación. En el contexto de encuestas de hogares, las estimaciones puntuales comprenden el cálculo de totales, promedios, razones y otras medidas agregadas. Heeringa et al. (2017) señalan que la estimación del total o promedio de una población y su varianza muestral es fundamental en la teoría del muestreo probabilístico, ya que permite obtener valores precisos sobre la situación de los hogares estudiados, facilitando la toma de decisiones informadas en políticas públicas. 4.3.2 Estimación de totales e intervalos de confianza Una vez definido el diseño muestral (como se hizo en la sección anterior), se procede a realizar los procesos de estimación de los parámetros de interés. Para efectos de este texto, se iniciará con la estimación del total de los ingresos de los hogares. En su mayoría, los paquetes estadísticos actuales no implementan técnicas avanzadas como estimadores generales de regresión (GREG) o métodos de calibración. Sin embargo, Valliant et al. (2000) desarrollaron una librería en S-plus que permite realizar estos procedimientos de estimación, los cuales también pueden implementarse en R (Valliant et al., 2013). Para la estimación de totales con diseños muestrales complejos que incluyen estratificación (\\(h=1,2,...,H\\)) y muestreo por conglomerados (cuyos conglomerados están dentro del estrato \\(h\\), indexados por \\(\\alpha=1,2,...,a\\_h\\)), el estimador del total se puede expresar como: \\[ \\hat{Y}_{\\omega} = \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_h}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i} y_{h\\alpha i} \\] El estimador insesgado de la varianza para este total es: \\[ \\text{var}\\left(\\hat{Y}_{\\omega}\\right) = \\sum_{h=1}^{H} \\frac{a_h}{a_h - 1} \\left[ \\sum_{\\alpha=1}^{a_h} \\left( \\sum_{i=1}^{n_{h\\alpha}} \\omega_{h\\alpha i} y_{h\\alpha i} \\right)^2 - \\frac{\\left( \\sum_{\\alpha=1}^{a_h} \\omega_{h\\alpha i} y_{h\\alpha i} \\right)^2}{a_h} \\right] \\] La determinación de los totales poblacionales constituye uno de los pilares del análisis de encuestas. Tanto las medias como las proporciones y las razones derivan de los totales. Un total se define como la suma de una variable específica (por ejemplo, ingreso o gasto) a nivel de toda la población. Para estimar el ingreso total de todos los hogares de un país, se combinan los datos de la muestra aplicando los pesos muestrales que reflejan el diseño y aseguran representatividad. En el caso de variables numéricas simples, las estimaciones básicas son los totales y medias, mientras que las razones permiten establecer comparaciones entre dos variables numéricas. Estos cálculos pueden realizarse para toda la población o de manera desagregada por dominios de estudio, dependiendo de las preguntas de investigación. Para encuestas con estratificación (\\(h=1,2,...,H\\)) y submuestreo en las UPM (ubicadas dentro de cada estrato \\(h\\), identificadas por \\(i\\)), el total poblacional se estima mediante: \\[ \\hat{Y} = \\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik} \\, y_{hik} \\] Cuando se cuenta con respuesta completa, la varianza de \\(\\hat{Y}\\) puede calcularse usando el estimador de Ultimate Cluster. El intervalo de confianza de nivel \\(1-\\alpha\\) para el total poblacional \\(Y\\) se calcula como: \\[ \\hat{Y} \\pm t_{1-\\alpha/2, df} \\times \\sqrt{\\hat{V}_{UC}(\\hat{Y})} \\] A medida que los grados de libertad aumentan, la distribución \\(t\\) de Student tiende a la normal, lo que explica por qué muchas Oficinas Nacionales de Estadística (ONE) utilizan esta aproximación para reportar intervalos de confianza. No obstante, es importante considerar que esta aproximación puede ser menos fiable cuando el tamaño de la muestra es reducido, aunque suele ofrecer buenos resultados en encuestas de hogares extensas. Como se puede observar, calcular la estimación del total y su varianza estimada es complejo. 4.3.2.1 Enfoques para la estimación de la varianza Tal como se mencionó anteriormente, al trabajar con encuestas de hogares es fundamental proporcionar no solo estimaciones puntuales, sino también cuantificar la incertidumbre asociada a dichas estimaciones. Comprender y estimar esta incertidumbre constituye una parte crítica del análisis de los datos provenientes de encuestas de hogares. Mediante la aplicación de métodos apropiados, los usuarios pueden medir la precisión de sus estimaciones. Existen diversos métodos para estimar dicha precisión y, con el apoyo de software moderno, estos enfoques pueden implementarse de manera eficiente para respaldar análisis rigurosos y significativos. Entre los principales métodos se encuentran: Ecuaciones de estimación: ofrecen un marco flexible para estimar totales, medias, razones y otros parámetros, así como sus varianzas correspondientes, integrando una idea unificadora de la teoría de muestreo (Binder, 1983). Linealización de Taylor: consiste en aproximar estadísticas no lineales complejas mediante expresiones lineales y posteriormente estimar la varianza de esta cantidad aproximada. Método del Clúster Último: utilizado con frecuencia en encuestas que emplean muestreo estratificado en múltiples etapas; se basa en calcular la varianza a partir de las diferencias entre las estimaciones obtenidas a nivel de las unidades primarias de muestreo (PSU). Suele combinarse con la Linealización de Taylor para estimar la varianza de estadísticas no lineales, como medias o razones. Bootstrap y otros métodos de replicación: se fundamentan en tomar repetidas submuestras del conjunto de datos observado, calcular estimaciones para cada réplica y luego utilizar la variabilidad entre estas estimaciones replicadas para inferir la varianza del estimador principal. Ecuaciones de estimación y linealización de Taylor Como se mencionó en el apartado anterior, uno de los enfoques más utilizados para cuantificar la incertidumbre en encuestas de hogares se basa en la formulación de ecuaciones de estimación y en la aplicación de la linealización de Taylor. Estos métodos proporcionan un marco general que permite definir los parámetros de interés, obtener sus estimadores muestrales y, a partir de ellos, aproximar sus varianzas. Muchos parámetros poblacionales pueden expresarse como soluciones de ecuaciones de estimación que involucran totales poblacionales. Aunque los detalles técnicos pueden ser complejos, la idea fundamental es que los mismos principios utilizados para estimar totales pueden aplicarse también para la estimación de varianzas. Este marco general hace que el método sea sencillo y versátil, permitiendo una implementación eficiente en software especializado. Una ecuación de estimación poblacional genérica se expresa como: \\[ \\sum_{k\\in U} z_k(\\theta)=0, \\] donde \\(z_k(\\cdot)\\) es una función de estimación evaluada para la unidad \\(k\\) y \\(\\theta\\) representa el parámetro poblacional de interés. Estas ecuaciones proporcionan un marco general para definir y calcular diversos parámetros de la población, como totales, medias y razones. Para el total poblacional: \\(z_k(\\theta)=y_k-\\theta/N\\). La ecuación de estimación es \\(\\sum_{k\\in U}(y_k-\\theta/N)=0\\), cuya solución es \\(\\theta=\\sum_{k\\in U} y_k = Y\\). Para la media poblacional: \\(z_k(\\theta)=y_k-\\theta\\). La ecuación es \\(\\sum_{k\\in U}(y_k-\\theta)=0\\), cuya solución es \\(\\theta=\\left(\\sum_{k\\in U} y_k\\right)/N = \\overline{Y}\\). Para razones de totales: \\(z_k(\\theta)=y_k-\\theta x_k\\). La ecuación \\(\\sum_{k\\in U}(y_k-\\theta x_k)=0\\) conduce a la razón poblacional \\(\\theta=\\dfrac{\\sum_{k\\in U} y_k}{\\sum_{k\\in U} x_k} = R\\). La idea de definir parámetros poblacionales como soluciones de ecuaciones de estimación a nivel de población conduce naturalmente a un método general para obtener los estimadores muestrales. En este caso, se utilizan ecuaciones de la forma: \\[ \\sum_{k\\in s} d_k\\, z_k(\\theta)=0, \\] donde \\(d_k\\) son los pesos de diseño y \\(z_k(\\theta)\\) la función de estimación evaluada para cada unidad de la muestra. Bajo un muestreo probabilístico y asumiendo respuesta completa, la suma muestral \\(\\sum_{k\\in s} d_k\\, z_k(\\theta)\\) es insesgada respecto a su análoga poblacional, lo que garantiza que las soluciones de estas ecuaciones sean estimadores consistentes de los parámetros poblacionales. La linealización de Taylor constituye un complemento natural a este marco, ya que permite aproximar la varianza de estimadores no lineales. El procedimiento consiste en aplicar una expansión de Taylor de primer orden alrededor del parámetro estimado, con el fin de reemplazar el estimador no lineal por una expresión lineal. De esta manera, se facilita el cálculo de varianzas en situaciones donde no existen fórmulas exactas o su derivación resulta demasiado compleja. Un estimador consistente de la varianza, derivado mediante linealización de Taylor para soluciones de ecuaciones de estimación muestrales, puede expresarse como: \\[ \\hat{V}_{TL}(\\hat{\\theta}) = [\\hat{J}(\\hat{\\theta})]^{-1} \\, \\hat{V}_p \\Bigg[\\sum_{k\\in s} d_k\\, z_k(\\hat{\\theta})\\Bigg] \\, [\\hat{J}(\\hat{\\theta})]^{-1} \\] donde \\(\\hat{J}(\\hat{\\theta}) = \\sum_{k\\in s} d_k \\left[ \\frac{\\partial z_k(\\theta)}{\\partial \\theta} \\right]_{\\theta=\\hat{\\theta}}\\). Este resultado muestra cómo la linealización de Taylor convierte la estimación de varianzas de parámetros complejos en un problema de estimación de totales, lo que explica su amplia adopción en software especializado para el análisis de encuestas. Ultimate Cluster El método del Ultimate Cluster constituye un enfoque directo y robusto para estimar la varianza de totales en encuestas que emplean diseños de muestreo por conglomerados estratificados en múltiples etapas. Propuesto por Hansen, Hurwitz y Madow (1953), este método simplifica la complejidad de los diseños multinivel al centrarse únicamente en la variación entre las Unidades Primarias de Muestreo (PSU). Se asume que, dentro de cada estrato de muestreo, las PSU fueron seleccionadas de manera independiente con reemplazo (posiblemente con probabilidades desiguales), aunque en la práctica la selección suele realizarse sin reemplazo. El método se basa en la variación entre las estadísticas calculadas a nivel de PSU. Cuando se aplica correctamente, refleja implícitamente cualquier submuestreo realizado dentro de las PSU, permitiendo estimaciones de varianza más simples pero confiables. Es especialmente útil en diseños complejos que incluyen estratificación y probabilidades desiguales de selección tanto de PSU como de unidades de niveles inferiores (hogares e individuos). Los requisitos para aplicar este método son: Disponibilidad de estimaciones insesgadas de totales para las variables de interés en cada PSU muestreada. Al menos dos PSU muestreadas por estrato, si la muestra se estratifica en la primera etapa. Información completa sobre PSU, estratos y pesos en el conjunto de datos de la encuesta. Considere un diseño de muestreo en múltiples etapas donde se seleccionan \\(n_h\\) PSU en el estrato \\(h\\), \\(h=1,\\dots,H\\). Sea \\[ \\hat{Y}_{hi} = \\sum_{k\\in s_{hi}} d_{hik} y_{hik} \\] una estimación del total poblacional \\(Y_{hi}\\) de la PSU \\(i\\) en el estrato \\(h\\). Un estimador insesgado del total poblacional \\(Y = \\sum_{h=1}^H \\sum_{i \\in U_{1h}} Y_{hi}\\) se expresa como \\[ \\hat{Y}_{UC} = \\sum_{h=1}^H \\hat{Y}_h, \\quad \\text{donde} \\quad \\hat{Y}_h = \\frac{1}{n_h} \\sum_{i\\in s_{1h}} \\hat{Y}_{hi}. \\] El estimador Ultimate Cluster de la varianza correspondiente se calcula mediante: \\[ \\hat{V}_{UC}(\\hat{Y}) = \\sum_{h=1}^H \\frac{n_h}{n_h-1} \\sum_{i \\in s_{1h}} (\\hat{Y}_{hi} - \\hat{Y}_h)^2 \\] Para más detalles, véase Hansen, Hurwitz y Madow (1953, vol. I, p. 257) o Wolter (2007). Aunque originalmente se diseñó para calcular varianzas de estimadores de totales, el método puede combinarse con linealización de Taylor o ecuaciones de estimación para derivar varianzas de otros parámetros poblacionales formulables como soluciones de ecuaciones de estimación. Esta flexibilidad hace que el método sea aplicable a diversos contextos de análisis de encuestas de hogares. Un supuesto clave es que, dentro de cada estrato, las PSU se eligen de forma independiente y con reemplazo. En la práctica, la mayoría de las encuestas selecciona PSU sin reemplazo, generando diseños más eficientes. Así, las varianzas calculadas bajo la hipótesis de independencia constituyen aproximaciones de las verdaderas varianzas de muestreo. Cuando la fracción muestral es pequeña (por ejemplo, &lt;5 %), estas aproximaciones suelen ser suficientemente precisas para su uso por oficinas nacionales de estadística o analistas secundarios. El método Ultimate Cluster destaca por su simplicidad y robustez, lo que lo hace muy atractivo en la práctica. Aunque los métodos más sofisticados que consideran todas las etapas del diseño pueden ofrecer estimaciones de varianza ligeramente más precisas, su aplicación requiere información más detallada y mayor complejidad computacional. Por el contrario, el método Ultimate Cluster proporciona una aproximación confiable y eficiente, especialmente útil al estimar totales o medias en encuestas de hogares. Para un análisis detallado sobre la precisión de esta aproximación y posibles alternativas, véase Särndal, Swensson y Wretman (1992, p. 153). Bootstrap En muchos casos, los microdatos de encuestas públicas omiten información esencial del diseño, como identificadores de estratos o de unidades primarias de muestreo (UPM), para proteger la confidencialidad de los encuestados. Esta omisión limita la capacidad de los usuarios para calcular varianzas válidas. En tales situaciones, se recomienda que las oficinas nacionales de estadística (NSO) proporcionen pesos de replicación, lo que permite a los analistas estimar errores estándar de manera correcta. Sin estos datos, los usuarios secundarios no pueden reproducir los errores estándar publicados ni considerar adecuadamente el diseño complejo de la encuesta. Los métodos de replicación estiman la varianza generando subconjuntos de la muestra original, calculando estimaciones para cada uno y utilizando la variabilidad observada entre estas estimaciones para aproximar la varianza del estimador principal. Son particularmente útiles cuando no se dispone de información sobre estratos o UPM, situación en la que no se puede aplicar el método de Ultimate Cluster. El Bootstrap es una herramienta de replicación robusta y versátil. Originalmente introducido por Efron (1979) para datos que no provenían de encuestas, su adaptación más usada para encuestas de hogares es el Bootstrap de Reescalamiento Rao-Wu-Yue (Rao, Wu y Yue, 1992). Este método se ajusta de manera óptima a diseños de muestreo estratificados y multietápicos, y es ampliamente empleado para la estimación de varianzas en encuestas complejas. El procedimiento consiste en generar muchas réplicas de la muestra original, simulando extracciones repetidas de la población. Cada réplica se construye mediante la creación de columnas adicionales de pesos de replicación en la base de datos, siguiendo este proceso: Para cada estrato, se seleccionan aleatoriamente las UPM con reemplazo; algunas pueden repetirse y otras no aparecer. Cada UPM elegida se incorpora con todas sus observaciones. Si el tamaño de la muestra de primera etapa en el estrato \\(h\\) es mayor que dos (\\(n_h &gt; 2\\)), el número de UPM seleccionadas por réplica es \\(n_h - 1\\). Este proceso se repite muchas veces, habitualmente cientos, generando un gran número de réplicas. La cantidad de veces que una UPM \\(i\\) del estrato \\(h\\) aparece en la réplica \\(r\\) se denota \\(n_{hi}^{(r)}\\), variando entre 0 y \\(n_h - 1\\). A partir de cada réplica se calculan nuevos pesos bootstrap para todas las unidades, reflejando cuántas veces fue seleccionada su UPM. El peso de la unidad \\(k\\) en la réplica \\(r\\) se calcula como: \\[ w_{hik}^{(r)} = w_{hik} \\times \\frac{n_h}{n_h - 1} \\times n_{hi}^{(r)} \\] Si los pesos originales incluyen ajustes por no respuesta o calibración, estos deben aplicarse también a cada conjunto de pesos bootstrap. Cuando la NSO proporciona únicamente pesos de replicación Bootstrap, los analistas pueden estimar errores estándar correctamente, aun sin identificadores de estratos o UPM. Para cada réplica \\(r\\), se calcula el parámetro de interés \\(\\hat{\\theta}^{(r)}\\) usando los pesos bootstrap \\(w_{hik}^{(r)}\\). La varianza del estimador original se aproxima mediante la variabilidad entre todas las réplicas: \\[ \\hat{V}_B(\\hat{\\theta}) = \\frac{1}{R} \\sum_{r=1}^{R} \\left(\\hat{\\theta}^{(r)} - \\tilde{\\theta}\\right)^2, \\quad \\tilde{\\theta} = \\frac{1}{R} \\sum_{r=1}^{R} \\hat{\\theta}^{(r)} \\] Este enfoque asegura que la dispersión entre réplicas capture fielmente la incertidumbre del parámetro. El Bootstrap ofrece múltiples ventajas. A pesar de requerir un mayor procesamiento computacional, es eficaz para diseños de encuesta complejos y permite estimar parámetros difíciles de calcular con métodos tradicionales, como medianas u otras estadísticas no lineales. Es especialmente útil para analistas que trabajan con bases de datos sin identificadores de estratos y UPM, pero con pesos de replicación. La simplicidad del método facilita su aplicación incluso sin software estadístico especializado. Sin embargo, la mayoría de los paquetes estadísticos modernos ya incluyen procedimientos para aplicar Bootstrap y calcular varianzas, ampliando su disponibilidad y robustez. No obstante, su uso no es recomendable en encuestas repetidas con muestras superpuestas ni en situaciones con fracciones de muestreo grandes y tamaños de muestra pequeños (Bruch, 2011). En este ejemplo se muestra cómo estimar totales y sus intervalos de confianza para diferentes variables de interés en R, utilizando dos funciones de la librería survey: la función svytotal para calcular los totales y la función confint para obtener los intervalos de confianza. A continuación, se presentan los códigos correspondientes: total_Ingresos&lt;- svytotal(~Income, diseno, deff=T, ) total_Ingresos ## total SE DEff ## Income 85793667 4778675 11 confint(total_Ingresos, level = 0.95) ## 2.5 % 97.5 % ## Income 76427637 95159697 Los argumentos que utiliza de la función svytotal con muy sencillos. Para el ejemplo, se le introduce primero la variable en la cual está la información que se desea estimar (Income). Posterior a esto, se introduce el diseño muestral del cual proviene la muestra y, por último, se indica si desea que se reporte el deff de la estimación o no. Por otro lado, para el cálculo del intervalo de confianza, lo único que requiere es indicarle a la función confint el estimador y la confianza requerida. Paras seguir ilustrando el uso de la función svytotal y de confint, estimemos el total de gastos de los hogares, pero ahora el intervalo de confianza se calculará al 90% de confianza. Los siguientes códigos realizan las estimaciones: total_gastos&lt;- svytotal (~Expenditure, diseno, deff=T) total_gastos ## total SE DEff ## Expenditure 55677504 2604139 10.222 confint(total_gastos, level = 0.9) ## 5 % 95 % ## Expenditure 51394077 59960931 Si el objetivo ahora es estimar el total de los ingreso de los hogares pero discriminado por sexo, se utilizará ahora la función cascadede la libraría srvyr, la cual permite agregar la suma de las categorías al final la tabla. También se utilizará la función group_by la cual permite obtener resultados agrupados por los niveles de interés. diseno %&gt;% group_by(Sex) %&gt;% cascade(Total = survey_total( Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;Total ingreso&quot;) ## # A tibble: 3 × 5 ## Sex Total Total_se Total_low Total_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 44153820. 2324452. 39551172. 48756467. ## 2 Male 41639847. 2870194. 35956576. 47323118. ## 3 Total ingreso 85793667. 4778674. 76331414. 95255920. Como se pudo observar en lo códigos anteriores, otra forma de obtener las estimaciones del total, su desviación estándar y el intervalo de confianza es usando el argumento vartype e indicándole las opciones “se”, “ci” respectivamente. 4.3.3 Estimación de la media e intervalo de confianza La estimación de la media poblacional es un parámetro muy importante en las encuestas de hogares, dado que, por ejemplo, uno de los indicadores trazadores en este tipo de encuestas son los ingresos medios por hogar. Además, este tipo de parámetros no permiten describir y analizar las tendencias centrales de estas variables en poblaciones de interés. Según Gutiérrez (2016) un estimador de la media poblacional se puede escribir como una razón no lineal de dos totales de población finitas estimados como sigue: \\[\\begin{eqnarray*} \\bar{Y}_{\\omega} &amp; = &amp; \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}y_{h\\alpha i}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}}\\\\ &amp; = &amp; \\frac{\\hat{Y}}{\\hat{N}}. \\end{eqnarray*}\\] Como una observación tenga en cuenta que, si \\(y\\) es una variable binaria, la media ponderada estima la proporción de la población. Por otro lado, como \\(\\bar{Y}_{\\omega}\\) no es una estadística lineal, no existe una fórmula cerrada para la varianza de este estimador. Es por lo anterior que, se deben recurrir a usar métodos de remuestreo o series de Taylor. Para este caso en particular, usando series de Taylor el estimador insesgado de la varianza para este estimador es: \\[\\begin{eqnarray*} var\\left(\\bar{Y}_{\\omega}\\right) &amp; \\dot{=} &amp; \\frac{var\\left(\\hat{Y}\\right)+\\bar{Y}_{\\omega}^{2}\\times var\\left(\\hat{N}\\right)-2\\times\\bar{Y}_{\\omega}\\times cov\\left(\\hat{Y},\\hat{N}\\right)}{\\hat{N}^{2}} \\end{eqnarray*}\\] Como se puede observar, el cálculo de la estimación de la varianza tiene componentes complejos de calcular de manera analítica, como la covarianza entre el total estimado y el tamaño poblacional estimado. Sin embargo, R tiene funciones que incorpora estos estimadores. A continuación, se presenta la sintaxis para hacer dichos cálculos. Media_ingresos&lt;- svymean(~Income, diseno, deff=T) Media_ingresos ## mean SE DEff ## Income 570.945 28.478 8.8211 confint(Media_ingresos, level = 0.95) ## 2.5 % 97.5 % ## Income 515.1299 626.7607 Como se puede observar, los argumentos que utiliza la función svymean para realizar la estimación de la media de los ingresos de los hogares y la desviación estándar estimada del estimador son similares a los utilizando con la función svytotal. Similarmente ocurre con el intervalo de confianza. Por otro lado, tal como se realizó con el total, a manera de ejemplo, se estima la media de los gastos en los hogares como sigue a continuación: Media_gastos&lt;- svymean (~Expenditure, diseno, deff=T) Media_gastos ## mean SE DEff ## Expenditure 370.526 13.294 6.0156 confint(Media_gastos) ## 2.5 % 97.5 % ## Expenditure 344.4697 396.5829 También se pueden realizar estimaciones de la media por subgrupos siguiendo el mismo esquema mostrado para la función svytotal. Particularmente, los gastos de los hogares discriminados por sexo es: diseno %&gt;% group_by(Sex) %&gt;% cascade( Media = survey_mean( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;El gasto medio&quot; ) %&gt;% arrange(desc(Sex)) ## # A tibble: 3 × 5 ## Sex Media Media_se Media_low Media_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Male 374. 16.1 343. 406. ## 2 Female 367. 12.3 343. 391. ## 3 El gasto medio 371. 13.3 344. 397. Por zona, diseno %&gt;% group_by(Zone) %&gt;% cascade( Media = survey_mean( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;El gasto medio&quot;)%&gt;% arrange(desc(Zone)) ## # A tibble: 3 × 5 ## Zone Media Media_se Media_low Media_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Urban 460. 22.2 416. 504. ## 2 Rural 274. 10.3 254. 294. ## 3 El gasto medio 371. 13.3 344. 397. Por sexo y zona, diseno %&gt;% group_by(Zone, Sex) %&gt;% cascade( Media = survey_mean( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;El gasto medio&quot;) %&gt;% arrange(desc(Zone), desc(Sex)) %&gt;% data.frame() ## Zone Sex Media Media_se Media_low Media_upp ## 1 Urban Male 469.8124 26.96068 416.4276 523.1973 ## 2 Urban Female 450.8151 20.11853 410.9784 490.6518 ## 3 Urban El gasto medio 459.6162 22.20655 415.6450 503.5874 ## 4 Rural Male 275.3018 10.24848 255.0088 295.5948 ## 5 Rural Female 272.6769 11.61470 249.6786 295.6751 ## 6 Rural El gasto medio 273.9461 10.26141 253.6275 294.2647 ## 7 El gasto medio El gasto medio 370.5263 13.29444 344.2020 396.8506 Las medias o promedios poblacionales son esenciales para describir la tendencia central de una variable. Por ejemplo, el gasto promedio de los hogares es un indicador representativo del comportamiento económico de la población. Su cálculo consiste en dividir el total estimado de la variable entre el tamaño poblacional estimado, de modo que su precisión depende de la exactitud en ambos componentes. El estimador de la media poblacional se puede expresar como: \\[ \\widehat{\\bar{Y}} = \\frac{\\displaystyle \\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik} y_{hik}} {\\displaystyle \\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik}} = \\frac{\\hat{Y}}{\\hat{N}} \\] Dado que \\(\\widehat{\\bar{Y}}\\) es un estimador no lineal, su varianza exacta no puede expresarse en forma cerrada. Por ello, los paquetes estadísticos especializados en encuestas complejas emplean métodos como el remuestreo o la aproximación de Taylor, que se implementan automáticamente para facilitar su cálculo. 4.3.4 Estimación de medidas de dispersión y localización En las encuestas de hogares siempre es necesario estimar medidas de dispersión de las variables estudiadas. Esto con el fin de, por ejemplo, ver qué tan disímiles son los ingresos medios de los hogares en un país determinado y con esto poder tomar acciones de política pública. Por lo anterior, es importante estudiar este parámetro en este texto. A continuación, se presenta el estimador de la desviación estándar: \\[\\begin{eqnarray} s\\left(y\\right){}_{\\omega} &amp; = &amp; \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-\\bar{Y}_{\\omega}\\right)^{2}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}-1} \\end{eqnarray}\\] Para llevar a cabo la estimación en R de la desviación estándar en encuestas de hogares, se utilizan la función survey_var la cual se ejemplifica a continuación: (sd_Est &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise(Sd = sqrt( survey_var( Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), ) ))) ## # A tibble: 2 × 5 ## Zone Sd Sd_se Sd_low Sd_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 310. 117. 263. 352. ## 2 Urban 582. 285. 422. 707. Como se pudo ver en el ejemplo anterior, se estimó la desviación estándar de los ingresos por zona reportando el error estándar en la estimación y un intervalo de confianza al 95%. Los argumentos que utiliza la función survey_var son similares a los usados en las funciones anteriores para estimar medias y totales. Si el interés ahora se centra en estimar la desviación estándar clasificando por sexo y zona, los códigos computacionales son los siguientes: (sd_Est &lt;- diseno %&gt;% group_by(Zone, Sex) %&gt;% summarise(Sd = sqrt( survey_var( Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), ) ))) %&gt;% data.frame() ## Zone Sex Sd Sd_se Sd_low Sd_upp ## 1 Rural Female 294.8683 111.6203 249.5537 334.0921 ## 2 Rural Male 325.7584 124.9643 274.2209 370.1890 ## 3 Urban Female 568.3920 286.4585 400.7312 696.8166 ## 4 Urban Male 596.7756 288.9435 436.8362 722.1194 Las medidas de posición no central (Percentiles) se diseñaron con el fin de conocer otros puntos característicos de la distribución de los datos que no son los valores centrales. Entre las medidas de posición no central más importantes están la mediana, cuartiles y percentiles. En la mayoría de las encuestas de hogares no solo estiman totales, medias y proporciones. En algunos indicadores es necesario estimar otros parámetros, por ejemplo, medianas y percentiles. Como lo menciona Tellez et al (2015) la mediana una medida de tendencia central la cual, a diferencia del promedio, no es fácilmente influenciada por datos atípicos y, por esto, se conoce como una medida robusta. La mediana es el valor que divide la población en dos partes iguales. Lo que implica que, la mitad de las observaciones de la característica de interés está por encima de la media y la otra mitad está por debajo. Por otro lado, la estimación de percentiles de ingresos en un país determinado puede definir el inicio de una política pública. por ejemplo, poner a tributar aquellas personas naturales que son el 10% más alto de la distribución de los ingresos o por el contrario, generar subsidios de transporte a aquellas familias que están en el 15% inferior de la distribución de los ingresos. La estimación de cuantiles (Loomis et al., 2005) se basa en los resultados relacionados con el estimador ponderado para totales, empleando una estimación de la función de distribución (CDF, por sus siglas en inglés) acumulada de la población. Específicamente, la CDF para una variable y en una población finita dada de tamaño \\(N\\) se define de la siguiente manera: \\[\\begin{eqnarray*} F\\left(x\\right) &amp; = &amp; \\frac{{ \\sum_{i=1}^{N}}I\\left(y_{i}\\leq x\\right)}{N} \\end{eqnarray*}\\] Donde, \\(I\\left(y_{i}\\leq x\\right)\\) es una variable indicadora la cual es igual a 1 si \\(y_{i}\\) es menor o igual a un valor específico \\(x\\), 0 en otro caso. Un estimador de la CDF en un diseño complejo (encuesta de hogares) de tamaño \\(n\\) está dado por: \\[\\begin{eqnarray*} \\hat{F}\\left(x\\right) &amp; = &amp; \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}I\\left(y_{i}\\leq x\\right)}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}} \\end{eqnarray*}\\] Una vez estimada la CDF utilizando los pesos del diseño muestral, el cuantil q-ésimo de una variable \\(y\\) es el valor más pequeño de \\(y\\) tal que la CDF de la población es mayor o igual que \\(q\\). Como es bien sabido, la mediana es aquel valor donde la CDF es mayor o igual a 0.5 y, por tanto, la media estimada es aquel valor donde la estimación de CDF es mayor o igual a 0.5. Siguiendo las recomendaciones de Heeringa et al (2017) para estimar cuantiles, primero se considera las estadísticas de orden que se denotan como \\(y_{1},\\ldots,y_{n}\\), y encuentra el valor de \\(j\\) \\((j=1,\\ldots,n)\\) tal que: \\[\\begin{eqnarray*} &amp; \\hat{F}\\left(y_{j}\\right)\\leq q\\leq\\hat{F}\\left(y_{j+1}\\right) \\end{eqnarray*}\\] Ahora bien, la estimación del q-ésimo cuantil \\(Y_{q}\\) en un diseño de muestreo complejo está dado por: \\[\\begin{eqnarray*} \\hat{Y}_{q} &amp; = &amp; y_{j}+\\frac{q-\\hat{F}\\left(y_{j}\\right)}{\\hat{F}\\left(y_{j+1}\\right)-\\hat{F}\\left(y_{j}\\right)}\\left(y_{j+1}-y_{j}\\right) \\end{eqnarray*}\\] Para la estimación de la varianza e intervalos de confianza de cuantiles, Kovar et al. (1988) muestra los resultados de un estudio de simulación en donde recomienda el uso de Balanced Repeated Replication (BRR) para estimarla. Los estimadores y procedimientos antes mencionados para la estimación de percentiles y sus varianzas están implementados en R. Particularmente, la estimación de la mediana se realiza usando la función survey_median. A continuación, se muestra la sintaxis de cómo calcular la mediana de los gastos, la desviación estándar y el intervalo de confianza al 95% de los hogares en la base de datos de ejemplo. diseno %&gt;% summarise(Mediana = survey_median( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), )) ## # A tibble: 1 × 4 ## Mediana Mediana_se Mediana_low Mediana_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 298. 8.83 282. 317. Como se puede observar, los argumentos de la función survey_median son similares a los del total y la media. Ahora bien, al igual que con los demás parámetros, si el objetivo ahora es estimar la mediana de los gastos de los hogares, pero esta vez discriminada por zona y también por sexo, el código computacional sería el siguiente: diseno %&gt;% group_by(Zone) %&gt;% summarise(Mediana = survey_median( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), )) ## # A tibble: 2 × 5 ## Zone Mediana Mediana_se Mediana_low Mediana_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 241. 11.0 214. 258. ## 2 Urban 381. 19.8 337. 416. diseno %&gt;% group_by(Sex) %&gt;% summarise(Mediana = survey_median( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), )) ## # A tibble: 2 × 5 ## Sex Mediana Mediana_se Mediana_low Mediana_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 300. 10.5 282. 324. ## 2 Male 297. 9.29 277. 314. Si el objetivo ahora es estimar cuantiles, por ejemplo, el cuantil 0.25 de los gastos de los hogares, se realizaría usando la función survey_quantile como sigue: diseno %&gt;% summarise( Q = survey_quantile( Expenditure, quantiles = 0.5, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), interval_type = &quot;score&quot; )) ## # A tibble: 1 × 4 ## Q_q50 Q_q50_se Q_q50_low Q_q50_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 298. 12.0 265. 312. si ahora se desea estimar el cuantil 0.25 pero discriminando por sexo y por zona se realizaría como sigue: diseno %&gt;% group_by(Sex) %&gt;% summarise( Q = survey_quantile( Expenditure, quantiles = 0.25, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), interval_type = &quot;score&quot; )) ## # A tibble: 2 × 5 ## Sex Q_q25 Q_q25_se Q_q25_low Q_q25_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 210. 14.9 169. 228. ## 2 Male 193. 10.4 163. 205. diseno %&gt;% group_by(Zone) %&gt;% summarise( Q = survey_quantile( Expenditure, quantiles = 0.25, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), interval_type = &quot;score&quot; )) ## # A tibble: 2 × 5 ## Zone Q_q25 Q_q25_se Q_q25_low Q_q25_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 160. 4.64 145. 163. ## 2 Urban 258. 9.05 256. 292. "],["estimación-del-coeficiente-de-ginni-en-encuestas-de-hogares.html", "4.4 Estimación del coeficiente de Ginni en encuestas de hogares", " 4.4 Estimación del coeficiente de Ginni en encuestas de hogares Para iniciar esta sección tengamos en cuenta la siguiente reflexión: Definir lo justo siempre será difícil y es algo a lo que quizá sea poco realista aspirar a conseguir. Sin embargo si estamos un poco más conscientes de cómo la desigualdad afecta nuestra libertad y cómo se refleja en el bienestar y calidad de vida de las personas, podremos poner en contexto una discusión que tendremos cada vez más presente en el mundo y en el país. La desigualdad en todos los aspectos es un problema más comunes en todos los países del mundo. Particularmente, la desigualdad económica es un problema que atañe a muchas instituciones internacionales como, por ejemplo, Naciones Unidas quien tiene este problema detectado en los Objetivos de Desarrollo Sostenibles (ODS, por sus siglas). Dado lo anterior, es clave poder medir la desigualdad económica de los hogares en los países y para esto, el indicador más utilizado es el coeficiente de Gini (CG). El valor del índice de Gini se encuentra entre 0 y 1. Un valor del coeficiente de Gini de \\(G = 0\\) indica perfecta igualdad en la distribución de la riqueza, con valores más grandes significa una desigualdad cada vez mayor en la distribución de la riqueza. Siguiendo la ecuación de estimación de Binder y Kovacevic (1995), un estimador del coeficiente de Gini es: \\[\\begin{eqnarray*} \\hat{G}\\left(y\\right) &amp; = &amp; \\frac{2\\times\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}^{*}\\hat{F}_{h\\alpha i}y_{h\\alpha i}-1}{\\bar{y}_{\\omega}} \\end{eqnarray*}\\] Donde, \\(\\omega_{h\\alpha i}^{*}=\\frac{\\omega_{h\\alpha i}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}}\\). \\(\\hat{F}_{h\\alpha i}=\\) La estimación de la CDF en el conglomerado \\(\\alpha\\) en el estrato \\(h\\). \\(\\bar{y}_{\\omega}=\\) La estimación del promedio. Para calcular el índice de Gini y su varianza estimada en una encuesta de hogares, R tiene cargados los procedimientos en la librería convey. A continuación, se muestra la sintaxis de cómo se realiza la estimación del índice de Gini para los hogares en la base de ejemplo de este capítulo. library(convey) diseno_gini &lt;- convey_prep(diseno) svygini( ~Income, design = diseno_gini) %&gt;% data.frame() ## gini Income ## Income 0.4132757 0.0186633 En primer lugar, se carga el diseño de muestreo con la función convey_prep. Luego, se estima el índice Gini con la función svygini. En los argumentos de esta última función se introducen la variable ingresos y el diseño muestral complejo. Por otro lado, si el interés ahora es estimar la curva de Lorenz. La cual, según Kovacevic, M. S. et. al (1997) para una distribución dada de ingresos, traza el porcentaje acumulado de la población (desplegado desde el más pobre hasta el más rico) frente a su participación en el ingreso total. El área entre la curva de Lorenz y la línea de 45 grados se conoce como el área de Lorenz. El índice de Gini es igual al doble del área de Lorenz. Una población con la curva de Lorenz más cerca de la línea de 45 grados tiene una distribución de ingresos más equitativa. Si todos los ingresos son iguales, la curva de Lorenz degenera a la línea de 45 grados. Para realizar la curva de Lorenz en R se utiliza la función svylorenz. A continuación, se muestran los códigos computacionales para realizar la curva de Lorenz para los ingresos: library(convey) svylorenz(formula = ~Income, design = diseno_gini, quantiles = seq(0,1,.05), alpha = .01 ) ## lorenz SE ## L(0) 0.0000000 0.0000 ## L(0.05) 0.0068191 0.0008 ## L(0.1) 0.0175964 0.0013 ## L(0.15) 0.0316596 0.0033 ## L(0.2) 0.0492230 0.0041 ## L(0.25) 0.0694365 0.0056 ## L(0.3) 0.0925871 0.0064 ## L(0.35) 0.1181331 0.0071 ## L(0.4) 0.1469261 0.0082 ## L(0.45) 0.1791978 0.0095 ## L(0.5) 0.2158231 0.0106 ## L(0.55) 0.2565784 0.0123 ## L(0.6) 0.3027002 0.0137 ## L(0.65) 0.3537989 0.0149 ## L(0.7) 0.4096304 0.0159 ## L(0.75) 0.4706565 0.0167 ## L(0.8) 0.5398749 0.0177 ## L(0.85) 0.6174169 0.0183 ## L(0.9) 0.7042464 0.0176 ## L(0.95) 0.8151774 0.0152 ## L(1) 1.0000000 0.0000 Los argumentos que requiere la función son, inicialmente, los ingresos de los hogares y el diseño muestral complejo. Adicionalmente, se definen una secuencia de probabilidades que define la suma de los cuantiles a calcular (quantiles) y por último, un número que especifica el nivel de confianza para el gráfico (alpha). "],["análisis-de-la-relación-entre-dos-variable-continuas.html", "4.5 Análisis de la relación entre dos variable continuas", " 4.5 Análisis de la relación entre dos variable continuas En muchos análisis de variables relacionadas con encuestas de hogares no solo basta con analizar el comportamiento de variables de manera individual, por ejemplo, ingresos medios de hombres y mujeres en un país sino también, analizar la diferencia entre los ingresos de los hombres y las mujeres. Esto último con el fin de ir cerrando la brecha salarial que existe. En este capítulo se estudiará la prueba de hipótesis para diferencia de medias, se darán las herramientas computacionales para estimar razones y contrastes. "],["prueba-de-hipótesis-para-la-diferencia-de-medias-en-encuestas-de-hogares.html", "4.6 Prueba de hipótesis para la diferencia de medias en encuestas de hogares", " 4.6 Prueba de hipótesis para la diferencia de medias en encuestas de hogares Es llamado prueba de hipótesis a una técnica la cual consiste en hacer una afirmación acerca del valor que el parámetro de la población bajo estudio puede tomar. Esta afirmación puede estar basada en alguna creencia o experiencia pasada que será contrastada con la evidencia que se obtengan a través de la información contenida en la muestra. Como dicha afirmación puede ser o no cierta, dos hipótesis pueden ser planteadas (antagónicas) las cuales se conocen como \\(H_{0}:\\) Hipótesis nula y \\(H_{1}:\\) Hipótesis alterna. Si se sospecha que el parámetro \\(\\theta\\) es igual a cierto valor particular \\(\\theta_{0}\\), los posibles juegos de hipótesis a contrastar son: \\[ \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta\\neq\\theta_{0} \\end{cases}\\,\\,\\, \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta&gt;\\theta_{0} \\end{cases}\\,\\,\\, \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta&lt;\\theta_{0} \\end{cases} \\] Se dirá que una de las dos hipótesis es cierta solo si la evidencia estadística, la cual es obtenida de la muestra, la apoya. El proceso por medio del cual se escoge una de las dos hipótesis es llamado Prueba de Hipótesis. En términos generales, algunos parámetros importantes en la estadística descriptivas se pueden escribir como una combinación lineal de medidas de interés. Los casos más usuales son diferencias de medias, sumas ponderadas de medias utilizadas para construir índices económicos, etc. Considere una función que es una combinación lineal de \\(j\\) estadísticas descriptivas como se muestra a continuación: \\[\\begin{eqnarray*} f\\left(\\theta_{1},\\theta_{2},...,\\theta_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\theta_{j} \\end{eqnarray*}\\] Una estimación de esta función está dada por: \\[\\begin{eqnarray*} f\\left(\\hat{\\theta}_{1},\\hat{\\theta}_{2},...,\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j} \\end{eqnarray*}\\] cuya varianza del estimador se calcula como sigue: \\[\\begin{eqnarray*} var\\left(\\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}^{2}var\\left(\\hat{\\theta}_{j}\\right)+2\\times\\sum_{j=1}^{J-1}\\sum_{k&gt;j}^{J}a_{j}a_{k}\\,cov\\left(\\hat{\\theta}_{j},\\hat{\\theta}_{k}\\right) \\end{eqnarray*}\\] Como se pudo observar en la ecuación de la varianza del estimador, esta incorpora las varianzas de las estimaciones de los componentes individuales, así como las covarianzas de las estadísticas estimadas. En primer lugar, una combinación lineal de estadísticas descriptivas de interés en este capítulo es la diferencia de media cuyo parámetro es \\({\\bar{Y}_{1}-\\bar{Y}_{2}}\\), donde, \\(\\bar{Y}_{1}\\) es la media de la población 1, por ejemplo, ingresos medios en los hogares obtenido por los padres de familia y \\(\\bar{Y}_{2}\\) es la media de la población 2, que para seguir el ejemplo serían, los ingresos medios de las madres en un hogar. Considerando el parámetro de interés en esta sección, las hipótesis a estudiar serían las siguientes: \\[\\begin{eqnarray*} \\begin{cases} H_{0}:\\bar{Y}_{1}-\\bar{Y}_{2}=0\\\\ H_{1}:\\bar{Y}_{1}-\\bar{Y}_{2}\\neq0 \\end{cases} &amp; \\begin{cases} H_{0}:\\bar{Y}_{1}-\\bar{Y}_{2}=0\\\\ H_{1}:\\bar{Y}_{1}-\\bar{Y}_{2}&gt;0 \\end{cases} &amp; \\begin{cases} H_{0}:\\bar{Y}_{1}-\\bar{Y}_{2}=0\\\\ H_{1}:\\bar{Y}_{1}-\\bar{Y}_{2}&lt;0 \\end{cases} \\end{eqnarray*}\\] Para probar estas hipótesis se utiliza el siguiente estadístico de prueba que se distribuye t-student: \\[\\begin{eqnarray*} t &amp; = &amp; \\frac{\\bar{Y}_{1}-\\bar{Y}_{2}}{se\\left(\\bar{Y}_{1}-\\bar{Y}_{2}\\right)}, \\end{eqnarray*}\\] donde, \\[\\begin{eqnarray*} se\\left(\\bar{Y}_{1}-\\bar{Y}_{2}\\right) &amp; = &amp; \\sqrt{var\\left(\\bar{y}_{1}\\right)+var\\left(\\bar{y}_{2}\\right)-2cov\\left(\\bar{y}_{1},\\bar{y}_{2}\\right)} \\end{eqnarray*}\\] Si se desea construir un intervalo de confianza para la diferencia de media se realizaría de la siguiente manera: \\[\\begin{eqnarray*} &amp; \\left(\\bar{Y}_{1}-\\bar{Y}_{2}\\right)\\pm t_{gl,\\,\\alpha/2}\\,se\\left(\\bar{Y}_{1}-\\bar{Y}_{2}\\right) \\end{eqnarray*}\\] Para poder llevar a cabo la prueba de hipótesis para la diferencia de media de los ingresos en un hogar por sexo, tomemos la base de datos que tenemos como ejemplo. La función que se encarga de realizar la prueba es svyttest y solo requiere como argumentos la variable ingreso (o variable de interés), la variable sexo (variable discriminadora), el diseño muestral y el nivel de confianza. A continuación, se muestran los códigos computacionales que se requieren: svyttest(Income ~ Sex, design = diseno, level=0.95) ## ## Design-based t-test ## ## data: Income ~ Sex ## t = 1.3625, df = 118, p-value = 0.1756 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -12.82205 69.38503 ## sample estimates: ## difference in mean ## 28.28149 En esta salida podemos observar que el p-valor de la prueba es 0.14. Si tomamos una significancia del 5% para la prueba se puede concluir que, con una confianza del 95% y basados en la muestra, no existe suficiente evidencia estadística para decir que los ingresos medios en los hogares son diferentes por sexo. Por otro lado, el intervalo de confianza al 95% para la diferencia de medias entre los ingresos de hombres y mujeres es \\(\\left(-77.35,\\,11.41\\right)\\). Si ahora el objetivo es realizar la prueba de diferencia de medias para los ingresos entre hombres y mujeres pero solo en la zona urbana, los códigos computacionales son los siguientes: svyttest(Income ~ Sex, design = sub_Urbano, level = 0.95) ## ## Design-based t-test ## ## data: Income ~ Sex ## t = 1.5667, df = 63, p-value = 0.1222 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -12.31754 101.74023 ## sample estimates: ## difference in mean ## 44.71134 En donde, al igual que el anterior, no se rechaza la hipótesis nula con una confianza del 95%. Por otro lado, la función svyttest permite usar filtro. Si se requiere probar la hipótesis de diferencia de medias de ingresos por sexo pero solo en aquellas personas del hogar mayores a 18 años, se utilizará dentro de la función svyttest la función filter como se muestra a continuación: svyttest(Income ~ Sex, design = diseno %&gt;% filter(Age &gt; 18), level = 0.95 ) ## ## Design-based t-test ## ## data: Income ~ Sex ## t = 1.5263, df = 118, p-value = 0.1296 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -10.72746 82.85253 ## sample estimates: ## difference in mean ## 36.06253 y con una confianza del 95% y basado en la muestra tampoco se rechaza la hipótesis hula. Es decir, no existe evidencia estadística para concluir que los ingresos medios entre hombres y mujeres mayores de 18 años son diferentes. "],["estimando-razones-en-encuestas-de-hogares.html", "4.7 Estimando razones en encuestas de hogares", " 4.7 Estimando razones en encuestas de hogares Un caso particular de una función no lineal de totales es la razón poblacional, definida como el cociente de dos totales poblacionales de características de interés. En encuestas de hogares, este parámetro es relevante cuando, por ejemplo, se requiere conocer la cantidad de hombres por cada mujer, la proporción de ocupados respecto a la población en edad de trabajar, o la cantidad de mascotas por cada hogar. Puesto que la razón es un cociente de totales, tanto el numerador como el denominador son cantidades desconocidas y, por lo tanto, deben estimarse (Bautista, 1998). Formalmente, la razón poblacional se define como: \\[ R = \\frac{Y}{X} \\] y su estimador puntual en el marco de un muestreo complejo se expresa como: \\[ \\hat{R} = \\frac{\\hat{Y}}{\\hat{X}} = \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i} y_{h\\alpha i}} {\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i} x_{h\\alpha i}}. \\] Sin embargo, dado que $$ es un cociente entre dos estimadores —es decir, dos variables aleatorias— el cálculo de su varianza no es trivial. Para ello se emplea la linealización de Taylor, como lo muestra Gutiérrez (2016), o bien métodos de remuestreo. En términos de la función de estimación, se define: \\[ z_{hik} = y_{hik} - \\hat{R}x_{hik}, \\] y a partir de esta expresión se aplica el estimador de varianza descrito en la Sección 9.2. Implementación computacional en R En la práctica, estos cálculos se facilitan gracias a los paquetes especializados. En particular, la función survey_ratio implementa la estimación de razones y sus varianzas en el marco de encuestas complejas. Para ello es necesario especificar claramente: La variable del Expenditure (numerator), La variable del Income (denominator), El nivel de confianza (level) para los intervalos, y Las estadísticas de resumen deseadas (vartype). A continuación, se ilustra cómo estimar la razón entre el gasto y el ingreso de los hogares: diseno %&gt;% summarise( Razon = survey_ratio( numerator = Expenditure, denominator = Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.649 0.0232 0.603 0.695 Interpretación aplicada Las razones permiten expresar la relación entre dos variables, lo cual resulta especialmente útil para construir indicadores comparativos y de seguimiento. Un ejemplo directo en encuestas de hogares es la razón gasto/ingreso, que ayuda a identificar patrones de consumo y niveles de sostenibilidad económica de los hogares. Más allá de este caso, las razones también se utilizan en marcos internacionales. Por ejemplo, el Indicador 2.1.1 de los Objetivos de Desarrollo Sostenible (ODS) —prevalencia de subalimentación— se calcula a partir de la razón entre el consumo de alimentos, medido en calorías ingeridas, y los requerimientos energéticos mínimos de la dieta, determinados según edad, sexo y nivel de actividad física. De esta forma, las razones constituyen una herramienta central para transformar los resultados de encuestas en indicadores significativos que facilitan el análisis de realidades socioeconómicas y poblacionales. Como se puede observar, la razón entre el gasto y el ingreso es, aproximando, 0.71. Lo que implica que por cada unidad 100 unidades monetarias que le ingrese al hogar, se gastan 71 unidades, consiguiendo un intervalo de confianza al 95% de 0.65 y 0.76. Si ahora el objetivo es estimar la razón entre mujeres y hombres en la base de ejemplo, se realiza de la siguiente manera: diseno %&gt;% summarise( Razon = survey_ratio( numerator = (Sex == &quot;Female&quot;), denominator = (Sex == &quot;Male&quot;), level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.11 0.0351 1.04 1.18 Como la variable sexo en la base de datos es una variable categórica, se tuvo la necesidad de generar las variables dummys para su cálculo realizando, Sex == “Female” para el caso de las mujeres y Sex == “Male” para el caso de los hombres. Los resultados del ejercicio anterior muestran que en la base de datos hay más mujeres que hombres, generando una razón de 1.13. Esto significa que, por cada 100 hombres hay aproximadamente 113 mujeres con un intervalo que varía entre 1.04 y 1.21. Si se desea hacer la razón de mujeres y hombres pero en la zona rural, se haría de la siguiente manera: sub_Rural %&gt;% summarise( Razon = survey_ratio( numerator = (Sex == &quot;Female&quot;), denominator = (Sex == &quot;Male&quot;), level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.07 0.0352 0.997 1.14 Obteniendo nuevamente que hay más mujeres que hombres. Ahora bien, otro análisis de interés es estimar la razón de gastos pero solo en la población femenina. A continuación, se presentan los códigos computacionales. sub_Mujer %&gt;% summarise( Razon = survey_ratio( numerator = Expenditure, denominator = Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.658 0.0199 0.619 0.698 Dando como resultado que por cada 100 unidades monetarias que le ingresan a las mujeres se gastan 70 con un intervalo de confianza entre 0.65 y 0.76. Por último, análogamente para los hombres, la razón de gastos resulta muy similar que para las mujeres. sub_Hombre %&gt;% summarise( Razon = survey_ratio( numerator = Expenditure, denominator = Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.639 0.0288 0.582 0.696 "],["estimando-contrastes-en-encuestas-de-hogares.html", "4.8 Estimando contrastes en encuestas de hogares", " 4.8 Estimando contrastes en encuestas de hogares En muchas ocasiones, en encuestas de hogares se requiere comparar más de dos poblaciones al mismo tiempo, por ejemplo, comparar los ingresos medios de los hogares en 3 regiones o municipalidades en la postpandemia con el fin de verificar y sectorizar aquellas municipalidades o regiones donde más impacto en el desempleo y falta de ingresos tuvo el Covid-19 en los hogares. En casos como estos la diferencia de media que estudiamos en capítulos anteriores se queda corta dado que permite solo comprar parejas de poblaciones y por ende que, hacer contraste resulta una muy buena alternativa para abordar este tipo de problemas. Recurriendo en las definiciones que se han trabajado en este capítulo, un contraste es una combinación lineal de parámetros de la forma: \\[\\begin{eqnarray*} f\\left(\\theta_{1},\\theta_{2},...,\\theta_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\theta_{j} \\end{eqnarray*}\\] Una estimación de esta función está dada por: \\[\\begin{eqnarray*} f\\left(\\hat{\\theta}_{1},\\hat{\\theta}_{2},...,\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j} \\end{eqnarray*}\\] cuya varianza del estimador se calcula como sigue: \\[\\begin{eqnarray*} var\\left(\\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}^{2}var\\left(\\hat{\\theta}_{j}\\right)+2\\times\\sum_{j=1}^{J-1}\\sum_{k&gt;j}^{J}a_{j}a_{k}\\,cov\\left(\\hat{\\theta}_{j},\\hat{\\theta}_{k}\\right) \\end{eqnarray*}\\] Los procedimientos metodológicos para implementar los contrastes en diseños de muestreo complejos están desarrolladas en la función svycontrast. A continuación, se muestra el uso de dicha función para el cálculo de contraste en la base de datos de ejemplo, comparando el promedio de ingresos por región. Como primer ejemplo, se realizará la comparación de dos poblaciones, las regiones Norte y Sur (\\(\\bar{Y}_{Norte} - \\bar{Y}_{Sur}\\)) y luego sí se compararán todas las regiones. Puesto que esto es un contraste en donde hay 5 regiones y solo se construirá el contraste para la región Norte y la Sur, el contraste queda definido de la siguiente manera: \\[\\begin{eqnarray*} &amp; 1\\times\\hat{\\bar{Y}}_{Norte}+\\left(-1\\right)\\times\\hat{\\bar{Y}}_{Sur}+0\\times\\hat{\\bar{Y}}_{Centro}+0\\times\\hat{\\bar{Y}}_{Occidente}+0\\times\\hat{\\bar{Y}}_{Oriente} \\end{eqnarray*}\\] que de forma matricial queda de la siguiente manera: \\[\\begin{eqnarray*} &amp; \\left[1,\\,-1,\\,0,\\,0,\\,0\\right]\\times\\left[\\begin{array}{c} \\hat{\\bar{Y}}_{Norte}\\\\ \\hat{\\bar{Y}}_{Sur}\\\\ \\hat{\\bar{Y}}_{Centro}\\\\ \\hat{\\bar{Y}}_{Occidente}\\\\ \\hat{\\bar{Y}}_{Oriente} \\end{array}\\right] \\end{eqnarray*}\\] Como se puede observar, en este caso el vector de contraste es \\(\\left[1,\\,-1,\\,0,\\,0,\\,0\\right]\\). Ahora bien, para realizar el procesos de la construcción del estimador del contraste y su varianza estimada paso a paso se inicia con calcular las medias estimadas por región con la función svyby como se muestra a continuación: prom_region &lt;- svyby(formula = ~Income, by = ~Region, design = diseno, FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) prom_region ## Region Income se ci_l ci_u ## Norte Norte 552.3637 55.35987 443.8603 660.8670 ## Sur Sur 625.7740 62.40574 503.4610 748.0870 ## Centro Centro 650.7820 61.46886 530.3053 771.2588 ## Occidente Occidente 517.0071 46.22077 426.4161 607.5982 ## Oriente Oriente 541.7543 71.66487 401.2938 682.2149 La función svyby permite aplicar una función, en este caso la media (svymean) por región (by) utilizando el diseño muestral empleado (design). Las demás componentes de la función ya se han utilizado previamente. Como resultado de aplicar esta función se obtienen las medias estimadas de los ingresos por región. Se tomarán solo los ingresos medios estimados de las regiones Norte y Sur y calcularemos su diferencia: # Paso 1: diferencia de estimaciones (Norte - Sur) 552.4 - 625.8 ## [1] -73.4 El paso siguiente es calcular la matriz de varianzas y covarianzas y de allí extraer las varianzas y covarianzas de las regiones Norte y Sur: # Paso 2: Matriz de varianzas y covarianzas vcov(prom_region) ## Norte Sur Centro Occidente Oriente ## Norte 3064.715 0.000 0.00 0.000 0.000 ## Sur 0.000 3894.476 0.00 0.000 0.000 ## Centro 0.000 0.000 3778.42 0.000 0.000 ## Occidente 0.000 0.000 0.00 2136.359 0.000 ## Oriente 0.000 0.000 0.00 0.000 5135.854 Para calcular el error estándar de la diferencia (contraste) se usará las propiedades de la varianza como es \\(se\\left(\\hat{\\bar{y}}_{Norte}-\\hat{\\bar{y}}_{Sur}\\right)=\\sqrt{var\\left(\\hat{\\bar{y}}_{Norte}\\right)+var\\left(\\hat{\\bar{y}}_{Sur}\\right)-2\\,cov\\left(\\hat{\\bar{y}}_{Norte},\\hat{\\bar{y}}_{Sur}\\right)}\\) tenemos: sqrt(3065 + 3894 - 2*0) ## [1] 83.42062 Finalmente, la función svycontrast nos devuelve el contraste estimado y su error estándar. Los argumentos de esta función son los promedios de los ingresos estimados (stat) y las constantes de contraste (contrasts). svycontrast(stat = prom_region, contrasts = list(diff_NS = c(1, -1, 0, 0, 0))) %&gt;% data.frame() ## contrast diff_NS ## diff_NS -73.41034 83.42176 Obteniendo como resultado que los ingresos medios estimados para la región Sur es 73.4 unidades monetarias mayor que los ingresos en la región Norte con un error estándar de 83.42 unidades. Ahora bien, si el objetivo es estimar los siguientes contrastes: \\(\\bar{Y}_{Norte} - \\bar{Y}_{Centro}\\), \\(\\bar{Y}_{Sur}-\\bar{Y}_{Centro}\\) \\(\\bar{Y}_{Occidente}-\\bar{Y}_{Oriente}\\) Que escritas de forma matricial se tiene: \\[ \\left[\\begin{array}{ccccc} 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right] \\] Ahora, aplicando la función svycontrast en R se obtiene: svycontrast(stat = prom_region, contrasts = list( Norte_sur = c(1, 0, -1, 0, 0), Sur_centro = c(0, 1, -1, 0, 0), Occidente_Oriente = c(0, 0, 0, 1, -1))) %&gt;% data.frame() ## contrast SE ## Norte_sur -98.41834 82.72324 ## Sur_centro -25.00800 87.59507 ## Occidente_Oriente -24.74720 85.27727 De lo cual se puede concluir que, las regiones con los ingresos medios de los hogares más similares son la región sur y la región centro. También es posible construir contraste en variables que estén correlacionadas. Por ejemplo, Ingreso y Sexo. Como se hizo en el ejemplo anterior, se inicia con el promedio estimado por sexo. prom_sexo &lt;- svyby(formula = ~Income, by = ~Sex, design = diseno, FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) prom_sexo ## Sex Income se ci_l ci_u ## Female Female 557.5681 25.82995 506.9423 608.1939 ## Male Male 585.8496 34.58759 518.0592 653.6400 El contraste a estimar es: \\[ \\bar{Y}_{F} - \\bar{Y}_{M}\\] Por tanto, usando la función svycontrast se obtiene el contraste estimado: svycontrast(stat = prom_sexo, contrasts = list(diff_Sexo = c(1, -1))) %&gt;% data.frame() ## contrast diff_Sexo ## diff_Sexo -28.28149 20.75651 Obteniendo como resultado que, en promedio, los hombres obtienen 28.3 unidades monetarias más que las mujeres con una desviación de 20.76. Otra posibilidad es poder obtener resultados agregados, por ejemplo: \\(\\hat{\\bar{y}}_{Norte}+\\hat{\\bar{y}}_{Sur} +\\hat{\\bar{y}}_{Centro}\\) sum_region &lt;- svyby( ~ Income, ~ Region, diseno, svytotal, na.rm = T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) sum_region ## Region Income se ci_l ci_u ## Norte Norte 14277323 1507575 11322530 17232115 ## Sur Sur 16068151 1877989 12387359 19748942 ## Centro Centro 16483319 2383556 11811634 21155003 ## Occidente Occidente 16853540 1823807 13278944 20428135 ## Oriente Oriente 22111335 2833460 16557856 27664814 La matriz de contraste queda como: \\[ \\left[\\begin{array}{cccccc} 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\end{array}\\right] \\] el procedimiento en R es: svycontrast(stat = sum_region, contrasts = list( Agregado_NCS = c(1, 1, 1, 0, 0))) %&gt;% data.frame() ## contrast Agregado_NCS ## Agregado_NCS 46828792 3388357 Por otro lado, si se desean obtener los promedios por categorías. Por ejemplo: \\[ \\hat{\\bar{y}}_{Edad} = \\frac{1}{k}\\sum_{k=1}^K\\hat{\\bar{y}}_{k} \\] donde \\(K\\) es el número de categorías de la variable. En R se hace de la siguiente manera: prom_edad &lt;- svyby(formula = ~Income, by = ~CatAge, design = diseno, FUN = svymean, na.rm=T, covmat = TRUE) prom_edad ## CatAge Income se ## 0-5 0-5 463.7715 28.86795 ## 6-15 6-15 511.6179 34.88031 ## 16-30 16-30 607.2917 37.41561 ## 31-45 31-45 573.4167 26.94744 ## 46-60 46-60 763.0610 58.97170 ## Más de 60 Más de 60 466.6133 31.20795 Cuya matriz de contraste estaría dada por: \\[ \\left[\\begin{array}{cccccc} \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} \\end{array}\\right] \\] El procedimiento en R es: svycontrast(stat = prom_edad, contrasts = list( agregado_edad = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))) %&gt;% data.frame() ## contrast agregado_edad ## agregado_edad 564.2954 25.40408 Puesto que los contrastes, como ya se mencionó, es una función lineal de parámetros, se puede también realizar contraste con parámetros tipo razón. Por ejemplo, la relación de gastos contra ingresos por sexo. A continuación, se muestran los códigos computacionales: razon_sexo &lt;- svyby( formula = ~Income, by = ~Sex, denominator = ~Expenditure, design = diseno, FUN = svyratio, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) razon_sexo ## Sex Income/Expenditure se.Income/Expenditure ci_l ci_u ## Female Female 1.519060 0.04582607 1.429243 1.608878 ## Male Male 1.564762 0.07044239 1.426698 1.702827 Cuya estimación de contraste sería: svycontrast(stat = razon_sexo, contrasts = list( diff_sexo = c(1, -1))) %&gt;% data.frame() ## contrast diff_sexo ## diff_sexo -0.04570214 0.04163431 de lo que se puede concluir que la diferencia de las proporciones es 0.045 en favor de los hombres. "],["análisis-de-variables-categóricas-en-encuestas-de-hogares.html", "Capítulo 5 Análisis de variables categóricas en encuestas de hogares", " Capítulo 5 Análisis de variables categóricas en encuestas de hogares Al analizar datos de encuestas de hogares, uno de los productos más habituales son los parámetros descriptivos, cuyo propósito es sintetizar las principales características de la población. Estas estimaciones permiten ofrecer una representación clara y comprensible de la realidad poblacional a partir de la información obtenida en una muestra representativa. En ocasiones, no es sencillo distinguir entre las variables denominadas cualitativas y cuantitativas, puesto que algunas variables de tipo cuantitativo pueden considerarse categóricas si se divide el rango de valores en intervalos o categorías. Un ejemplo clásico es la variable edad, que en una encuesta de hogares se registra como cuantitativa, pero puede agruparse en categorías. Por ejemplo, en Colombia, las categorías podrían ser: Adolescencia (12–18 años), Juventud (14–26 años), Adultez (27–59 años) y Persona Mayor (60 años o más), incorporando también conceptos de envejecimiento y vejez. De manera inversa, una variable categórica también puede transformarse en cuantitativa mediante análisis específicos, como un análisis de correspondencias. Esto es frecuente cuando se construyen índices compuestos, como el índice de fuerza laboral. En el contexto de encuestas, las preguntas que contienen variables categóricas son muy comunes y sus resultados suelen presentarse en porcentajes, por ejemplo: parentesco, sexo, jefe o jefa del hogar, acceso a agua potable, entre otros. Entre los resultados más comunes de este tipo de análisis se encuentran frecuencias, proporciones, medias y totales. Las medias proporcionan información sobre el valor promedio de una variable, mientras que los totales reflejan su acumulado en toda la población. Las frecuencias cuentan cuántos hogares o individuos pertenecen a una categoría determinada —por ejemplo, el número de personas en situación de pobreza—, y las proporciones expresan la participación relativa de quienes presentan una característica específica, como el porcentaje de población pobre. Actualmente, el análisis descriptivo va más allá de los parámetros básicos, incorporando métricas más complejas. Se estiman cuantiles de variables numéricas, como la mediana del ingreso de los hogares, para describir la distribución de los datos con mayor detalle. Además, se aplican indicadores especializados para evaluar fenómenos concretos, como los índices FGT para la medición de pobreza, los indicadores de desigualdad (Gini, Theil, Atkinson) y de polarización (Wolfson, DER), entre otros (Jacob, Damico y Pessoa, 2024). library(tidyverse) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) head(encuesta) ## HHID Stratum NIh nIh dI PersonID PSU Zone Sex Age MaritalST ## 1 idHH00031 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Male 68 Married ## 2 idHH00031 idStrt001 9 2 4.5 idPer02 PSU0003 Rural Female 56 Married ## 3 idHH00031 idStrt001 9 2 4.5 idPer03 PSU0003 Rural Female 24 Married ## 4 idHH00031 idStrt001 9 2 4.5 idPer04 PSU0003 Rural Male 26 Married ## 5 idHH00031 idStrt001 9 2 4.5 idPer05 PSU0003 Rural Female 3 &lt;NA&gt; ## 6 idHH00041 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Female 61 Widowed ## Income Expenditure Employment Poverty dki dk wk Region CatAge ## 1 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte Más de 60 ## 2 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 46-60 ## 3 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 16-30 ## 4 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte 16-30 ## 5 409.87 346.34 &lt;NA&gt; NotPoor 8 36 33.63761 Norte 0-5 ## 6 823.75 392.24 Employed NotPoor 8 36 33.63761 Norte Más de 60 Definición del diseño y creación de variables categóricas Se inicia este capítulo haciendo el ajuste del diseño muestral (como se mostró en capítulos anteriores) usando como ejemplo la misma base de datos del capítulo anterior. Luego, para efectos del ejemplo, se genera una variable categórica la cual indica si la persona encuestada está en estado de pobreza o no como sigue: library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = TRUE) A continuación, se define una variable categórica que nace de variables propias de la encuesta, diseno &lt;- diseno %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0), edad_18 = case_when(Age &lt; 18 ~ &quot;&lt; 18 anios&quot;, TRUE ~ &quot;&gt;= 18 anios&quot;) ) Como se pudo observar en el código anterior, se ha introducido la función case_when la cual es una extensión del a función ifelse que permite crear múltiples categorías a partir de una o varias condiciones. Como se ha mostrado anteriormente, en ocasiones se desea realizar estimaciones por sub-grupos de la población, en este caso se extraer 4 sub-grupos de la encuesta y se definen a continuación: sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) "],["tamaño-de-población-y-subpoblaciones-en-encuestas-de-hogares.html", "5.1 Tamaño de población y subpoblaciones en encuestas de hogares", " 5.1 Tamaño de población y subpoblaciones en encuestas de hogares En el análisis de encuestas de hogares, resulta esencial determinar el tamaño de las subpoblaciones, es decir, identificar cuántas personas u hogares pertenecen a categorías específicas y qué proporción representan dentro del total poblacional. Este tipo de estimaciones permite caracterizar el perfil demográfico y socioeconómico de la población, información clave para orientar la asignación de recursos, el diseño de políticas públicas y la formulación de programas sociales. Así, es de gran utilidad conocer cuántas personas se encuentran por debajo de la línea de pobreza, cuántas no tienen empleo o cuántas han alcanzado determinado nivel educativo. Analizar cómo se distribuyen los individuos entre distintas categorías ofrece información indispensable para reducir brechas y avanzar hacia un desarrollo inclusivo. La estimación del tamaño de una población o subpoblación se realiza a partir de variables categóricas, que segmentan a la población en grupos mutuamente excluyentes. Estas categorías pueden corresponder, por ejemplo, a quintiles de ingreso, estados de ocupación o niveles educativos alcanzados. El tamaño poblacional hace referencia al número total de individuos u hogares que, en la base de datos de la encuesta, pertenecen a una categoría determinada. Para obtener estas estimaciones, se combinan las respuestas de los encuestados con los pesos muestrales, que indican cuántas personas u hogares representa cada unidad de la muestra dentro de la población total. El estimador del tamaño de la población se define como: \\[\\hat{N} = \\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik}\\] donde \\(s_{hi}\\) corresponde a la muestra de hogares o individuos en la UPM \\(i\\) del estrato \\(h\\); \\(s_{1h}\\) representa la muestra de UPM seleccionadas en el estrato \\(h\\); y \\(w_{hik}\\) es el peso o factor de expansión de la unidad \\(k\\) en la UPM \\(i\\) del estrato \\(h\\). La estimación del tamaño de una subpoblación sigue el mismo principio que el cálculo del tamaño poblacional total, pero se enfoca en un subconjunto definido por una característica específica. Para determinar cuántas personas pertenecen a una categoría particular, se identifica dicho grupo en la base de datos y se suman sus pesos muestrales. Esto permite cuantificar grupos de interés específicos y conocer su tamaño dentro de la población: \\[\\hat{N}_d = \\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik} I(y_{hik}=d)\\] donde \\(I(y_{hik}=d)\\) es una variable binaria que toma el valor de 1 si la unidad \\(k\\) de la UPM \\(i\\) en el estrato \\(h\\) pertenece a la categoría \\(d\\) de la variable discreta \\(y\\), y 0 en caso contrario. Si \\(d\\) fue utilizada en la calibración de los pesos, el valor de \\(\\hat{N}_d\\) coincidirá con el control externo aplicado. Estimaciones de totales en R En esta sección se presentan los procedimientos para estimar tamaños de población y subpoblaciones usando R, con el diseño muestral definido previamente. Por ejemplo, para estimar el tamaño de la población por zona: tamano_zona &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise( n = unweighted(n()), Nd = survey_total(vartype = c(&quot;se&quot;,&quot;ci&quot;))) tamano_zona ## # A tibble: 2 × 6 ## Zone n Nd Nd_se Nd_low Nd_upp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 1297 72102 3062. 66039. 78165. ## 2 Urban 1308 78164 2847. 72526. 83802. En la tabla resultante, n indica el número de observaciones en la muestra por zona y Nd representa la estimación del total de observaciones en la población. La función unweighted() calcula resúmenes no ponderados a partir de los datos muestrales. Por ejemplo, el tamaño de muestra en la zona rural fue de 1297 personas y en la urbana de 1308. Esto permitió estimar una población de 72,102 (desviación estándar 3,062) en la zona rural y 78,164 (desviación estándar 2,847) en la zona urbana. Con un nivel de confianza del 95%, los intervalos de confianza fueron: Zona rural: (66,038.5, 78,165.4) Zona urbana: (72,526.2, 83,801.7) De manera similar, es posible estimar el número de personas en condición de pobreza extrema, pobreza y no pobres: tamano_pobreza &lt;- diseno %&gt;% group_by(Poverty) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;,&quot;ci&quot;))) tamano_pobreza ## # A tibble: 3 × 5 ## Poverty Nd Nd_se Nd_low Nd_upp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NotPoor 91398. 4395. 82696. 100101. ## 2 Extreme 21519. 4949. 11719. 31319. ## 3 Relative 37349. 3695. 30032. 44666. Estos cálculos permiten obtener estimaciones precisas y sus intervalos de confianza para cada subpoblación, facilitando el análisis socioeconómico y la toma de decisiones basadas en evidencia. Otra variable de interés en encuestas de hogares es conocer el estado de ocupación de las personas. A continuación, se muestra el código computacional: tamano_ocupacion &lt;- diseno %&gt;% group_by(Employment) %&gt;% summarise( Nd = survey_total(vartype = c(&quot;se&quot;,&quot;ci&quot;))) tamano_ocupacion ## # A tibble: 4 × 5 ## Employment Nd Nd_se Nd_low Nd_upp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Unemployed 4635. 761. 3129. 6141. ## 2 Inactive 41465. 2163. 37183. 45748. ## 3 Employed 61877. 2540. 56847. 66907. ## 4 &lt;NA&gt; 42289. 2780. 36784. 47794. De los resultados de la estimación se puede concluir que, 4634.8 personas están desempleadas con un intervalo de confianza de (3128.6, 6140.9). 41465.2 personas están inactivas con un intervalo de confianza de (37182.6, 45747.8) y por último, 61877.0 personas empleadas con intervalos de confianza (36784.2, 47793.5). Utilizando la función group_by es posible obtener resultados por más de un nivel de agregación. A continuación, se muestra la estimación ocupación desagregada por niveles de pobreza: tamano_ocupacion_pobreza &lt;- diseno %&gt;% group_by(Employment, Poverty) %&gt;% cascade( Nd = survey_total(vartype = c(&quot;se&quot;,&quot;ci&quot;)), .fill = &quot;Total&quot;) %&gt;% data.frame() tamano_ocupacion_pobreza ## Employment Poverty Nd Nd_se Nd_low Nd_upp ## 1 Unemployed NotPoor 1768.375 405.3765 965.6891 2571.061 ## 2 Unemployed Extreme 1169.201 348.1340 479.8603 1858.541 ## 3 Unemployed Relative 1697.231 457.8077 790.7262 2603.736 ## 4 Unemployed Total 4634.807 760.6242 3128.6948 6140.919 ## 5 Inactive NotPoor 24346.008 1736.2770 20908.0064 27784.010 ## 6 Inactive Extreme 6421.825 1320.7349 3806.6383 9037.012 ## 7 Inactive Relative 10697.414 1460.2792 7805.9155 13588.913 ## 8 Inactive Total 41465.248 2162.8040 37182.6798 45747.816 ## 9 Employed NotPoor 44600.347 2596.1915 39459.6282 49741.065 ## 10 Employed Extreme 5127.531 1121.6461 2906.5601 7348.503 ## 11 Employed Relative 12149.142 1346.6159 9482.7078 14815.576 ## 12 Employed Total 61877.020 2540.0762 56847.4153 66906.624 ## 13 Total Total 150266.000 4181.3587 141986.4921 158545.508 ## 14 &lt;NA&gt; NotPoor 20683.603 1256.6158 18195.3777 23171.827 ## 15 &lt;NA&gt; Extreme 8800.209 2979.9150 2899.6792 14700.738 ## 16 &lt;NA&gt; Relative 12805.115 1551.0291 9733.9220 15876.307 ## 17 &lt;NA&gt; Total 42288.926 2779.9913 36784.2652 47793.586 De lo cual se puede concluir, entre otros que, 44600.3 personas que trabajan no son pobres con un intervalo de confianza (39459.6, 49741.0) y 6421.8 inactivas están en pobreza extrema con un intervalo de confianza de (3806.6, 9037.0). "],["estimación-de-proporciones.html", "5.2 Estimación de proporciones", " 5.2 Estimación de proporciones En las encuestas de hogares, las proporciones permiten expresar el peso relativo que tienen determinados grupos dentro de la población. Por ejemplo, conocer el porcentaje de hogares que se encuentran por debajo de la línea de pobreza es esencial para evaluar desigualdades socioeconómicas. Para obtener este indicador, se calcula el promedio ponderado de una variable dicotómica, lo que asegura que la estimación represente adecuadamente la distribución poblacional. De acuerdo con Heeringa, West y Berglund (2017), al transformar las categorías de respuesta originales en variables indicadoras \\(y\\) con valores de 1 y 0 (por ejemplo, 1 = Sí y 0 = No), la proporción estimada se obtiene mediante: \\[\\hat{p}_d = \\frac{\\hat{N}_d}{\\hat{N}} = \\frac{\\displaystyle\\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik} I(y_{hik}=d)} {\\displaystyle\\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik}}\\] Dado que se trata de un estimador no lineal, su varianza puede aproximarse mediante la técnica de linealización de Taylor, utilizando como función de estimación \\(z_{hik}=I(y_{hik}=d)-\\hat{p}_d\\). Actualmente, la mayoría de los programas estadísticos generan estas proporciones junto con sus errores estándar, generalmente presentados en forma de porcentajes. En situaciones donde las proporciones se aproximan a 0 o 1, puede ser necesario aumentar el tamaño de la muestra para garantizar resultados sólidos. También es recomendable aplicar métodos que aseguren que los intervalos de confianza permanezcan dentro del rango [0,1], ya que los intervalos convencionales basados en la normal pueden desbordar estos límites y perder su utilidad interpretativa. Una alternativa es la transformación logit de la proporción estimada: \\[CI(\\hat{p}_d; 1-\\alpha) = \\frac{\\exp \\left[\\ln\\left(\\frac{\\hat{p}_d}{1-\\hat{p}_d}\\right) \\pm \\frac{me(\\hat{p}_d)}{\\hat{p}_d(1-\\hat{p}_d)}\\right]}{1 + \\exp \\left[\\ln\\left(\\frac{\\hat{p}_d}{1-\\hat{p}_d}\\right) \\pm \\frac{me(\\hat{p}_d)}{\\hat{p}_d(1-\\hat{p}_d)}\\right]}\\] donde \\(me(\\hat{p}_d) = t_{1-\\alpha/2, df} \\times se(\\hat{p}_d)\\), siendo \\(t_{1-\\alpha/2, df}\\) el cuantil de la distribución t de Student con \\(df = n - H\\) grados de libertad, dejando un área \\(\\alpha/2\\) a su derecha. Este procedimiento asegura resultados interpretables incluso para proporciones extremas (0 o 1). 5.2.1 Estimación de proporciones en R Otro parámetro de interés en las encuestas de hogares, particularmente con variables categóricas, es la estimación de proporciones poblacionales y sus errores estándar. En términos de notación, la estimación de proporciones de población se define como \\(p\\) y las proporciones muestrales como \\(\\pi\\). Es común que muchos paquetes estadísticos generen estas estimaciones en escala de porcentaje, mientras que R las produce en la escala [0,1]. A continuación, se muestra cómo estimar la proporción de personas por zona usando srvyr y el diseño muestral previamente definido: prop_zona &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise( prop = survey_mean(vartype = c(&quot;se&quot;,&quot;ci&quot;), proportion = TRUE)) prop_zona ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.480 0.0140 0.452 0.508 ## 2 Urban 0.520 0.0140 0.492 0.548 En este ejemplo, la función survey_mean calcula la proporción y, con el parámetro proportion = TRUE, se indica que se desea estimar una proporción poblacional. Los resultados muestran que aproximadamente el 47.9% de las personas viven en zona rural (IC 95%: 45.2% - 50.7%) y el 52% en zona urbana (IC 95%: 49.2% - 54.7%). La librería survey también cuenta con la función survey_prop, diseñada específicamente para estimar proporciones, generando resultados equivalentes: prop_zona2 &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) prop_zona2 ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.480 0.0140 0.452 0.508 ## 2 Urban 0.520 0.0140 0.492 0.548 El lector puede elegir la función que le resulte más cómoda, ya que ambas permiten obtener estimaciones precisas y sus intervalos de confianza, facilitando la interpretación de los resultados para variables categóricas. Si el interés ahora se centra en estimar subpoblaciones por ejemplo, proporción de hombres y mujeres que viven en la zona urbana, el código computacional es: prop_sexoU &lt;- sub_Urbano %&gt;% group_by(Sex) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) prop_sexoU ## # A tibble: 2 × 5 ## Sex prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 0.537 0.0130 0.511 0.563 ## 2 Male 0.463 0.0130 0.437 0.489 Arrojando como resultado que, el 53.6% de las mujeres y 46.4% de los hombres viven en la zona urbana y con intervalos de confianza (51%, 56.2%) y (43.7%, 48.9%) respectivamente. Los intervalos anteriores nos reflejan que, con una confianza del 95% la cantidad estimada de mujeres que viven en la zona urbana es de56% y de hombres es de 48%. Realizando el mismo ejercicio anterior, pero ahora en la zona rural se tiene: prop_sexoR &lt;- sub_Rural %&gt;% group_by(Sex) %&gt;% summarise( n = unweighted(n()), prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) prop_sexoR ## # A tibble: 2 × 6 ## Sex n prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 679 0.516 0.00824 0.500 0.533 ## 2 Male 618 0.484 0.00824 0.467 0.500 el 51.6% de las mujeres y el 48.4% de los hombres viven en la zona rural con intervalos de confianza de (49.9%, 53.2%) y (46,7%, 50%) respectivamente. Los intervalos de confianza anteriores nos reflejan que, inclusive, con una confianza del 95%, la cantidad estimada de mujeres en la zona rural es de 53% y de hombres es de 50%. Ahora bien, si nos centramos solo en la población de hombres en la base de datos y se desea estimar la proporción de hombres por zona, el código computacional es el siguiente: prop_ZonaH &lt;- sub_Hombre %&gt;% group_by(Zone) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) prop_ZonaH ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.491 0.0178 0.455 0.526 ## 2 Urban 0.509 0.0178 0.474 0.545 En la anterior tabla se puede observar que el 49% de los hombres están en la zona rural y el 51% en la zona urbana. Si se observa el intervalo de confianza se puede concluir que, con una confianza del 95%, la población estimada de hombres que viven en la zona rural puede llegar a ser el 52% y en urbana un 54%. Si se realiza ahora el mismo ejercicio para la mujeres el código computacional es: prop_ZonaM &lt;- sub_Mujer %&gt;% group_by(Zone) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) prop_ZonaM ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.470 0.0140 0.443 0.498 ## 2 Urban 0.530 0.0140 0.502 0.557 De la tabla anterior se puede inferir que, el 47% de las mujeres están en la zona rural y el 52% en la zona urbana. Observando también intervalos de confianza al 95% de (44%, 49%) y (50%, 55%) para las zonas rural y urbana respectivamente. Si se desea estimar por varios niveles de desagregación, con el uso de la función group_by es posible estimar un mayor número de niveles de agregación al combinar dos o más variables. Por ejemplo, si se desea estimar la proporción de hombres por zona y en estado de pobreza, se realiza de la siguiente manera: prop_ZonaH_Pobreza &lt;- sub_Hombre %&gt;% group_by(Zone, Poverty) %&gt;% summarise( prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;)))%&gt;% data.frame() prop_ZonaH_Pobreza ## Zone Poverty prop prop_se prop_low prop_upp ## 1 Rural NotPoor 0.5488453 0.06264753 0.42434340 0.6675180 ## 2 Rural Extreme 0.1975254 0.06745258 0.09582905 0.3637294 ## 3 Rural Relative 0.2536293 0.03724070 0.18711180 0.3340755 ## 4 Urban NotPoor 0.6599255 0.03662268 0.58415144 0.7283141 ## 5 Urban Extreme 0.1128564 0.02451869 0.07264146 0.1712240 ## 6 Urban Relative 0.2272181 0.02604053 0.17979436 0.2828371 De la salida anterior se puede observar que, en la ruralidad, el 19% de los hombres están en pobreza extrema mientras que en la zona urbana el 11% también están en pobreza extrema. Por otro lado, el 54% de los hombres que viven en la zona rural no están en pobreza mientras que, en la zona urbana el 65% no está en esta condición. El mismo ejercicio anterior para la población de mujeres sería: prop_ZonaM_Pobreza &lt;- sub_Mujer %&gt;% group_by(Zone, Poverty) %&gt;% summarise( prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) %&gt;% data.frame() prop_ZonaM_Pobreza ## Zone Poverty prop prop_se prop_low prop_upp ## 1 Rural NotPoor 0.5539176 0.05568825 0.44281376 0.6598834 ## 2 Rural Extreme 0.1599702 0.05574533 0.07728197 0.3021593 ## 3 Rural Relative 0.2861122 0.04357612 0.20803909 0.3794466 ## 4 Urban NotPoor 0.6612172 0.03224726 0.59475977 0.7218725 ## 5 Urban Extreme 0.1093753 0.02209821 0.07267359 0.1613865 ## 6 Urban Relative 0.2294075 0.02655874 0.18106582 0.2861459 De la salida anterior se puede observar que, en la ruralidad, el 16% de las mujeres están en pobreza extrema mientras que en la zona urbana el 10% también están en pobreza extrema. Por otro lado, el 55% de las mujeres que viven en la zona rural no están en pobreza mientras que, en la zona urbana el 66% no está en esta condición. Si lo que se desea ahora es estimar la proporción de hombres empleados o no por zona, se realiza de la siguiente manera: prop_ZonaH_Ocupacion &lt;- sub_Hombre %&gt;% group_by(Zone, Employment) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;)))%&gt;% data.frame() prop_ZonaH_Ocupacion ## Zone Employment prop prop_se prop_low prop_upp ## 1 Rural Unemployed 0.05125186 0.015733138 0.02767737 0.09298588 ## 2 Rural Inactive 0.10351629 0.020267044 0.06970747 0.15106011 ## 3 Rural Employed 0.52251375 0.026522751 0.46994089 0.57459249 ## 4 Rural &lt;NA&gt; 0.32271810 0.034987840 0.25763953 0.39547790 ## 5 Urban Unemployed 0.04374724 0.008492664 0.02969659 0.06400729 ## 6 Urban Inactive 0.16331307 0.018093938 0.13056379 0.20236490 ## 7 Urban Employed 0.51337023 0.023637331 0.46658553 0.55992181 ## 8 Urban &lt;NA&gt; 0.27956945 0.022085422 0.23799131 0.32531045 De la salida anterior se puede observar que, el 5% de los hombres que viven en la ruralidad están desempleados mientras que el 4% de los que viven en la zona urbana están en esta misma condición. Ahora bien, el 52% de los hombres que viven en la ruralidad trabajan mientras que el 51% de los que viven en la zona rural también están empleados. Si se hace este mismo ejercicio para las mujeres se obtiene: prop_ZonaM_Ocupacion &lt;- sub_Mujer %&gt;% group_by(Zone, Employment) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;,&quot;ci&quot;))) %&gt;% data.frame() prop_ZonaM_Ocupacion ## Zone Employment prop prop_se prop_low prop_upp ## 1 Rural Unemployed 0.01017065 0.005540256 0.003443802 0.02964628 ## 2 Rural Inactive 0.44719272 0.035247218 0.378871481 0.51756811 ## 3 Rural Employed 0.23999716 0.039151859 0.171118101 0.32570701 ## 4 Rural &lt;NA&gt; 0.30263948 0.030765644 0.245379430 0.36676711 ## 5 Urban Unemployed 0.02109678 0.005964137 0.012019202 0.03677508 ## 6 Urban Inactive 0.36445938 0.021442387 0.323143427 0.40787461 ## 7 Urban Employed 0.38455672 0.019452094 0.346831628 0.42372325 ## 8 Urban &lt;NA&gt; 0.22988711 0.013850398 0.203613820 0.25845036 Para las mujeres se puede observar que, el 1% de las mujeres que viven en la ruralidad están desempleados mientras que el 2% de las que viven en la zona urbana están en esta misma condición. Ahora bien, el 24% de las mujeres que viven en la ruralidad trabajan mientras que el 38% de las que viven en la zona rural también están empleados. Otro parámetro que es de interés es estimar en encuestas de hogares la cantidad de personas menores y mayores de edad en los hogares. A continuación, ejemplificamos la estimación de menores y mayores a 18 años cruzado por pobreza: diseno %&gt;% group_by(edad_18, pobreza) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_18 pobreza Prop Prop_se Prop_low Prop_upp ## 1 &lt; 18 anios 0 0.4984504 0.03729355 0.4251710 0.5717964 ## 2 &lt; 18 anios 1 0.5015496 0.03729355 0.4282036 0.5748290 ## 3 &gt;= 18 anios 0 0.6646140 0.02978353 0.6033275 0.7208132 ## 4 &gt;= 18 anios 1 0.3353860 0.02978353 0.2791868 0.3966725 De la anterior salida se puede observar que, el 50% de los menores de edad y el 33% de los mayores de edad están en estado de pobreza. Al observar los intervalos de confianza para los menores de edad en estado de pobreza se puede observar que, dicha estimación puede llegar, con una confianza del 95% a 57% mientras que a los mayores de edad puede llegar a 39%. Ahora, si se hace este mismo ejercicio, pero esta vez cruzando con la variable que indica empleo se obtiene: diseno %&gt;% group_by(edad_18, desempleo) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_18 desempleo Prop Prop_se Prop_low Prop_upp ## 1 &lt; 18 anios 0 0.166704172 0.014856561 0.139321648 0.19822898 ## 2 &lt; 18 anios 1 0.003729693 0.001969183 0.001309174 0.01057808 ## 3 &lt; 18 anios NA 0.829566135 0.015009188 0.797760089 0.85726505 ## 4 &gt;= 18 anios 0 0.955234872 0.007552778 0.937660285 0.96802386 ## 5 &gt;= 18 anios 1 0.044765128 0.007552778 0.031976144 0.06233972 De la tabla anterior se puede observar que, el 0.3% de los menores de edad y el 4% de los mayores de edad están desempleados. Adicionalmente, con una confianza del 95% y basados en la muestra se puede observar que el desempleo en menores de edad puede llegar a 0.7% y para los mayores llega a un 5%. Por otro lado, si el objetivo ahora es estimar la cantidad de menores de edad en la zona rural se realiza de la siguiente manera: sub_Rural %&gt;% group_by(edad_18) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_18 Prop Prop_se Prop_low Prop_upp ## 1 &lt; 18 anios 0.3711613 0.03021982 0.3128746 0.4334566 ## 2 &gt;= 18 anios 0.6288387 0.03021982 0.5665434 0.6871254 De la anterior tabla se puede observar que, el 37% de las personas que viven en la zona rural de la base de ejemplo son menores de edad con un intervalo de confianza al 95% comprendido entre 31% y 43%. Como se mencionó al inicio del capítulo, es posible categorizar una variable de tipo cuantitativo como por ejemplo la edad y cruzarla con la variable que categoriza la empleabilidad. A continuación, se estima la edad de las mujeres por rango. sub_Mujer %&gt;% mutate(edad_rango = case_when( Age&gt;= 18 &amp; Age &lt;=35 ~ &quot;18 - 35&quot;, TRUE ~ &quot;Otro&quot;)) %&gt;% group_by(edad_rango, Employment) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_rango Employment Prop Prop_se Prop_low Prop_upp ## 1 18 - 35 Unemployed 0.02893412 0.009142347 0.015403014 0.05370358 ## 2 18 - 35 Inactive 0.51653851 0.037905184 0.441673039 0.59066889 ## 3 18 - 35 Employed 0.45452737 0.035685710 0.385232560 0.52562948 ## 4 Otro Unemployed 0.01015164 0.004026104 0.004617517 0.02217073 ## 5 Otro Inactive 0.35271022 0.020725430 0.312834830 0.39474850 ## 6 Otro Employed 0.25483870 0.021700305 0.214292062 0.30012671 ## 7 Otro &lt;NA&gt; 0.38229944 0.022313379 0.339191706 0.42734277 De la anterior tabla se puede observar, entre otros que, las mujeres con edades entre 18 y 35 años el 2% están desempleadas y el 45% están empleadas. Análisis similares se pueden hacer para los demás rangos de edades. Este mismo ejercicio se puede realizar para los hombres y hacer los mismos análisis. A continuación, se muestra el código computacional: sub_Hombre %&gt;% mutate(edad_rango = case_when( Age&gt;= 18 &amp; Age &lt;=35 ~ &quot;18 - 35&quot;,TRUE ~ &quot;Otro&quot;)) %&gt;% group_by(edad_rango, Employment) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() ## edad_rango Employment Prop Prop_se Prop_low Prop_upp ## 1 18 - 35 Unemployed 0.09637042 0.018215667 0.06584071 0.13895080 ## 2 18 - 35 Inactive 0.08939940 0.016438321 0.06175556 0.12773290 ## 3 18 - 35 Employed 0.81423018 0.022991735 0.76436394 0.85553799 ## 4 Otro Unemployed 0.02606667 0.007175709 0.01506262 0.04474457 ## 5 Otro Inactive 0.15344056 0.019883462 0.11805657 0.19706023 ## 6 Otro Employed 0.38849664 0.020270309 0.34919327 0.42930563 ## 7 Otro &lt;NA&gt; 0.43199614 0.021111842 0.39076987 0.47418649 "],["tablas-cruzadas..html", "5.3 Tablas cruzadas.", " 5.3 Tablas cruzadas. Una tabla de contingencia o tablas cruzadas es una herramienta muy utilizada en el análisis de encuestas de hogares puesto que, está conformada por al menos dos filas y dos columnas y representa información de variables categóricos en términos de conteos de frecuencia. Estas tablas tienen el objetivo de representar de manera resumida, la relación entre diferentes variables categóricas. una tabla de contingencia se asume como un arreglo bidimensional de \\(r=1,\\ldots,R\\) filas y \\(c=1,\\ldots,C\\) columnas. Cabe resaltar que, Las tablas cruzadas o de contingencia no se limitan a dos dimensiones, también se pueden incluir una tercera variable o más, es decir, \\(l=1,\\ldots,L\\) subtablas basadas en las categorías de una tercera variable. Para efectos de ilustración y facilitación de los ejemplos y conceptos teóricos, en esta sección de trabajarán, en su mayoría con tablas \\(2\\times2\\). Gráficamente, estas tablas se construyen con frecuencias no estimadas como se muestra a continuación: Variable 2 Variable 1 Marginal fila 0 1 0 \\(n_{00}\\) \\(n_{01}\\) \\(n_{0+}\\) 1 \\(n_{10}\\) \\(n_{11}\\) \\(n_{1+}\\) Marginal columna \\(n_{+0}\\) \\(n_{+1}\\) \\(n_{++}\\) A continuación, se muestra la tabla de doble entrada con las frecuencias estimadas o ponderadas: Variable 2 Variable 1 Marginal fila 0 1 0 \\(\\hat{N}_{00}\\) \\(\\hat{N}_{01}\\) \\(\\hat{N}_{0+}\\) 1 \\(n_{10}\\) \\(n_{11}\\) \\(n_{1+}\\) Marginal columna \\(n_{+0}\\) \\(n_{+1}\\) \\(n_{++}\\) donde, por ejemplo, la frecuencia ponderada o estimada en la celda (0, 1) está dada por \\(\\hat{N}_{01}={ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i\\in\\left(0,1\\right)}^{n_{h\\alpha}}}\\omega_{h\\alpha i}\\). Las proporciones estimadas a partir de estas frecuencias muestrales ponderadas, se obtienen de la siguiente manera \\(p_{rc}=\\frac{\\hat{N}_{rc}}{\\hat{N}_{++}}\\). Estimación de proporciones para variables binarias La estimación de una sola proporción, \\(\\pi\\), para una variable de respuesta binaria requiere solo una extensión directa del estimador de razón mostrado en secciones anteriores. Como lo menciona Heeringa, S. G. (2017) Al recodificar las categorías de respuesta originales en una sola variable indicadora \\(y_{i}\\) con valores posibles de 1 y 0 (por ejemplo, sí = 1, no = 0), el estimador de la media de la razón estima la proporción o prevalencia, \\(\\pi\\), de “1” en la población está dada por: \\[ p = \\frac{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i\\in\\left(0,1\\right)}^{n_{h\\alpha}}}\\omega_{h\\alpha i}I\\left(y_{i}=1\\right)}{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i\\in\\left(0,1\\right)}^{n_{h\\alpha}}}\\omega_{h\\alpha i}} = \\frac{\\hat{N}_{1}}{\\hat{N}} \\] Aplicando Linealización de Taylor (TSL) al estimador de razón de \\(\\pi\\) genera el siguiente estimador para la varianza: \\[ v\\left(p\\right) \\dot{=} \\frac{V\\left(\\hat{N}_{1}\\right)+p^{2}V\\left(\\hat{N}\\right)-2\\,p\\,cov\\left(\\hat{N}_{1},\\hat{N}\\right)}{\\hat{N}^{2}} \\] Como es bien sabido en la literatura especializada, cuando la proporción de interés estimada está cerca de 0 o 1, los límites del intervalo de confianza estándar basados en el diseño de muestreo pueden ser menores que 0 o superiores a 1. Lo cual no tendría interpretación por la naturaleza del parámetro. Es por lo anterior que, para solventar este problema se puede realizar cálculos alternativos de IC basados en el diseño de muestreo para las proporciones como lo proponen Wilson modificado (Rust y Hsu, 2007; Dean y Pagano, 2015). El intervalo de confianza utilizando la transformación \\(Logit\\left(p\\right)\\) está dado por: \\[ IC\\left[logit\\left(p\\right)\\right] = \\left\\{ ln\\left(\\frac{p}{1-p}\\right)\\pm\\frac{t_{1-\\alpha/2,\\,gl}se\\left(p\\right)}{p\\left(1-p\\right)}\\right\\} \\] Por tanto, el intervalo de confianza para \\(p\\) sería: \\[ IC\\left(p\\right) = \\left\\{ \\frac{exp\\left[ln\\left(\\frac{p}{1-p}\\right)\\pm\\frac{t_{1-\\alpha/2,\\,gl}se\\left(p\\right)}{p\\left(1-p\\right)}\\right]}{1+exp\\left[ln\\left(\\frac{p}{1-p}\\right)\\pm\\frac{t_{1-\\alpha/2,\\,gl}se\\left(p\\right)}{p\\left(1-p\\right)}\\right]}\\right\\} \\] Ahora bien, si se el interés es estimar proporciones para variables multinomiales. El estimador es el siguiente: \\[ p_{k} = \\frac{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i=1}^{n_{h\\alpha}}}\\omega_{h\\alpha i}I\\left(y_{i}=k\\right)}{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i=1}^{n_{h\\alpha}}}\\omega_{h\\alpha i}} = \\frac{\\hat{N}_{k}}{\\hat{N}} \\] A continuación, siguiendo con la base de ejemplo, se estima la proporción de hombres y mujeres en pobreza y no pobreza junto con su error estándar e intervalos de confianza. prop_sexo_zona &lt;- diseno %&gt;% group_by(pobreza,Sex) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() prop_sexo_zona ## pobreza Sex prop prop_se prop_low prop_upp ## 1 0 Female 0.5291800 0.01242026 0.5045356 0.5536829 ## 2 0 Male 0.4708200 0.01242026 0.4463171 0.4954644 ## 3 1 Female 0.5236123 0.01586237 0.4921512 0.5548870 ## 4 1 Male 0.4763877 0.01586237 0.4451130 0.5078488 Como se puede observar, el 52.3% de las mujeres y el 47.6% son pobres. Generando intervalos de confianza al 95% de (49.2%, 55.5%) para las mujeres y (44.5%, 50.7%) para los hombres. En la librería survey existe una alternativa para estimar tablas de contingencias y es utilizando la función svyby como se muestra a continuación: tab_Sex_Pobr &lt;- svyby(formula = ~Sex, by = ~pobreza, design = diseno, FUN = svymean) tab_Sex_Pobr ## pobreza SexFemale SexMale se.SexFemale se.SexMale ## 0 0 0.5291800 0.4708200 0.01242026 0.01242026 ## 1 1 0.5236123 0.4763877 0.01586237 0.01586237 Como se pudo observar, los argumentos que requiere la función son definir la variable a la cual se desea estimar (formula), las categorías por la cual se desea estimar (by), el diseño muestral (desing) y el parámetro que se desea estimar (FUN). Para la estimación de los intervalos de confianza se utiliza la función confint como sigue: Para la estimación de los intervalos de confianza utilizar la función confint. confint(tab_Sex_Pobr) %&gt;% as.data.frame() ## 2.5 % 97.5 % ## 0:SexFemale 0.5048367 0.5535232 ## 1:SexFemale 0.4925226 0.5547019 ## 0:SexMale 0.4464768 0.4951633 ## 1:SexMale 0.4452981 0.5074774 Los cuales coinciden con los generados anteriormente usando la funicón group_by. Otro análisis de interés relacionado con tablas de doble entrada en encuestas de hogares es estimar el porcentaje de desempleados por sexo. tab_Sex_Ocupa &lt;- svyby(formula = ~Sex, by = ~Employment, design = diseno, FUN = svymean) tab_Sex_Ocupa ## Employment SexFemale SexMale se.SexFemale se.SexMale ## Unemployed Unemployed 0.2726730 0.7273270 0.05351318 0.05351318 ## Inactive Inactive 0.7703406 0.2296594 0.02340005 0.02340005 ## Employed Employed 0.4051575 0.5948425 0.01851986 0.01851986 De la anterior salida se puede observar que, el 27.2% de las mujeres y el 72.7% de los hombres están desempleados con errores estándares para estas estimaciones de 5.3% para mujeres y hombres. cuyos intervalos de confianza se calculan a continuación: confint(tab_Sex_Ocupa) %&gt;% as.data.frame() ## 2.5 % 97.5 % ## Unemployed:SexFemale 0.1677891 0.3775570 ## Inactive:SexFemale 0.7244773 0.8162038 ## Employed:SexFemale 0.3688592 0.4414557 ## Unemployed:SexMale 0.6224430 0.8322109 ## Inactive:SexMale 0.1837962 0.2755227 ## Employed:SexMale 0.5585443 0.6311408 Si ahora el objetivo es estimar la pobreza, pero por las distintas regiones que se tienen en la base de datos. Primero, dado que la variable pobreza es de tipo numérica, es necesario convertirla en factor y luego realizar la estimación con la función svyby. tab_region_pobreza &lt;- svyby(formula = ~as.factor(pobreza), by = ~Region, design = diseno, FUN = svymean) tab_region_pobreza ## Region as.factor(pobreza)0 as.factor(pobreza)1 ## Norte Norte 0.6410318 0.3589682 ## Sur Sur 0.6561536 0.3438464 ## Centro Centro 0.6346152 0.3653848 ## Occidente Occidente 0.5991839 0.4008161 ## Oriente Oriente 0.5482079 0.4517921 ## se.as.factor(pobreza)0 se.as.factor(pobreza)1 ## Norte 0.05547660 0.05547660 ## Sur 0.04348901 0.04348901 ## Centro 0.07858599 0.07858599 ## Occidente 0.04670473 0.04670473 ## Oriente 0.08849644 0.08849644 De lo anterior se puede concluir que, en la región Norte, el 35% de las personas están en estado de pobreza mientras que en el sur es el 34%. La pobreza más alta se tiene en la región oriente con un 45% de pobres. Los errores estándares de las estimaciones. Prueba de independencia \\(\\chi^{2}\\) Esta prueba es una de las más utilizadas para determinar si no existe asociación o independencia entre dos variables de tipo cualitativa. En otras palabras, que dos variables sean independientes significa que una no depende de la otra, ni viceversa. A modo de ejemplificar la técnica, para una tabla de \\(2\\times2\\), la prueba \\(\\chi^{2}\\) de personas se define como: \\[ \\chi^{2} = n_{++}\\sum_{r}\\sum_{c}\\frac{\\left(p_{rc}-\\hat{\\pi}_{rc}\\right)^{2}}{\\hat{\\pi}_{rc}} \\] donde, \\(\\hat{\\pi}_{rc}=\\frac{n_{r+}}{n_{++}}\\,\\frac{n_{+c}}{n_{++}}\\,p_{r+}\\,p_{+c}\\). Para realizar la prueba de independencia \\(\\chi^{2}\\) en R, se utilizará la función svychisq del paquete srvyr. Esta función requiere que se definan las variables de interés (formula) y requiere que se le defina el diseño muestral (desing). Ahora, para ejemplificar el uso de esta función tomaremos la base de datos de ejemplo y se probará si la pobreza es independiente del sexo. A continuación, se presentan los códigos computacionales: svychisq(formula = ~Sex + pobreza, design = diseno, statistic=&quot;F&quot;) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 0.056464, ndf = 1, ddf = 119, p-value = 0.8126 Dado que el p-valor es superior al nivel de significancia 5% se puede concluir que, con una confianza del 95% y basado en la muestra, la pobreza no depende del sexo de las personas. En este mismo sentido, si se desea saber si el desempleo está relacionado con el sexo, se realiza la prueba de hipótesis \\(\\chi^{2}\\) como sigue: svychisq(formula = ~Sex + Employment, design = diseno, statistic=&quot;F&quot;) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 62.251, ndf = 1.6865, ddf = 200.6978, p-value &lt; 2.2e-16 Concluyendo que, con una confianza del 95% y basado en la muestra se rechaza la hipótesis nula, es decir, no se puede afirmar que las variables sexo y desempleo sean independiente. Si en el análisis ahora se quiere verificar que la pobreza de las personas es independiente de las regiones establecidas en la base de datos, se realiza de la siguiente manera: svychisq(formula = ~Region + pobreza, design = diseno, statistic=&quot;F&quot;) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 0.48794, ndf = 3.0082, ddf = 357.9731, p-value = 0.6914 Concluyendo que, con una confianza del 95% y basado en la muestra hay independencia entre la pobreza y la región. Lo anterior implica que, no existe relación lineal entre las personas en estado de pobreza por región. Razón de odds Como lo menciona Monroy, L. G. D. (2018) La traducción más aproximada del término odds es “la ventaja”, en términos de probabilidades es la posibilidad de que un evento ocurra con relación a que no ocurra, es decir, es un número que expresa cuánto más probable es que se produzca un evento frente a que no se produzca. También se puede utilizar para cuantificar la asociación entre los niveles de una variable y un factor categórico (Heeringa, S. G. 2017). Suponga que se desea calcular la siguiente razón de odds. \\[ \\frac{\\frac{P(Sex = Female \\mid pobreza = 0 )}{P(Sex = Female \\mid pobreza = 1 )}}{ \\frac{P(Sex = Male \\mid pobreza = 1 )}{P(Sex = Male \\mid pobreza = 0 )} } \\] El procedimiento para realizarlo en R sería, primero estimar las proporciones de la tabla cruzada entre las variables sexo y pobreza: tab_Sex_Pobr &lt;- svymean(x = ~interaction (Sex, pobreza), design = diseno, se=T, na.rm=T, ci=T, keep.vars=T) tab_Sex_Pobr %&gt;% as.data.frame() ## mean SE ## interaction(Sex, pobreza)Female.0 0.3218703 0.01782709 ## interaction(Sex, pobreza)Male.0 0.2863733 0.01768068 ## interaction(Sex, pobreza)Female.1 0.2051285 0.01659697 ## interaction(Sex, pobreza)Male.1 0.1866279 0.01778801 Luego, se realiza el contraste dividiendo cada uno de los elementos de la expresión mostrada anteriormente: svycontrast(stat = tab_Sex_Pobr, contrasts = quote((`interaction(Sex, pobreza)Female.0`/`interaction(Sex, pobreza)Female.1`) /(`interaction(Sex, pobreza)Male.0`/ `interaction(Sex, pobreza)Male.1`))) ## nlcon SE ## contrast 1.0226 0.0961 Obtiendo que, se estima que el odds de las mujeres que no están en estado de pobreza es 1.02 comparandolo con el odds de los hombres. En otras palabras, se estima que las probabilidades de que las mujeres no estén en estado de pobreza sin tener en cuenta ninguna otra variable de la encuesta es cera de 2% mayor que las probabilidades de los hombres. Diferencia de proporciones en tablas de contingencias Como lo menciona Heeringa, S. G. (2017) las estimaciones de las proporciones de las filas en las tablas de doble entrada son, de hecho, estimaciones de subpoblaciones en las que la subpoblación se define por los niveles de la variable factorial. Ahora bien, si el interés se centra en estimar diferencias de las proporciones de las categorías entre dos niveles de una variable factorial, se pueden utilizando contrastes. A manera de ejemplo, se requiere estimar ahora, el contraste de proporciones de mujeres en estado de pobreza versus los hombres en esta misma condición (\\(\\hat{p}_F - \\hat{p}_M\\)). Para ellos, primero, estimemos la proporción de hombres y mujeres en estado de pobreza como se ha mostrado en capítulos anteriores: (tab_sex_pobreza &lt;- svyby(formula = ~pobreza, by = ~Sex, design = diseno , svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## Sex pobreza se ci_l ci_u ## Female Female 0.3892389 0.03159581 0.3273123 0.4511656 ## Male Male 0.3945612 0.03662762 0.3227724 0.4663501 Ahora bien, para calcular la estimación de la diferencia de proporciones junto con sus errores estándares, se realizarán los siguientes pasos: Paso 1: Calcular la diferencia de estimaciones 0.3892 - 0.3946 ## [1] -0.0054 Con la función vcov se obtiene la matriz de covarianzas: library(kableExtra) vcov(tab_sex_pobreza)%&gt;% data.frame() %&gt;% kable(digits = 10, format.args = list(scientific = FALSE)) Female Male Female 0.0009982953 0.0009182927 Male 0.0009182927 0.0013415823 Paso 2: El cálculo del error estándar es: sqrt(0.0009983 + 0.0013416 - 2*0.0009183) ## [1] 0.02243435 Ahora bien, aplicando la función svycontrast se puede obtener la estimación de la diferencia de proporciones anterior: svycontrast(tab_sex_pobreza, list(diff_Sex = c(1, -1))) %&gt;% data.frame() ## contrast diff_Sex ## diff_Sex -0.005322297 0.02243418 De lo que se concluye que, la diferencia entre las proporciones de mujeres y hombres en estado de pobreza es -0.005 (-0.5%) con una desviación estándar de 0.022. Otro ejercicio de interés en un análisis de encuestas de hogares es verificar la diferencia del desempleo por sexo. Al igual que el ejemplo anterior, se inicia con la estimación del porcentaje de desempleados por sexo: tab_sex_desempleo &lt;- svyby(formula = ~desempleo, by = ~Sex, design = diseno %&gt;% filter(!is.na(desempleo)) , FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) tab_sex_desempleo ## Sex desempleo se ci_l ci_u ## Female Female 0.02168620 0.005580042 0.01074952 0.03262288 ## Male Male 0.06782601 0.012161141 0.04399062 0.09166141 Para calcular la estimación de la diferencia de proporciones junto con sus errores estándares, se realizarán los siguientes pasos: Paso 1: Diferencia de las estimaciones 0.02169 - 0.06783 ## [1] -0.04614 Estimación de la matriz de covarianza: vcov(tab_sex_desempleo) %&gt;% data.frame() %&gt;% kable(digits = 10, format.args = list(scientific = FALSE)) Female Male Female 0.0000311369 0.0000208130 Male 0.0000208130 0.0001478933 Paso 2: Estimación del error estándar. sqrt(0.00003114 + 0.00014789 - 2*0.00002081) ## [1] 0.0117222 Siguiendo el ejemplo anterior, utilizando la función svycontrast se tiene que: svycontrast(tab_sex_desempleo, list(diff_Sex = c(-1, 1))) %&gt;% data.frame() ## contrast diff_Sex ## diff_Sex 0.04613982 0.01172195 de lo que se concluye que, la estimación del contraste es 0.04 (4%) con un error estándar de 0.011. Otro ejercicio que se puede realizar en una encuesta de hogares es ahora estimar la proporción de desempleados por región. Para la realización de este ejercicio, se seguirán los pasos de los dos ejemplos anteriores: tab_region_desempleo &lt;- svyby(formula = ~desempleo, by = ~Region, design = diseno %&gt;% filter(!is.na(desempleo)) , FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) tab_region_desempleo ## Region desempleo se ci_l ci_u ## Norte Norte 0.04877722 0.02002293 0.009532997 0.08802144 ## Sur Sur 0.06563877 0.02375124 0.019087202 0.11219034 ## Centro Centro 0.03873259 0.01240317 0.014422832 0.06304235 ## Occidente Occidente 0.03996523 0.01229650 0.015864529 0.06406592 ## Oriente Oriente 0.02950231 0.01256905 0.004867428 0.05413719 Ahora, el interés es realizar los contrastes siguientes para desempleo: \\(\\hat{p}_{Norte} - \\hat{p}_{Centro} = 0.01004\\), \\(\\hat{p}_{Sur} - \\hat{p}_{Centro} = 0.02691\\) \\(\\hat{p}_{Occidente} - \\hat{p}_{Oriente} = 0.01046\\) Escrita de forma matricial sería: \\[ \\left[\\begin{array}{ccccc} 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right] \\] La matriz de varianzas y covarianzas es: vcov(tab_region_desempleo)%&gt;% data.frame() %&gt;% kable(digits = 10, format.args = list(scientific = FALSE)) Por tanto, la varianza estimada está dada por: sqrt(0.0002981 + 0.0002884 - 2*0) ## [1] 0.02421776 sqrt(0.0001968 + 0.0002884 - 2*0) ## [1] 0.02202726 sqrt(0.0001267 + 0.0004093 - 2*0) ## [1] 0.02315167 Usando la función svycontrast, la estimación de los contrastes sería: svycontrast(tab_region_desempleo, list( Norte_sur = c(1, 0, -1, 0, 0), Sur_centro = c(0, 1, -1, 0, 0), Occidente_Oriente = c(0, 0, 0, 1, -1))) %&gt;% data.frame() ## contrast SE ## Norte_sur 0.01004463 0.02355327 ## Sur_centro 0.02690618 0.02679477 ## Occidente_Oriente 0.01046292 0.01758365 Por último, repitiendo el contraste anterior y los pasos para resolverlo, pero ahora utilizando la variable pobreza se tiene: tab_region_pobreza &lt;- svyby(formula = ~pobreza, by = ~Region, design = diseno %&gt;% filter(!is.na(desempleo)) , FUN = svymean, na.rm=T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;)) tab_region_pobreza ## Region pobreza se ci_l ci_u ## Norte Norte 0.3262813 0.04800361 0.2321959 0.4203666 ## Sur Sur 0.2946736 0.04794292 0.2007072 0.3886400 ## Centro Centro 0.3233923 0.07211854 0.1820426 0.4647421 ## Occidente Occidente 0.3673286 0.04400234 0.2810856 0.4535716 ## Oriente Oriente 0.3870632 0.09160150 0.2075276 0.5665989 El interés se centra en realizar los contrastes siguientes para pobreza: \\(\\hat{p}_{Norte} - \\hat{p}_{Centro}\\), \\(\\hat{p}_{Sur}-\\hat{p}_{Centro}\\) \\(\\hat{p}_{Occidente}-\\hat{p}_{Oriente}\\) Escrita de forma matricial es: \\[ \\left[\\begin{array}{ccccc} 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right] \\] Y, utilizando la función svycontrast se obtiene: svycontrast(tab_region_pobreza, list( Norte_sur = c(1, 0, -1, 0, 0), Sur_centro = c(0, 1, -1, 0, 0), Occidente_Oriente = c(0, 0, 0, 1, -1))) %&gt;% data.frame() ## contrast SE ## Norte_sur 0.002888908 0.08663389 ## Sur_centro -0.028718759 0.08660027 ## Occidente_Oriente -0.019734641 0.10162205 "],["modelos-de-regresión-bajo-diseños-de-muestreo-complejos.html", "Capítulo 6 Modelos de regresión bajo diseños de muestreo complejos", " Capítulo 6 Modelos de regresión bajo diseños de muestreo complejos Un modelo matemático es una relación funcional entre variables. El interés consiste en encontrar modelos que relacionen un conjunto de variables de entrada provenientes de censos, registros administrativos, etc. con una variable de salida proveniente de encuestas de hogares. Normalmente en un proceso se tienen varias salidas, pero en este libro se estudia una variable de salida o respuesta del proceso que se asume condicionada a una, o que depende de los valores de una o más variables de entrada. A modo de contexto histórico (Heringa), los primeros autores en discutir, de manera empírica, el impacto que surten los diseños muestrales complejos en las inferencias relacionadas con modelos de regresión fueron Kish y Frankel (1974). Adicional a lo anterior, Fuller (1975) desarrolló un estimador de varianza tomando como insumos teóricos la linealización para modelos de regresión lineal múltiple con ponderación desigual de las observaciones e introdujo estimadores de varianza para parámetros de regresión estimados bajo diseños de muestreo estratificado y de dos etapas. Ahora bien, como es bien sabido, para el uso de la teoría de modelos de regresión se requieren que se cumplan algunos supuestos estadísticos que en ocasiones no se cumplen. En este sentido, Sha et al. (1977) discutieron las violaciones de dichos supuestos y métodos apropiados para hacer inferencias sobre los parámetros estimados de los modelos de regresión lineal usando datos de encuestas, y presentaron una evaluación empírica del desempeño de los estimadores de varianza basados en TSL. En relación con las distribuciones muestrales Binder (1983) se centró en dichas distribuciones muestrales de estimadores para parámetros de regresión en poblaciones finitas y estimadores de varianza relacionados definidos. Skinner et al. (1989) trabajaron estimadores de las varianzas para los coeficientes de regresión que permitieron diseños de muestras complejos y recomendaron el uso de métodos de linealización u otros métodos para la estimación de la varianza. Avanzando un poco en la línea de tiempo, Fuller (2002) generó un resumen de los métodos de estimación para modelos de regresión que contienen información relacionada con muestras complejas. Por último, Pfeffermann (2011) hizo una discusión sobre los distintos enfoques basados en el ajuste de modelos de regresión lineal a datos de encuestas de muestras complejas, presentando apoyo empírico para el uso de un método “q-weighted”. Un modelo de regresión lineal simple se define como \\(y=\\beta_{0}+\\beta_{1}x+\\varepsilon\\) donde \\(y\\) se define como la variable dependiente, \\(x\\) es la variable independiente y \\(\\beta_{0}\\) y \\(\\beta_{1}\\) los parámetros del modelo. La variable \\(\\varepsilon\\) se conoce como el error aleatorio del modelo y se define como \\(\\varepsilon=y-\\hat{y}=y-\\beta_{0}+\\beta_{1}x\\). Generalizando el modelo anterior, se definen los modelos de regresión lineal múltiples como \\[ y = \\boldsymbol{x}\\boldsymbol{\\beta}+\\varepsilon = \\sum_{j=0}^{p}\\beta_{j}x_{j}+\\varepsilon = \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{p}x_{p}+\\varepsilon \\] Donde \\(x_{0}=1\\). Por otro lado, se define como el valor esperado para la variable dependiente condicionado con las variables independientes \\(x\\) como, \\(E\\left(y\\mid x\\right)=\\hat{\\beta}_{0}+\\hat{\\beta_{1}}x_{1}+\\hat{\\beta}_{2}x_{2}+\\cdots+\\hat{\\beta}_{p}x_{p}\\). Otra manera de escribir el modelo de regresión múltiple es: \\[ y_{i} = x_{i}\\boldsymbol{\\beta}+\\varepsilon_{i} \\] donde, \\(x_{i}=\\left[1\\,x_{1i}\\,\\ldots\\,x_{pi}\\right]\\) y \\(\\boldsymbol{\\beta}^{T}=\\left[\\beta_{0}\\,\\,\\beta_{1}\\,\\,\\ldots\\,\\,\\beta_{p}\\right]\\). El subíndice \\(i\\) hace referencia al elemento muestral o respondiente en el conjunto de datos. Algunas consideraciones para los modelos de regresión lineal son tomadas de Heringa y se describen a continuación: \\(E\\left(\\varepsilon_{i}\\mid x_{i}\\right)=0,\\) lo que significa que el valor esperado de los residuos condicionado a un grupo de covariables es igual a 0. \\(Var\\left(\\varepsilon_{i}\\mid x_{i}\\right)=\\sigma_{y,x}^{2}\\) (homogenidad de varianza) lo que significa que, la varianza de los residuos condicionado a un grupo de covariables es igual constante. \\(\\varepsilon_{i}\\mid x_{i}\\sim N\\left(0,\\,\\sigma_{y,x}^{2}\\right)\\) (Normalidad en los errores) lo que significa que, los residuos condicionados a un grupo de covariables se distribuye normal. Esta propiedad también se extiende a la variable respuesta \\(y_{i}\\). \\(cov\\left(\\varepsilon_{i},\\,\\varepsilon_{j}\\mid x_{i},x_{j}\\right)\\) (independencia en los residuales) los residuales en diferentes sujetos no están correlacionados con los valores dados en sus variables predictoras. Una vez definido el modelo de regresión lineal y sus supuestos, se puede deducir los siguiente: \\[ \\hat{y} = E\\left(y\\mid x\\right) = E\\left(\\boldsymbol{x}\\boldsymbol{\\beta}\\right)+E\\left(\\varepsilon\\right) = \\boldsymbol{x}\\boldsymbol{\\beta}+0 = \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{p}x_{p} \\] y Adicionalmente, \\[ var\\left(y_{i}\\mid x_{i}\\right) = \\sigma_{y,x}^{2} \\] \\[ cov\\left(y_{i},y_{j}\\mid x_{i},x_{j}\\right) = 0 \\] \\[ y_{i} \\sim N\\left(x_{i}\\boldsymbol{\\beta},\\sigma_{y,x}^{2}\\right) \\] "],["estimación-de-los-parámetros-en-un-modelo-de-regresión-con-muestras-complejas..html", "6.1 Estimación de los parámetros en un modelo de regresión con muestras complejas.", " 6.1 Estimación de los parámetros en un modelo de regresión con muestras complejas. Una vez se establecen los supuestos del modelo y las características distribucionales de los errores el paso siguientes es el proceso de estimación de los parámetros. A modo ilustrativo e introductorio al proceso de estimación de los parámetros con información provenientes de muestras complejas, si en lugar de observar una muestra de tamaño \\(n\\) de los \\(N\\) elementos de población se hubiera realizado un censo completo, el parámetro de regresión de población finita \\(\\beta_{1}\\) podría calcularse como sigue (Tellez, et. al 2016): \\[ \\beta_{1} = \\frac{{ \\sum_{i=1}^{N}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}}{\\sum_{i=1}^{N}\\left(X_{i}-\\bar{X}\\right)^{2}} \\] Ahora bien, cuando se desea estimar los parámetros de un modelo de regresión lineal, pero considerando que la información muestral proviene de encuestas con muestras complejas se altera el enfoque estándar que se le da a la estimación de coeficientes de regresión y sus errores estándar. La principal razón por la que los métodos de estimación de parámetros de los coeficientes de regresión cambian es que la información recolectada por medio de una encuesta con muestra compleja generalmente no está distribuida de manera idéntica dado que el diseño muestral así es planeado. En este contexto, al ajustar modelos de regresión con este conjunto de datos, dado que los diseños complejos en su mayoría contienen estratificación, conglomerados, probabilidades de selección desiguales, etc, impiden el uso de estimadores de varianza convencionales que se pueden derivar por máxima verosimilitud puesto que, con esta metodología se asumen que los datos son independientes e idénticamente distribuidos y que provienen de alguna distribución de probabilidad (binomial, Poisson, exponencial, normal, etc.). En su lugar, según Wolter, (2007) se emplean métodos no paramétricos robustos basados en linealización de Taylor o métodos de estimación de la varianza usando replicación (Jackknife, bootstrapping, etc). Con el fin de ilustrar la forma cómo se estiman los parámetros de regresión del modelo en el contexto de encuestas complejas se realizará estimando el parámetro \\(\\beta_{1}\\) y su varianza para una regresión lineal simple. La extensión a la estimación de los parámetros de un modelo de regresión múltiple, algebraicamente es compleja y se sale del contexto de este libro. A continuación, se presenta la estimación del intercepto y su varianza en un modelo de regresión lineal simple: \\[ \\hat{\\beta_{1}} = \\frac{{\\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-\\bar{y}_{\\omega}\\right)\\left(x_{h\\alpha i}-\\bar{x}_{\\omega}\\right)}}{{ \\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(x_{h\\alpha i}-\\bar{x}_{\\omega}\\right)^{2}}} = \\frac{t_{xy}}{t_{x^{2}}} \\] Como se puede observar en la ecuación anterior, el estimador del parámetro es un cociente de totales, por ende, su varianza estimada está dada por: \\[ var\\left(\\hat{\\beta_{1}}\\right) = \\frac{var\\left(t_{xy}\\right)+\\hat{\\beta}_{1}^{2}var\\left(t_{x^{2}}\\right)-2\\hat{\\beta}_{1}cov\\left(t_{xy},t_{x^{2}}\\right)}{\\left(t_{x^{2}}\\right)^{2}} \\] A modo de generalización según Kish y Frankel, (1974) para la estimación de la varianza en un modelo de regresión lineal múltiple los métodos de aproximación requieren totales de muestra ponderados para los cuadrados y productos cruzados de todas las combinaciones \\(y\\) y \\(x = {1 x_{1} … x_{p}}\\). A continuación, se presenta la estimación: \\[\\begin{eqnarray*} var\\left(\\hat{\\beta}\\right)=\\hat{\\Sigma}\\left(\\hat{\\beta}\\right) &amp; = &amp; \\left[\\begin{array}{cccc} var\\left(\\hat{\\beta}_{0}\\right) &amp; cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{1}\\right) &amp; \\cdots &amp; cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{p}\\right)\\\\ cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{1}\\right) &amp; var\\left(\\hat{\\beta}_{1}\\right) &amp; \\cdots &amp; cov\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{p}\\right)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{p}\\right) &amp; cov\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{p}\\right) &amp; \\cdots &amp; var\\left(\\hat{\\beta}_{p}\\right) \\end{array}\\right] \\end{eqnarray*}\\] Para ejemplificar los conceptos trabajados hasta este momento, se tomará la misma base que se ha venido trabajando durante todo el desarrollo de este libro. Se inicia con el cargue de las librerías, la base de datos y la definición del diseño de muestreo: knitr::opts_chunk$set(warning = FALSE, message = FALSE, error = FALSE) options(digits = 4) options(tinytex.verbose = TRUE) library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(jtools) library(broom) Cargue de la base y definición del diseño muestral: data(BigCity, package = &quot;TeachingSampling&quot;) library(tidyverse) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) head(encuesta) HHID Stratum NIh nIh dI PersonID PSU Zone Sex Age MaritalST Income Expenditure Employment Poverty dki dk wk Region CatAge idHH00031 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Male 68 Married 409.9 346.3 Employed NotPoor 8 36 34.50 Norte Más de 60 idHH00031 idStrt001 9 2 4.5 idPer02 PSU0003 Rural Female 56 Married 409.9 346.3 Employed NotPoor 8 36 33.64 Norte 46-60 idHH00031 idStrt001 9 2 4.5 idPer03 PSU0003 Rural Female 24 Married 409.9 346.3 Employed NotPoor 8 36 33.64 Norte 16-30 idHH00031 idStrt001 9 2 4.5 idPer04 PSU0003 Rural Male 26 Married 409.9 346.3 Employed NotPoor 8 36 34.50 Norte 16-30 idHH00031 idStrt001 9 2 4.5 idPer05 PSU0003 Rural Female 3 NA 409.9 346.3 NA NotPoor 8 36 33.64 Norte 0-5 idHH00041 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Female 61 Widowed 823.8 392.2 Employed NotPoor 8 36 33.64 Norte Más de 60 library(srvyr) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) Para efectos de los ejemplos y como se ha hecho en anteriores ocasiones, se divide la muestra en sub-grupos de la encuesta como sigue: sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) En este capítulo se ajustarán los modelos de regresión usando la base de datos de ejemplo que se ha venido trabajando en capítulos anteriores. Puesto que, en modelos de regresión, se utiliza muy frecuente el recurso gráfico. A continuación, se define un tema estándar que la CEPAL tiene para generar sus gráficos el cual se utilizará en este capítulo. Para observar que existe una correlación entre el ingreso y el gasto, las cuales son las variables que se utilizarán para el ajuste de los modelos, se construye un scatterplot usando la librería ggplot. Cabe resaltar que, como la base de datos encuesta, la cual se usa para ejemplificar es una muestra de la base BigCity, analizaremos de manera gráfica si poblacionalmente las dos variables mencionadas anteriormente tienen correlación como se muestra a continuación: library(ggplot2); library(ggpmisc) plot_BigCity &lt;- ggplot(data = BigCity, aes(x = Expenditure, y = Income)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x) + theme_cepal() plot_BigCity + stat_poly_eq(formula = y~x, aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;),size = 3), parse = TRUE) Si bien, existen unas observaciones por fuera de la nube de punto, el comportamiento general de la relación ingresos vs gastos mantiene una tendencia lineal. Una vez hecho el análisis gráfico de las variables a utilizar en los modelos a trabajar, se realizará primero un ajuste del modelo con los datos poblacionales y con esto poder analizar qué tan bueno serán los ajustes que se realizarán posteriormente. A continuación, se muestra el ajuste del modelo con los datos poblacionales: fit &lt;- lm(Income ~ Expenditure, data = BigCity) Ahora bien, para observar los parámetros poblacionales del modelo se utilizará la función modelsummary de la librería modelsummary de la siguiente manera: /* tinytable css entries after */ .table td.tinytable_css_f24rei77nuy8uqea4vyy, .table th.tinytable_css_f24rei77nuy8uqea4vyy { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_myfs0e7oykvtunaaftic, .table th.tinytable_css_myfs0e7oykvtunaaftic { text-align: center; border-bottom: solid black 0.05em; } .table td.tinytable_css_7t7396242g7lxu5li2wf, .table th.tinytable_css_7t7396242g7lxu5li2wf { text-align: center; } .table td.tinytable_css_nv9u46smdtlt2lw9ueet, .table th.tinytable_css_nv9u46smdtlt2lw9ueet { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_hkv2t3izmavcsuhe4tdl, .table th.tinytable_css_hkv2t3izmavcsuhe4tdl { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_upu54aai993ebk1mwfgv, .table th.tinytable_css_upu54aai993ebk1mwfgv { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_b6lvve6vdnm6ciwp8ke9, .table th.tinytable_css_b6lvve6vdnm6ciwp8ke9 { text-align: left; } .table td.tinytable_css_dk6e7l75kt3w0mo7ezp2, .table th.tinytable_css_dk6e7l75kt3w0mo7ezp2 { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } Modelo BigCity Pob (Intercept) 123.337 Expenditure 1.229 Num.Obs. 150266 R2 0.359 R2 Adj. 0.359 RMSE 461.74 De la anterior salida se puede observar que, el intercepto es igual a 123.337 y el parámetro \\(\\beta_{1}\\) asociado al gasto es 1.229. La demás información relacionada a esta salida se analizará más adelante. Una vez revisada la información poblacional, se utilizará la información obtenida de la muestra para estimar los parámetros y con ello analizar qué tan buenas son las estimaciones. A continuación, se presenta una sintaxis similar a la anterior que permite construir el scatterplot pero para los datos de la muestra. plot_sin &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Income)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x) + theme_cepal() plot_sin + stat_poly_eq(formula = y~x, aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;), size = 5), parse = TRUE) Como se puede observar, los datos de la muestra tienen una tendencia lineal aunque un poco dispersa a medida que crecen los gastos en las familias. Una vez hecho los análisis gráficos se procede a ajustar los modelos de regresión lineal. A modo de comparar el efecto que tiene hacer un correcto uso de los factores de expansión del diseño, primero, se ajustará un modelo sin tener encuesta dichos factores como se muestra a continuación: fit_sinP &lt;- lm(Income ~ Expenditure, data = encuesta) fit_sinP ## ## Call: ## lm(formula = Income ~ Expenditure, data = encuesta) ## ## Coefficients: ## (Intercept) Expenditure ## 121.52 1.22 stargazer(fit_sinP, header = FALSE, title = &quot;Modelo sin factores de expansion&quot;, style = &quot;ajps&quot;) ## ## \\begin{table}[!htbp] \\centering ## \\caption{Modelo sin factores de expansion} ## \\label{} ## \\begin{tabular}{@{\\extracolsep{5pt}}lc} ## \\\\[-1.8ex]\\hline \\\\[-1.8ex] ## \\\\[-1.8ex] &amp; \\textbf{Income} \\\\ ## \\hline \\\\[-1.8ex] ## Expenditure &amp; 1.220$^{***}$ \\\\ ## &amp; (0.025) \\\\ ## Constant &amp; 121.500$^{***}$ \\\\ ## &amp; (11.410) \\\\ ## N &amp; 2605 \\\\ ## R-squared &amp; 0.487 \\\\ ## Adj. R-squared &amp; 0.487 \\\\ ## Residual Std. Error &amp; 345.000 (df = 2603) \\\\ ## F Statistic &amp; 2473.000$^{***}$ (df = 1; 2603) \\\\ ## \\hline \\\\[-1.8ex] ## \\multicolumn{2}{l}{$^{***}$p $&lt;$ .01; $^{**}$p $&lt;$ .05; $^{*}$p $&lt;$ .1} \\\\ ## \\end{tabular} ## \\end{table} Para el modelo ajustado sin factores de expansión, el \\(\\hat{\\beta}_{0}\\) es 121.52 y el \\(\\hat{\\beta}_{1}\\) asociado a la variable gastos es 1.22. Ahora, haciendo un Scatterplot con los datos encuesta pero utilizando los factores de expansión del diseño se debe agregar mapping = aes(weight = wk) en la función geom_smoothcomo sigue: plot_Ponde &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Income)) + geom_point(aes(size = wk)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x, mapping = aes(weight = wk)) + theme_cepal() plot_Ponde + stat_poly_eq(formula = y~x, aes(weight = wk, label = paste(..eq.label..,..rr.label.., sep = &quot;~~~&quot;)), parse = TRUE,size = 5) En este sentido, para ajustar modelos teniendo en cuenta los factores de expansión existen 2 formas, la primera es usando la función lm y la segunda es usando la función svyglm de la librería survey. A continuación. se ajusta el modelo usando la función lm: fit_Ponde &lt;- lm(Income ~ Expenditure, data = encuesta, weights = wk) fit_Ponde ## ## Call: ## lm(formula = Income ~ Expenditure, data = encuesta, weights = wk) ## ## Coefficients: ## (Intercept) Expenditure ## 103.14 1.26 stargazer(fit_Ponde, header = FALSE, title = &quot;Modelo encuesta ponderada&quot;, style = &quot;ajps&quot;) ## ## \\begin{table}[!htbp] \\centering ## \\caption{Modelo encuesta ponderada} ## \\label{} ## \\begin{tabular}{@{\\extracolsep{5pt}}lc} ## \\\\[-1.8ex]\\hline \\\\[-1.8ex] ## \\\\[-1.8ex] &amp; \\textbf{Income} \\\\ ## \\hline \\\\[-1.8ex] ## Expenditure &amp; 1.263$^{***}$ \\\\ ## &amp; (0.024) \\\\ ## Constant &amp; 103.100$^{***}$ \\\\ ## &amp; (11.260) \\\\ ## N &amp; 2605 \\\\ ## R-squared &amp; 0.509 \\\\ ## Adj. R-squared &amp; 0.509 \\\\ ## Residual Std. Error &amp; 2627.000 (df = 2603) \\\\ ## F Statistic &amp; 2703.000$^{***}$ (df = 1; 2603) \\\\ ## \\hline \\\\[-1.8ex] ## \\multicolumn{2}{l}{$^{***}$p $&lt;$ .01; $^{**}$p $&lt;$ .05; $^{*}$p $&lt;$ .1} \\\\ ## \\end{tabular} ## \\end{table} Para el modelo ajustado con factores de expansión usando la función lm, el \\(\\hat{\\beta}_{0}\\) es 103.14 y el \\(\\hat{\\beta}_{1}\\) asociado a la variable gastos es 1.26. Ahora, haciendo el mismo ajuste pero usando la función svyglm: fit_svy &lt;- svyglm(Income ~ Expenditure, design = diseno, family=stats::gaussian()) fit_svy ## Stratified 1 - level Cluster Sampling design (with replacement) ## With (238) clusters. ## Called via srvyr ## Sampling variables: ## - ids: PSU ## - strata: Stratum ## - weights: wk ## ## Call: svyglm(formula = Income ~ Expenditure, design = diseno, family = stats::gaussian()) ## ## Coefficients: ## (Intercept) Expenditure ## 103.14 1.26 ## ## Degrees of Freedom: 2604 Total (i.e. Null); 118 Residual ## Null Deviance: 6.35e+08 ## Residual Deviance: 3.11e+08 AIC: 38300 Obteniendo estimaciones para el \\(\\hat{\\beta}_{0}\\) es 103.14 y el \\(\\hat{\\beta}_{1}\\) asociado a la variable gastos es 1.26. Siendo exactamente las mismas que con la función lm ya que, como se definió en los argumentos de la función, la función de enlace es Gausiana. Por último y a modo de resumen se muestra un gráfico donde se encuentran depositados todos los modelos estimados anteriormente y así poder comparar de manera gráfica su ajuste: df_model &lt;- data.frame( intercept = c(coefficients(fit)[1], coefficients(fit_sinP)[1], coefficients(fit_Ponde)[1], coefficients(fit_svy)[1]), slope = c(coefficients(fit)[2], coefficients(fit_sinP)[2], coefficients(fit_Ponde)[2], coefficients(fit_svy)[2]), Modelo = c(&quot;Población&quot;, &quot;Sin ponderar&quot;, &quot;Ponderado(lm)&quot;, &quot;Ponderado(svyglm)&quot;)) plot_BigCity + geom_abline( data = df_model, mapping = aes( slope = slope, intercept = intercept, linetype = Modelo, color = Modelo ), size = 2 ) "],["diagnóstico-del-modelo.html", "6.2 Diagnóstico del modelo", " 6.2 Diagnóstico del modelo En el análisis de las encuestas de hogares cuando se ajusten modelo estadístico es importante realizar verificaciones de calidad y con esto tener certezas de las conclusiones que se obtienen. La mayoría de textos académicos dan un panorama bastante detallado de los supuestos y consideraciones que se deben tener en cuenta para tener un modelo correctamente definido. A continuación, se enlistan algunas de ellas (Tellez, 2016) Determinar si el modelo proporciona un adecuado ajuste a los datos. Examinar si los errores están normalmente distribuidos. Examinar si los errores tienen varianza constante. Verificar si los errores se pueden asumir no correlacionados. Determinar si alguno de los datos tiene valores con un efecto inusualmente grande sobre el modelo de regresión estimado, estos se conocen como datos influyentes. Determinar si algún punto no sigue la tendencia de la mayoría de los datos cuando se toma en cuenta el modelo, estos puntos se conocen como outliers. En este capítulo se abordarán alguno de los supuestos que se deben tener en cuenta al momento de ajustar un modelo de regresión lineal. Estimación del \\(R^{2}\\) y \\(R_{adj}^{2}\\) Una medida del ajuste del modelo de regresión es el coeficiente de determinación o coeficiente de correlación múltiple (\\(R^{2})\\)). Dicho parámetro estima la proporción de la varianza de la población explicada por la regresión y oscila entre 0 y 1. Entre más cercano esté de uno significa que mayor variabilidad explica y lo contrario ocurrirá si está cerca de cero. Lo anterior, en ocasiones es muy ambigüo puesto que por ejemplo, los físicos pueden obtienen \\(R^{2}\\) altísimos (mayores a 0.98–0.99) mientras que los químicos obtienen \\(R^{2}\\) superiores a 0.90. Sin embargo, los científicos sociales y demás que trabajen con poblaciones humanas encontrarán que su mejor modelo de regresión a menudo explicará solo entre el 20 % y el 40 % de la variación en la variable dependiente (Heringa). A continuación, se presenta como se calcula este coeficiente: \\[ R^{2} = 1-\\frac{SSE}{SST} \\] Donde, SST es la suma de cuadrados totales y SSE es la suma de cuadrados del error. El estimador de este parámetro usando muestras complejas está dado por: \\[ R_{\\omega}^{2} = 1-\\frac{WSSE}{WSST} \\] Donde, WSST es la variabilidad total estimada y WSSE es la suma de cuadrados estimada y se define como: \\[ \\hat{WSSE_{\\omega}} = \\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-x_{h\\alpha i}\\hat{\\beta}\\right)^{2} \\] Por último, \\(R_{adj}^{2}\\) se estima: \\[ R_{adj}^{2} = 1-\\frac{\\left(n-1\\right)}{\\left(n-p\\right)}R_{\\omega}^{2} \\] Para continuar con los modelos ajustados en la sección anterior, se procede a estimar los \\(R^{2}\\) utilizando R. Inicialmente, se procede a estimar los parámetros del modelo utilizando la función svyglm de survey como se mostró anteriormente y también, se ajusta un modelo solo con el intercepto para obtener la estimación de la SST: fit_svy &lt;- svyglm(Income ~ Expenditure, design = diseno) modNul &lt;- svyglm(Income ~ 1, design = diseno) s1 &lt;- summary(fit_svy) s0 &lt;-summary(modNul) WSST&lt;- s0$dispersion WSSE&lt;- s1$dispersion Por tanto, la estimación del \\(R^{2}\\) es: R2 = 1- WSSE/WSST R2 ## variance SE ## [1,] 0.509 19005 y, para estimar el \\(R_{adj}^{2}\\) se requiere definir el diseño muestral pero incluyendo los q-weigthed (Pffeferman, 2011). A continuación, se muestra los pasos para encontrar los q-weigthed: Ajustar un modelo de regresión a los pesos finales de la encuesta utilizando las variables predictoras en el modelo de regresión de interés. fit_Nul &lt;- lm(wk ~ 1, data = encuesta) Obtener las predicciones de los pesos de la encuesta para cada caso como una función de las variables predictoras en el conjunto de datos qw &lt;- predict(fit_Nul) Dividir los pesos finales de la encuesta por los valores predichos en el paso anterior: encuesta %&lt;&gt;% mutate(wk1 = wk/qw) Usar los nuevos pesos obtenidos para el ajuste de los modelos de regresión: diseno_qwgt &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk1, nest = T) Ahora bien, una vez definido el diseño muestral con los nuevos pesos q-weigthed, se procede a calcular el \\(R_{adj}^{2}\\) como sigue: n = sum(diseno_qwgt$variables$wk) p&lt;- 2 R2Adj = 1-( ( (n-1)/(n-p) )*R2 ) R2Adj ## variance SE ## [1,] 0.491 19005 Como se puede observar, el \\(R_{adj}^{2}\\) es un poco más bajo que el \\(R^{2}\\) y cercanos al 50% que como se comentó anteriormente, dependiendo del contexto del problema se podrá concluir si es grande o pequeño. Después de realizar la comparación entre las diferentes formas de estimar los coeficientes del modelo se opta por la metodología consolidadas en svyglm: diseno_qwgt %&lt;&gt;% mutate(Age2 = Age^2) mod_svy &lt;- svyglm( Income ~ Expenditure + Zone + Sex + Age2 , design = diseno_qwgt) s1 &lt;- summary(mod_svy) s0 &lt;- summary(modNul) mod_svy Stratified 1 - level Cluster Sampling design (with replacement) With (238) clusters. Called via srvyr Sampling variables: - ids: PSU - strata: Stratum - weights: wk1 Call: svyglm(formula = Income ~ Expenditure + Zone + Sex + Age2, design = diseno_qwgt) Coefficients: (Intercept) Expenditure ZoneUrban SexMale Age2 62.18419 1.22548 63.46000 21.73256 0.00852 Degrees of Freedom: 2604 Total (i.e. Null); 115 Residual Null Deviance: 6.35e+08 Residual Deviance: 3.08e+08 AIC: 38300 stargazer(mod_svy, header = FALSE,single.row = T, title = &quot;Modelo propuesto&quot;, style = &quot;ajps&quot;, omit.stat=c(&quot;bic&quot;, &quot;ll&quot;)) Diagnósticos de los residuales En el diagnóstico de los modelos es crucial el análisis de los residuales. Estos análisis proporcionan, bajo el supuesto que el modelo ajustado es adecuado, una estimación de los errores. Por tanto, un estudio cuidadoso de los residuales deberá ayudar al investigador a concluir si el procedimiento de ajuste no ha violado los supuestos o si, por el contrario, uno o varios de los supuestos no se verifican y hay necesidad de revisar el procedimiento de ajuste. Para realizar el análisis de los residuales, en primera instancia, se definen los residuales de Pearson como sigue (Heeringa) \\[ r_{p_{i}} = \\left(y_{i}-\\mu_{i}\\left(\\hat{\\beta}_{\\omega}\\right)\\right)\\sqrt{\\frac{\\omega_{i}}{V\\left(\\hat{\\mu}_{i}\\right)}} \\] Donde, \\(\\mu_{i}\\) es el valor esperado de \\(y_{i}\\), \\(w_{i}\\) es la ponderación de la encuesta para el i-ésimo individuo del diseño muestral complejo, Por último, \\(V(\\mu_{i})\\) es la función de varianza del resultado. Otra definición que se debe tener en consideración para el análisis de los residuales es el de la matriz hat, la cual se estima como: \\[ H = W^{1/2}X\\left(X&#39;WX\\right)^{-1}X&#39;W^{1/2} \\] donde, \\[ W = diag\\left\\{ \\frac{\\omega_{1}}{V\\left(\\mu_{1}\\right)\\left[g&#39;\\left(\\mu_{1}\\right)\\right]^{2}},...,\\frac{\\omega_{n}}{V\\left(\\mu_{n}\\right)\\left[g&#39;\\left(\\mu_{n}\\right)\\right]^{2}}\\right\\} \\] \\(W\\) es una matriz diagonal de \\(n\\times n\\) y \\(g()\\) es la función de enlace del modelo lineal generalizado. Otras técnicas utilizadas también para el análisis de los modelos consisten en el análisis de observaciones influyentes. Una observación se denomina influyente si al removerlo de la nube de puntos este causa un cambio grande en el ajuste del modelo. Una observación importante para resaltar es que un punto influyente podría o no ser un dato atípico. Para detectar observaciones influyentes es necesario tener claro qué tipo de influencia se quiere detectar. Lo anterior puesto que, por ejemplo, una observación puede ser influyente sobre la estimación de los parámetros, pero no para la estimación de la varianza del error. A continuación, se presentan las distintas técnicas estadísticas para la detección de datos influyentes: Distancia de Cook’s: Diagnostica si la i-ésima observación es influyente en la estimación del modelo, por estar lejos del centro de masa de los datos. Se calcula de la siguiente manera: \\[ c_{i}=\\frac{w_{i}^{*}w_{i}e_{i}^{2}}{p\\phi V\\left(\\hat{\\mu}_{i}\\right)\\left(1-h_{ii}\\right)^{2}}\\boldsymbol{x}_{i}^{t}\\left[\\widehat{Var}\\left(U_{w}\\left(\\hat{\\boldsymbol{B}}_{w}\\right)\\right)\\right]^{-1}\\boldsymbol{x}_{i} \\] donde, \\(w_i^* =\\) Pesos de la encuesta. \\(w_i\\) Elementos por fuera de la diagonal de la matriz hat \\(e_i=\\) residuales \\(p=\\) número de parámetros del Modelo de regresión. \\(\\phi =\\) parámetro de dispersión en el glm \\(\\widehat{Var}\\left(U_{w}\\left(\\hat{\\boldsymbol{B}}_{w}\\right)\\right) =\\) estimación de varianza linealizada de la ecuación de puntuación, que se utiliza para pseudo MLE en modelos lineales generalizados ajustados a datos de encuestas de muestras complejas. Luego del cálculo de la distancia de Cook’s para las observaciones de la muestra, se procede a calcular el siguiente estadístico de prueba para evaluar la importancia de la estadística: \\[ \\frac{\\left(df-p+1\\right)\\times c_{i}}{df} \\doteq F_{\\left(p,df-p\\right)} \\] donde \\(df=\\) grados de liberta basados en el diseño. Por otro lado, la literatura como Tellez (2016), Heeringa considera a las observaciones influyentes cuando \\(c_{i}\\) sean mayores a 2 o 3. \\(D_fBeta_{(i)}\\): Este estadístico mide el cambio en la estimación del vector de coeficientes de regresión cuando la i-ésima observación es eliminada. Se evalúa con la siguiente expresión: \\[ D_fBeta_{(i)} = \\hat{B}-\\hat{B}_{\\left(i\\right)}=\\frac{\\boldsymbol{A}^{-1}\\boldsymbol{X}_{\\left(i\\right)}^{t}\\hat{e}_{i}w_{i}}{1-h_{ii}} \\] Donde \\(\\boldsymbol{A} =\\boldsymbol{X}^{t}\\boldsymbol{WX}\\) \\(\\hat{B}_{(i)}\\) es el vector de parámetros estimados una vez se ha eliminado la i-ésima observación, \\(h_{ii}\\) es el correspondiente elemento de la diagonal de H y \\(\\hat{e}_i\\) es el residual de la i-ésima observación. Otra forma de reescribir este estadístico en términos de la matriz \\(H\\) es: \\[ D_fBetas_{\\left(i\\right)}=\\frac{{c_{ji}e_{i}}\\big/{\\left(1-h_{ii}\\right)}}{\\sqrt{v\\left(\\hat{B}_{j}\\right)}} \\] donde: \\(c_{ji}=\\) es el ji-estimo elemento de \\(\\boldsymbol{A}^{-1}w_{i}^{2}\\boldsymbol{X}_{\\left(i\\right)}\\boldsymbol{X}_{\\left(i\\right)}^{t}\\boldsymbol{A}^{-1}\\) El estimador de \\(v\\left(\\hat{B}_{j}\\right)\\) basado en el Modelo se obtiene como: \\(v_{m}\\left(\\hat{B}_{j}\\right)=\\hat{\\sigma}\\sum_{i=1}^{n}c_{ji}^{2}\\) con \\(\\hat{\\sigma}=\\sum_{i\\in s}w_{i}e^2/ \\left( \\hat{N} - p \\right)\\) y \\(\\hat{N} = \\sum_{i \\in s}w_{i}\\) La i-ésima observación es influyente para \\(B_j\\) si \\(\\mid D_{f}Betas_{\\left(i\\right)j}\\mid\\geq\\frac{z}{\\sqrt{n}}\\) con \\(z=\\) 2 o 3 Como alternativa puede usar \\(t_{0.025,n-p}/\\sqrt(n)\\) donde \\(t_{0.025,n-p}\\) es el percentil \\(97.5\\) Estadístico \\(D_{f}Fits_{\\left(i\\right)}\\): Este estadístico mide el cambio en el ajuste del modelo cuando se elimina el registro i-ésimo. Se calcula de la siguiente manera: \\[ D_{f}Fits_{\\left(i\\right)}= \\frac{h_{ii}e_{i}\\big/\\left(1-h_{ii}\\right)}{\\sqrt{v\\left(\\hat{\\beta}_{j}\\right)}} \\] Donde, \\(\\sqrt{v\\left(\\hat{\\beta}_{j}\\right)}\\) puede ser aproximada por el diseño o el Modelo. La i-ésima observación se considera influyente en el ajuste del Modelo si \\(\\mid DfFits\\left(i\\right)\\mid\\geq z\\sqrt{\\frac{p}{n}}\\) con \\(z =\\) 2 o 3 Por otro lado, un análisis que es de vital importancia en el ajuste de modelos de regresión más específicamente en el análisis de residuales es el de varianza constante en los errores. La principal consecuencia de no tener en cuenta la violación de este supuesto es que los estimadores pierden eficiencia. Si el supuesto de varianza constante no se cumple, los estimadores siguen siendo insesgados y consistentes, pero dejan de ser eficientes, es decir, dejan de ser los mejores en cuanto a que ya no tienen la menor varianza entre todos los estimadores insesgados. Como consecuencia de lo anterior, los intervalos de confianza serán más amplios y las pruebas t y F darán resultados imprecisos (Tellez, 2016). Una de las formas de analizar el supuesto de varianzas constantes en los errores es hacerlo de manera gráfica. Para ello, se grafica los residuos del modelo contra \\(\\hat{y}\\) o los residuos del modelo contra \\(X_{i}\\). Si al realizar estos gráficos se logra evidenciar un patrón (funciones cuadráticas, cúbicas, logarítmicas, etc), se puede decir que la varianza de los errores no es constante. Otro supuesto que se debe revisar en los errores al momento de realizar ajustes es la normalidad en lo errores. Una forma muy común para hacer dicha evaluación es realizar un gráfico cuantil-cuantil normal o QQplot. El QQplot es una gráfica de cuantiles para los residuos observados frente a los calculados a partir de una distribución normal teórica que tiene la misma media y varianza que la distribución de los residuos observados. Por lo tanto, una línea recta de 45° en este gráfico sugeriría que la normalidad es una suposición razonable para los errores aleatorios en el modelo. A manera de ejemplificar los conceptos vistos, se van a utilizar los modelos previamente ajustados. En primero instancia, el análisis del modelo se centrará en los supuestos de normalidad y varianza constante en los errores. Primero, se realizará el análisis de la normalidad en los errores de manera gráfica como se muestra a continuación: par(mfrow = c(2,2)) plot(mod_svy) Como se puedo observar en el QQplot, hay evidencia gráfica de que los errores no se distribuyen según una distribución normal. La librería svydiags está pensada en ayudar en el diagnostico de modelos de regresión lineal, siendo una extensión más para complementar el paquete survey. Con las librerías svydiags se extraen los residuales estandarizados como sigue: library(svydiags) stdresids = as.numeric(svystdres(mod_svy)$stdresids) diseno_qwgt$variables %&lt;&gt;% mutate(stdresids = stdresids) Podemos hacer el análisis de normalidad también por medio del histograma de los residuales estandarizados como sigue: ggplot(data = diseno_qwgt$variables, aes(x = stdresids)) + geom_histogram(aes(y = ..density..), colour = &quot;black&quot;, fill = &quot;blue&quot;, alpha = 0.3) + geom_density(size = 2, colour = &quot;blue&quot;) + geom_function(fun = dnorm, colour = &quot;red&quot;, size = 2) + theme_cepal()+labs(y = &quot;&quot;) y como se puede observar gráficamente los errores no siguen una distribución normal. Por otro lado, el otro análisis que se realiza de manera gráfica es el de varianzas constantes el cual se realizará a continuación: Primero, agreguemos las predicciones a la base de datos para poder realizar las gráficas. library(patchwork) diseno_qwgt$variables %&lt;&gt;% mutate(pred = predict(mod_svy)) g2 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Expenditure, y = stdresids))+ geom_point() + geom_hline(yintercept = 0) + theme_cepal() g3 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Age2, y = stdresids))+ geom_point() + geom_hline(yintercept = 0) + theme_cepal() g4 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Zone, y = stdresids))+ geom_point() + geom_hline(yintercept = 0) + theme_cepal() g5 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Sex, y = stdresids))+ geom_point() + geom_hline(yintercept = 0) + theme_cepal() (g2|g3)/(g4|g5) Como se puede observar en las gráficas de gastos y edad, ambas muestran tendencias y no un comportamiento aleatorio. Por lo anterior, se puede decir que las varianzas no son constantes. Otros de os análisis a realizar es revisar si existen datos influyentes en la base de datos. Para ejemplificar los conceptos definidos, se seguirán con los modelos ajustados en la sección anterior. Una vez ajustados estos modelos y verificados los supuestos, se procede a hacer el cálculo de la distancia de Cook’s usando la función svyCooksDdel paquete svydiags como sigue: library(svydiags) d_cook = data.frame( cook = svyCooksD(mod_svy), id = 1:length(svyCooksD(mod_svy))) table(d_cook$cook&gt;3) ggplot(d_cook, aes(y = cook, x = id)) + geom_point() + theme_bw(20) Como se puede observar, ninguna de las distancias de Cook’s es mayor a 3 por lo que, podemos decir que no existen observaciones influyentes. Ahora bien, se desea observar si hay observaciones influyentes pero utilizando \\(D_{f}Betas_{\\left(i\\right)j}\\) se realiza con la función svydfbetas como se muestra a continuación: d_dfbetas = data.frame(t(svydfbetas(mod_svy)$Dfbetas)) colnames(d_dfbetas) &lt;- paste0(&quot;Beta_&quot;, 1:5) d_dfbetas %&gt;% slice(1:10L) Beta_1 Beta_2 Beta_3 Beta_4 Beta_5 0.0006 -2e-04 0.0021 -0.0045 -0.0077 -0.0006 -1e-04 0.0014 0.0026 -0.0031 -0.0009 -1e-04 0.0009 0.0022 0.0008 -0.0004 -1e-04 0.0012 -0.0031 0.0007 -0.0009 0e+00 0.0008 0.0021 0.0014 0.0009 6e-04 -0.0036 -0.0063 0.0098 0.0027 4e-04 -0.0031 -0.0076 -0.0028 0.0011 3e-04 -0.0028 0.0077 -0.0043 0.0030 4e-04 -0.0030 -0.0078 -0.0051 -0.0003 4e-04 0.0012 -0.0037 -0.0040 Una vez calculado los \\(D_{f}Betas_{\\left(i\\right)j}\\) se procede a acomodar la salida con para verificar cuáles observaciones son influyentes. Para esto, de calcula el umbral (cutoff) para definir si es o no influyente la observación. Ese umbral es tomado de las salidas de la función svydfbetas. Por último, se genera una variable dicotómica que indique si la observación es o no influyente como se muestra a continuación: d_dfbetas$id &lt;- 1:nrow(d_dfbetas) d_dfbetas &lt;- reshape2::melt(d_dfbetas, id.vars = &quot;id&quot;) cutoff &lt;- svydfbetas(mod_svy)$cutoff d_dfbetas %&lt;&gt;% mutate( Criterio = ifelse(abs(value) &gt; cutoff, &quot;Si&quot;, &quot;No&quot;)) tex_label &lt;- d_dfbetas %&gt;% filter(Criterio == &quot;Si&quot;) %&gt;% arrange(desc(abs(value))) %&gt;% slice(1:10L) tex_label id variable value Criterio 889 Beta_1 0.2781 Si 890 Beta_2 -0.2593 Si 891 Beta_1 0.2559 Si 889 Beta_2 -0.2537 Si 891 Beta_2 -0.2491 Si 890 Beta_1 0.2456 Si 2311 Beta_5 0.2056 Si 889 Beta_5 -0.1993 Si 890 Beta_5 -0.1788 Si 890 Beta_4 0.1616 Si Como se pudo observar en la salida anterior hay varias observaciones que resultan influyentes dado el criterio del \\(D_{f}Betas_{\\left(i\\right)j}\\). A continuación, y de manera ilustrativa, se grafican los \\(D_{f}Betas_{\\left(i\\right)j}\\) y el umbral con el fin de ver de manera gráfica aquellas observaciones influyentes, teniendo en cuenta que, aquellos puntos rojos en la gráfica representan observaciones influyentes. ggplot(d_dfbetas, aes(y = abs(value), x = id)) + geom_point(aes(col = Criterio)) + geom_text(data = tex_label, angle = 45, vjust = -1, aes(label = id)) + geom_hline(aes(yintercept = cutoff)) + facet_wrap(. ~ variable, nrow = 2) + scale_color_manual( values = c(&quot;Si&quot; = &quot;red&quot;, &quot;No&quot; = &quot;black&quot;)) + theme_cepal() Si el objetivo ahora es detectar observaciones influyentes pero considerando ahora la estadística \\(D_{f}Fits_{\\left(i\\right)}\\), se utiliza la función svydffits y se siguen los mismos pasos mostrados para el estadístico \\(D_{f}Betas_{\\left(i\\right)j}\\): d_dffits = data.frame( dffits = svydffits(mod_svy)$Dffits, id = 1:length(svydffits(mod_svy)$Dffits)) cutoff &lt;- svydffits(mod_svy)$cutoff d_dffits %&lt;&gt;% mutate(C_cutoff = ifelse(abs(dffits) &gt; cutoff, &quot;Si&quot;, &quot;No&quot;)) ggplot(d_dffits, aes(y = abs(dffits), x = id)) + geom_point(aes(col = C_cutoff)) + geom_hline(yintercept = cutoff) + scale_color_manual( values = c(&quot;Si&quot; = &quot;red&quot;, &quot;No&quot; = &quot;black&quot;))+ theme_cepal() Como se puede observar en el gráfico anterior, también hay observaciones influyentes utilizando \\(D_{f}Fits_{\\left(i\\right)}\\), las cuales se muestran en rojo en el gráfico. Un último acercamiento que se trabajará en este texto para la detección de datos influyentes está encaminado al uso de la matriz H. En este sentido, la matriz asociada al Estimador de Pseudo Máxima Verosimilitud (PMLE) de \\(\\hat{\\boldsymbol{B}}\\) es \\(\\boldsymbol{H}=\\boldsymbol{XA}^{-1}\\boldsymbol{X}^{-t}\\boldsymbol{W}\\) cuya diagonal esta dado por \\(h_{ii} = \\boldsymbol{x_{i}^tA}^{-1}\\boldsymbol{x_{i}}^{-t}w_{i}\\). Utilizando la matriz H, una observación puede ser grande y, como resultado, influir en las predicciones, cuando un \\(x_i\\) es considerablemente diferente del promedio ponderado \\(\\bar{x}_w=\\sum_{i\\in s}w_{i}\\boldsymbol{x_{i}}\\big/\\sum_{i\\in s}w_i\\). Según (Tellez, 2016) una observación es considerada grande si es mayor a tres veces el promedio de los \\(h_{ii}\\). A continuación, se muestra el procedimiento en R cuya función a utilizar es svyhat: vec_hat &lt;- svyhat(mod_svy, doplot = FALSE) d_hat = data.frame(hat = vec_hat, id = 1:length(vec_hat)) d_hat %&lt;&gt;% mutate(C_cutoff = ifelse(hat &gt; (3 * mean(hat)),&quot;Si&quot;, &quot;No&quot;)) ggplot(d_hat, aes(y = hat, x = id)) + geom_point(aes(col = C_cutoff)) + geom_hline(yintercept = (3 * mean(d_hat$hat))) + scale_color_manual( values = c(&quot;Si&quot; = &quot;red&quot;, &quot;No&quot; = &quot;black&quot;))+ theme_cepal() Dado que esta última técnica es empírica, se puede observar en el gráfico anterior que hay varias observaciones posiblemente influyentes en el conjunto de datos de la muestra de hogares. "],["inferencia-sobre-los-parámetros-del-modelo.html", "6.3 Inferencia sobre los parámetros del Modelo", " 6.3 Inferencia sobre los parámetros del Modelo Una vez evaluado el correcto ajuste del modelo utilizando las metodologías vistas anteriormente y corroborando las propiedades distribucionales de los errores y por ende, de la variable respuesta \\(y\\), el paso siguiente es verificar si los parámetros estimados son significativos y por ende, las covariables utilizadas para ajustar el modelo aportan significativamente para explicar o predecir a la variable de estudio \\(y\\). Dada las propiedades distribucionales de los \\(\\beta&#39;s\\), un estadístico de prueba natural para evaluar la significancia de dicho parámetro se basa en la distribución “t-student” y se describe a continuación: \\[ t=\\frac{\\hat{\\beta}_{k}-\\beta_{k}}{se\\left(\\hat{\\beta}_{k}\\right)}\\sim t_{n-p} \\] Donde \\(p\\) es el número de parámetros del modelo y \\(n\\) el tamaño de la muestra de la encuesta. En este sentido, el estadístico de prueba anterior evalúa las hipótesis \\(H_{0}:\\beta_{k}=0\\) versus la alternativa \\(H_{1}:\\beta_{k}\\neq0\\). De las propiedades distribucionales de los \\(\\beta\\), se puede construir un intervalo de confianza al \\((1-\\alpha)\\times100\\%\\) para \\(\\beta_{k}\\) está dado por: \\[ \\hat{\\beta}_{k}\\pm t_{1-\\frac{\\alpha}{2},\\,df}\\,se\\left(\\hat{\\beta}_{k}\\right) \\] Donde, los grados de libertad para el intervalo (\\(df\\)) en una encuesta de hogares (muestras complejas) está dado por el número de conglomerados finales de la primera etapa menos el número de estratos de la etapa primaria \\(\\left(df=\\sum_{h}a_{h}-H\\right)\\). Para la aplicación de las temáticas vistas, es decir, realizar la prueba de hipótesis y los intervalos de confianza para los parámetros utilizaremos el modelo que se ha venido trabajando y aplicaremos las funciones summary.svyglm para las pruebas t y confint.svyglm para los intervalos de confianza como sigue: survey:::summary.svyglm(mod_svy) ## ## Call: ## svyglm(formula = Income ~ Expenditure + Zone + Sex + Age2, design = diseno_qwgt) ## ## Survey design: ## Called via srvyr ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 62.18419 62.98928 0.99 0.33 ## Expenditure 1.22548 0.19798 6.19 9.5e-09 *** ## ZoneUrban 63.46000 40.09025 1.58 0.12 ## SexMale 21.73256 15.78081 1.38 0.17 ## Age2 0.00852 0.00557 1.53 0.13 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 118297) ## ## Number of Fisher Scoring iterations: 2 survey:::confint.svyglm(mod_svy) 2.5 % 97.5 % (Intercept) -62.5855 186.9538 Expenditure 0.8333 1.6176 ZoneUrban -15.9511 142.8711 SexMale -9.5262 52.9913 Age2 -0.0025 0.0196 De lo anterior se puede observar que, con una confianza del 95% el único parámetro significativo del modelo es Expenditure y ese mismo resultado lo reflejan los intervalos de confianza. Estimación de una observación Los modelos de regresión lineales, según (Neter et al., 1996)., son utilizado esencialmente con 2 fines, el primero es tratar de explicar la variable respuesta en términos de covariables que pueden encontrarse en la encuesta o en registros administrativos, censos, etc. Adicionalmente, también son usados para predecir valores de la variable en estudio ya sea dentro del intervalo de valores recogidos en la muestra o por fuera de dicho intervalo. Lo primero se ha abordado a lo largo de todo el capítulo y lo segundo se obtiene de la siguiente manera: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=\\boldsymbol{x}_{obs,i}\\hat{\\boldsymbol{\\beta}} \\] De manera explícita, si se ajusta un modelo con 4 covariables la expresión sería: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{1i}+\\hat{\\beta}_{2}x_{2i}+\\hat{\\beta}_{3}x_{3i}+\\hat{\\beta}_{4}x_{4i} \\] La varianza de la estimación se calcula de la siguiente manera: \\[ var\\left(\\hat{E}\\left(y_{i}\\mid x_{obs,i}\\right)\\right) = x&#39;_{obs,i}cov\\left(\\hat{\\beta}\\right)x{}_{obs,i} \\] A continuación, se presenta cómo se realiza la estimación del valor esperado, primero se estiman los parámetros del modelo: term estimate std.error statistic p.value (Intercept) 62.1842 62.9893 0.9872 0.3256 Expenditure 1.2255 0.1980 6.1901 0.0000 ZoneUrban 63.4600 40.0902 1.5829 0.1162 SexMale 21.7326 15.7808 1.3772 0.1711 Age2 0.0085 0.0056 1.5297 0.1288 Por lo anterior, la estimación del valor esperado o predicción queda: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=62.2+1.2x_{1i}+63.5x_{2i}+21.7x_{3i}+0.01x_{4i} \\] Para calcular la varianza de la estimación, primero se deben obtener las varianzas de la estimación de los parámetros: vcov(mod_svy) (Intercept) Expenditure ZoneUrban SexMale Age2 (Intercept) 3967.6496 -11.3101 275.9858 312.4533 -0.1852 Expenditure -11.3101 0.0392 -3.5526 -0.5004 0.0005 ZoneUrban 275.9858 -3.5526 1607.2280 -130.7108 -0.0559 SexMale 312.4533 -0.5004 -130.7108 249.0339 -0.0043 Age2 -0.1852 0.0005 -0.0559 -0.0043 0.0000 Ahora bien, se procede a realizar los cálculos como lo indica la expresión mostrada anteriormente: xobs &lt;- model.matrix(mod_svy) %&gt;% data.frame() %&gt;% slice(1) %&gt;% as.matrix() cov_beta &lt;- vcov(mod_svy) %&gt;% as.matrix() as.numeric(xobs %*% cov_beta %*% t(xobs)) ## [1] 1921 Si el objetivo ahora es calcular el intervalo de confianza para la predicción se utiliza la siguiente ecuación: \\[ \\boldsymbol{x}_{obs,i}\\hat{\\beta}\\pm t_{\\left(1-\\frac{\\alpha}{2},n-p\\right)}\\sqrt{var\\left(\\hat{E}\\left(y_{i}\\mid\\boldsymbol{x}_{obs,i}\\right)\\right)} \\] Para realizar los cálculos en R, se utiliza la función confint y predict como sigue: pred &lt;- data.frame(predict(mod_svy, type = &quot;link&quot;)) pred_IC &lt;- data.frame(confint(predict(mod_svy, type = &quot;link&quot;))) colnames(pred_IC) &lt;- c(&quot;Lim_Inf&quot;, &quot;Lim_Sup&quot;) pred_IC Ahora, de manera gráfica las predicciones e intervalos se vería de la siguiente manera: pred &lt;- bind_cols(pred, pred_IC) pred$Expenditure &lt;- encuesta$Expenditure pred %&gt;% slice(1:6L) pd &lt;- position_dodge(width = 0.2) ggplot(pred %&gt;% slice(1:100L), aes(x = Expenditure , y = link)) + geom_errorbar(aes(ymin = Lim_Inf, ymax = Lim_Sup), width = .1, linetype = 1) + geom_point(size = 2, position = pd) + theme_bw() Por último, si el interés es hacer una predicción fuera del rango de valores que fue capturado en la muestra. Para esto, supongamos que se desea predecir: datos_nuevos &lt;- data.frame(Expenditure = 1600, Age2 = 40^2, Sex = &quot;Male&quot;, Zone = &quot;Urban&quot;) La varianza para la predicción se hace siguiendo la siguiente ecuación: \\[ var\\left(\\hat{E}\\left(y_{i}\\mid\\boldsymbol{x}_{obs,i}\\right)\\right)=\\boldsymbol{x}_{obs,i}^{t}cov\\left(\\boldsymbol{\\beta}\\right)\\boldsymbol{x}_{obs,i} + \\hat{\\sigma}^2_{yx} \\] Por tanto, se construye la matriz de observaciones y se calcula la varianza como sigue: x_noObs = matrix(c(1,1600,1,1,40^2),nrow = 1) as.numeric(sqrt(x_noObs%*%cov_beta%*%t(x_noObs))) ## [1] 244.6 Por último, el intervalo de confianza sigue la siguiente ecuación: \\[ \\boldsymbol{x}_{obs,i}\\hat{\\beta}\\pm t_{\\left(1-\\frac{\\alpha}{2},n-p\\right)}\\sqrt{var\\left(\\hat{E}\\left(y_{i}\\mid\\boldsymbol{x}_{obs,i}\\right)\\right)+\\hat{\\sigma}_{yx}^{2}} \\] En R se hace la predicción de la siguiente manera: predict(mod_svy, newdata = datos_nuevos, type = &quot;link&quot;) ## link SE ## 1 2122 245 y el intervalo: confint(predict(mod_svy,newdata = datos_nuevos)) 2.5 % 97.5 % 1642 2601 "],["gráficas-en-r.html", "Capítulo 7 Gráficas en R", " Capítulo 7 Gráficas en R El objetivo de esta capítulo es mostrarle al lector cómo hacer gráficos generales en R. En todo análisis de encuestas, el componente gráfico es fundamental para revisar tendencias en algunas variables de interés. También son muy necesarias las gráficas cuando se el objetivo es chequear algunos supuestos en el ajustes de modelo, por ejemplo, varianza constante en los errores, normalidad, etc. Uno de los paquetes más usados para graficar en R es ggplot2 el cual es un paquete potente y flexible, implementado por Hadley Wickham, para producir gráficos elegantes. El gg en ggplot2 significa Grammar of Graphics, el cual es un concepto gráfico que describe gráficos usando gramática. Como es de costumbre, se inicia este capítulo cargando las librerías y bases de datos: # knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, error = FALSE) options(digits = 4) library(survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(ggplot2) library(patchwork) El cargue de la base de datos se hace a continuación, data(BigCity, package = &quot;TeachingSampling&quot;) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) A continuación, se define el diseño de muestreo: library(srvyr) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) A partir de las variables de la encuesta, para efectos de los ejemplos, se definen las siguientes variables: diseno &lt;- diseno %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0), edad_18 = case_when( Age &lt; 18 ~ &quot;&lt; 18 años&quot;, TRUE ~ &quot;&gt;= 18 años&quot; ) ) Como se mostró en capítulos anteriores, se divide la muestra en sub grupos para ejemplificar los conceptos que se mostrarán en este capítulo: sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) Para crear las gráficas en este texto se utilizará por defecto el tema que la CEPAL tiene asignado por defecto. El tema se define a continuación: theme_cepal &lt;- function(...) { theme_light(10) + theme( axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), legend.position = &quot;bottom&quot;, legend.justification = &quot;left&quot;, legend.direction = &quot;horizontal&quot;, plot.title = element_text(size = 20, hjust = 0.5), ... ) } "],["histogramas-para-graficar-variables-continuas..html", "7.1 Histogramas para graficar variables continuas.", " 7.1 Histogramas para graficar variables continuas. Un histograma es una representación gráfica de los datos de una variable empleando rectángulos (barras) cuya altura es proporcional a la frecuencia de los valores representados y su ancho proporcional a la amplitud de los intervalos de la clase. Como se mencionó anteriormente, las gráficas se realizarán principalmente con la librería ggplot2y nos apoyamos en la librería patchwork para organizar la visual de las gráficas. A continuacuón, se presenta cómo realizar un histograma para la variable ingresos utilizando los factores de expansión de la encuesta. EN primera instancia se define la fuente de información (data), luego se definen la variable a graficar (x) y los pesos de muestreo (weight). Una vez definido los parámetros generales del gráfico se define el tipo de gráfico, que para nuestro caso como es un histograma es geom_histogram. Se definen los títulos que se quiere que tenga el histograma y por último, se aplica el tema de la CEPAL. plot1_Ponde &lt;- ggplot( data = encuesta, aes(x = Income, weight = wk)) + geom_histogram( aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot1_Ponde De forma análoga se define el siguiente histograma, note que en este caso se omitió el parámetro weight. Es decir, se genera un histograma sin pesos de muestreo: plot1_SinPonde &lt;- ggplot(encuesta, aes(x = Income)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() plot1_SinPonde Ahora bien, para efectos de comparación, se grafica la variable ingreso tomada de la población (BigCity) y se muestran los tres histogramas para notar las diferencias que tienen en comparación con el poblacional. plot1_censo &lt;- ggplot(BigCity, aes(x = Income)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 2500) plot1_censo | plot1_Ponde | plot1_SinPonde Por otro lado, repetimos ahora la secuencia de gráficos pero en este caso para la variable Expenditure: plot2_Ponde &lt;- ggplot( data = encuesta, aes(x = Expenditure, weight = wk) ) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot2_Ponde plot2_SinPonde &lt;- ggplot(data = encuesta, aes(x = Expenditure)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() plot2_SinPonde plot2_censo &lt;- ggplot(BigCity, aes(x = Expenditure)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) plot2_censo | plot2_Ponde | plot2_SinPonde Como conclusión, de ambos ejercicios, se puede observar que el histograma que mejor se aproxima al poblacional es aquel que utiliza los pesos de muestreo, aunque el gráfico que no los utiliza se aproxima bien y esto debido a la correcta selección de la muestra. Por otro lado, cuando el interés ahora es realizar comparaciones entre dos o más agrupaciones, es posible hacer uso del parámetro fill, el cual “rellena” las barras del histograma con diferentes colores según sea el grupo. Para este ejemplo, se van a graficar subgrupos por zonas: plot3_Ponde &lt;- ggplot( encuesta, aes(x = Income, weight = wk)) + geom_histogram( aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot3_Ponde Como se pudo observar en la generación del histograma, se utilizó el parámetro position el cual permite que las barras del gráfico sean distingibles. Ahora se graficará la misma variable pero esta vez sin los pesos de muestreo: plot3_SinPonde &lt;- ggplot(encuesta, aes(x = Income)) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() + ylab(&quot;&quot;) plot3_SinPonde Ahora, siguiendo el esquema de comparación anterior, se graficará la variable ingreso usando la información de la población y los subgrupos de zonas definidos anteriormente y por último, se muestran los 3 histogramas para poder compararlos: plot3_censo &lt;- ggplot(BigCity, aes(x = Income)) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) + ylab(&quot;&quot;) plot3_censo | plot3_Ponde | plot3_SinPonde Ahora, repetimos la secuencia de gráficos anteriores pero, para la variable Expenditure: plot4_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot4_Ponde Sin ponderar, plot4_SinPonde &lt;- ggplot( encuesta, aes(x = Expenditure) ) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() + ylab(&quot;&quot;) plot4_SinPonde Poblacional, plot4_censo &lt;- ggplot(BigCity, aes(x = Expenditure)) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) + ylab(&quot;&quot;) plot4_censo | plot4_Ponde | plot4_SinPonde Ahora, repetimos la secuencia de gráficos para la variable Income, pero hacemos las particiones por la variable sexo, Primero, hagamos el histogramas ponderado: plot5_Ponde &lt;- ggplot( encuesta, aes(x = Income, weight = wk) ) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot5_Ponde Sin ponderar, plot5_SinPonde &lt;- ggplot(encuesta, aes(x = Income)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() + ylab(&quot;&quot;) plot5_SinPonde Poblacional, plot5_censo &lt;- ggplot(BigCity, aes(x = Income)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) + ylab(&quot;&quot;) plot5_censo | plot5_Ponde | plot5_SinPonde Ahora, repetimos la secuencia de gráficos para la variable Expenditure desagregada por la variable sexo, primero, ponderado: plot6_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ylab(&quot;&quot;) + ggtitle(&quot;Ponderado&quot;) + theme_cepal() plot6_Ponde Sin ponderar, plot6_SinPonde &lt;- ggplot(encuesta, aes(x = Expenditure)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Sin ponderar&quot;) + theme_cepal() + ylab(&quot;&quot;) plot6_SinPonde Poblacional, plot6_censo &lt;- ggplot(BigCity, aes(x = Expenditure)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot; ) + ggtitle(&quot;Poblacional&quot;) + theme_cepal() + xlim(0, 1500) + ylab(&quot;&quot;) plot6_censo | plot6_Ponde | plot6_SinPonde "],["agregando-densidades-y-graficando-boxplot.html", "7.2 Agregando densidades y graficando Boxplot", " 7.2 Agregando densidades y graficando Boxplot Dadas las cualidades de la librería ggplot2, se pueden agregar nuevas capas a los gráficos, particularmente, a los histogramas antes realizados. La densidad se agrega con el argumento geom_density y se incorpora el parámetro alpha que regula la transparencia del relleno. A continuacuón, se muestra cómo se agregan las densidades: plot1_Ponde + geom_density(fill = &quot;blue&quot;, alpha = 0.3) | plot2_Ponde + geom_density(fill = &quot;blue&quot;, alpha = 0.3) Ahora bien, al aplicar aes(fill = Zone) permite que la densidad sea agregada para cada una de las agrupaciones como se muestra a continución, plot3_Ponde + geom_density(aes(fill = Zone), alpha = 0.3) | plot4_Ponde + geom_density(aes(fill = Zone), alpha = 0.3) En está oportunidad se agrega la desnidad por sexo, plot5_Ponde + geom_density(aes(fill = Sex), alpha = 0.3) | plot6_Ponde + geom_density(aes(fill = Sex), alpha = 0.3) Boxplot El boxplot, diagrama de caja y bigotes, es un gráfico resumen presentado por John Tukey en 1977 que en la actualidad es ampliamente utilizado en la práctica estadística. En este diagrama se visualiza de forma general un conjunto de datos empleando el resumen de cinco números. La forma generada por este gráfico compuesto por un rectángulo (“caja”) y dos brazos (“bigotes”) suministra información sobre la relación ente los cuartiles (Q1, Q2 o mediana y Q3) y los valores mínimo y máximo, sobre la existencia de valores atípicos y la simetría de la distribución. Para realizar este gráfico en ggplot2 se utiliza la función geom_boxplot. A continuación, se presentan los Boxplot para las variables ingresos y gastos respectivamente: plot7_Ponde &lt;- ggplot( data = encuesta, aes(x = Income, weight = wk) ) + geom_boxplot() + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot8_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_boxplot() + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot7_Ponde | plot8_Ponde En los gráficos anteriores se puede observar que la variable ingresos tiene más variabilidad que la variable gastos. En ambos gráficos se observan datos atípicos. Estos diagramas también permiten la comparación entre dos o más niveles de agrupamiento, por ejemplo, por zonas para las variables ingresos y gastos como se muestra a continuación, plot9_Ponde &lt;- ggplot( encuesta, aes(x = Income, weight = wk) ) + geom_boxplot(aes(fill = Zone)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot10_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_boxplot(aes(fill = Zone)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot9_Ponde | plot10_Ponde Observándose, entre otros que, para la variable gasto en la zona rural es donde más datos atípico hay. Ahora, si se desea personalizar los colores del relleno debe hacer uso de la función scale_fill_manualcomo se muestra a continuación: colorZona &lt;- c(Urban = &quot;#48C9B0&quot;, Rural = &quot;#117864&quot;) plot9_Ponde + scale_fill_manual(values = colorZona) | plot10_Ponde + scale_fill_manual(values = colorZona) Para mayores colores, ver la ayuda de la librería. Ahora, si se desea comparar los ingresos y gastos por sexo se procede de la siguiente manera: plot11_Ponde &lt;- ggplot( data = encuesta, aes(x = Income, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot12_Ponde &lt;- ggplot( encuesta, aes(x = Expenditure, weight = wk) ) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot11_Ponde | plot12_Ponde Definiendo el color del relleno para hombres y mujeres: colorSex &lt;- c(Male = &quot;#5DADE2&quot;, Female = &quot;#2874A6&quot;) plot11_Ponde + scale_fill_manual(values = colorSex) | plot12_Ponde + scale_fill_manual(values = colorSex) Realizando la comparación para más de dos categorías, por ejemplo región, se procede como: plot13_Ponde &lt;- ggplot( data = encuesta, aes(x = Income, weight = wk)) + geom_boxplot(aes(fill = Region)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot14_Ponde &lt;- ggplot( data = encuesta, aes(x = Expenditure, weight = wk)) + geom_boxplot(aes(fill = Region)) + ggtitle(&quot;Ponderado&quot;) + coord_flip() + theme_cepal() plot13_Ponde | plot14_Ponde Personalizando los coles cuando hay más de dos categorías, se realiza como se muestra a continuación: colorRegion &lt;- c( Norte = &quot;#D6EAF8&quot;, Sur = &quot;#85C1E9&quot;, Centro = &quot;#3498DB&quot;, Occidente = &quot;#2E86C1&quot;, Oriente = &quot;#21618C&quot; ) plot13_Ponde + scale_fill_manual(values = colorRegion) | plot14_Ponde + scale_fill_manual(values = colorRegion) La función geom_boxplotpermite realizar comparaciones con más de dos variables al tiempo. A continuación se compara los ingresos por sexo en las diferentes zonas. plot15_Ponde &lt;-ggplot(data = encuesta, aes(x = Income, y = Zone, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + scale_fill_manual(values = colorSex) + coord_flip() plot15_Ponde De forma análoga podemos realizar la comparación de los gastos por sexo en las diferentes zonas: plot16_Ponde &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Zone, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + scale_fill_manual(values = colorSex) + coord_flip() plot15_Ponde / plot16_Ponde Se puede extender las comparaciones a variables que tienen más de dos categorías. plot17_Ponde &lt;- ggplot(data = encuesta, aes(x = Income, y = Region, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + scale_fill_manual(values = colorSex) + coord_flip() plot17_Ponde plot18_Ponde &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Region, weight = wk)) + geom_boxplot(aes(fill = Sex)) + ggtitle(&quot;Ponderado&quot;) + scale_fill_manual(values = colorSex) + coord_flip() plot17_Ponde / plot18_Ponde "],["scaterplot.html", "7.3 Scaterplot", " 7.3 Scaterplot Un diagrama de dispersión o Scaterplot representa cada observación como un punto, posicionado según el valor de dos variables. Además de una posición horizontal y vertical, cada punto también tiene un tamaño, un color y una forma. Estos atributos se denominan estética y son las propiedades que se pueden percibir en el gráfico. Cada estética puede asignarse a una variable o establecerse en un valor constante. Para realizar este tipo de gráfico se usará la función geom_point. Para ejemplificar el uso de esta función, se graficarán las variables ingresos y gastos como se muestra a continuación: plot19_Ponde &lt;- ggplot( data = encuesta, aes( y = Income, x = Expenditure, weight = wk)) + geom_point() + theme_cepal() plot19_Ponde Note, que en este caso el parámetro weight no está aportando información visual al gráfico. El parámetro weight se puede usar para controlar el tamaño de los puntos, y así, tener un mejor panorama del comportamiento de la muestra: plot20_Ponde &lt;- ggplot( data = encuesta, aes(y = Income, x = Expenditure)) + geom_point(aes(size = wk), alpha = 0.3) + theme_cepal() plot20_Ponde Otra forma de usar la variable wk, es asignar la intensidad del color según el valor de la variable: plot21_Ponde &lt;- ggplot( data = encuesta, aes(y = Income, x = Expenditure)) + geom_point(aes(col = wk), alpha = 0.3) + theme_cepal() plot21_Ponde Se puede extender las bondades de los gráfico de ggplot2 para obtener mayor información de las muestra. Por ejemplo, agrupar los datos por Zona. Para lograr esto se introduce el parámetro shape: plot22_Ponde &lt;- ggplot( data = encuesta, aes(y = Income, x = Expenditure, shape = Zone)) + geom_point(aes(size = wk, color = Zone), alpha = 0.3) + labs(size = &quot;Peso&quot;) + scale_color_manual(values = colorZona) + theme_cepal() plot22_Ponde De forma similar se puede obtener el resultado por sexo: plot23_Ponde &lt;- ggplot( data = encuesta, aes( y = Income, x = Expenditure, shape = Sex)) + geom_point(aes( size = wk, color = Sex), alpha = 0.3) + labs(size = &quot;Peso&quot;) + scale_color_manual(values = colorSex) + theme_cepal() plot23_Ponde Un resultado equivalente se obtiene por región: plot24_Ponde &lt;- ggplot( data = encuesta, aes( y = Income, x = Expenditure, shape = Region)) + geom_point(aes( size = wk, color = Region), alpha = 0.3) + labs(size = &quot;Peso&quot;) + scale_color_manual(values = colorRegion) + theme_cepal() plot24_Ponde "],["diagrama-de-barras-para-variables-categoricas.html", "7.4 Diagrama de barras para variables categoricas", " 7.4 Diagrama de barras para variables categoricas Para realizar estos gráfico, en primer lugar, se deben realizar las estimaciones puntuales de los tamaños que se van a graficar: tamano_zona &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise( Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) tamano_zona Zone Nd Nd_se Nd_low Nd_upp Rural 72102 3062 66039 78165 Urban 78164 2847 72526 83802 Ahora, se procede a hacer el gráfico como se mostró en las secciones anteriores: plot25_Ponde &lt;- ggplot( data = tamano_zona, aes( x = Zone, y = Nd, ymax = Nd_upp, ymin = Nd_low, fill = Zone)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(position = position_dodge(width = 0.9), width = 0.3) + theme_bw() plot25_Ponde Como se ha visto en los gráficos anteriores, se pueden extender a variables con muchas más categorías: tamano_pobreza &lt;- diseno %&gt;% group_by(Poverty) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) tamano_pobreza Poverty Nd Nd_se Nd_low Nd_upp NotPoor 91398 4395 82696 100101 Extreme 21519 4949 11719 31319 Relative 37349 3695 30032 44666 El gráfico se obtiene con una sintaxis homologa a la anterior: plot26_Ponde &lt;- ggplot( data = tamano_pobreza, aes( x = Poverty, y = Nd, ymax = Nd_upp, ymin = Nd_low, fill = Poverty)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar( position = position_dodge(width = 0.9), width = 0.3) + theme_bw() plot26_Ponde De forma similar a los gráficos Boxplot, es posible realizar comparaciones entre más dos variables. tamano_ocupacion_pobreza &lt;- diseno %&gt;% group_by(desempleo, Poverty) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% as.data.frame() %&gt;% mutate(desempleo = ifelse(is.na(desempleo),&quot;Ninos&quot;,desempleo)) tamano_ocupacion_pobreza desempleo Poverty Nd Nd_se Nd_low Nd_upp 0 NotPoor 68946 3676.3 61666.8 76226 0 Extreme 11549 2208.8 7175.8 15923 0 Relative 22847 2558.5 17780.5 27913 1 NotPoor 1768 405.4 965.7 2571 1 Extreme 1169 348.1 479.9 1859 1 Relative 1697 457.8 790.7 2604 Ninos NotPoor 20684 1256.6 18195.4 23172 Ninos Extreme 8800 2979.9 2899.7 14701 Ninos Relative 12805 1551.0 9733.9 15876 El gráfico para la tabla anterior queda de la siguiente manera: plot27_Ponde &lt;- ggplot( data = tamano_ocupacion_pobreza, aes( x = Poverty, y = Nd, ymax = Nd_upp, ymin = Nd_low, fill = as.factor(desempleo))) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar( position = position_dodge(width = 0.9), width = 0.3) + theme_bw() plot27_Ponde En estos gráficos también se pueden presentar proporciones, como se muestra a continuación: prop_ZonaH_Pobreza &lt;- sub_Hombre %&gt;% group_by(Zone, Poverty) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) prop_ZonaH_Pobreza ## # A tibble: 6 × 6 ## # Groups: Zone [2] ## Zone Poverty prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural NotPoor 0.549 0.0626 0.424 0.668 ## 2 Rural Extreme 0.198 0.0675 0.0958 0.364 ## 3 Rural Relative 0.254 0.0372 0.187 0.334 ## 4 Urban NotPoor 0.660 0.0366 0.584 0.728 ## 5 Urban Extreme 0.113 0.0245 0.0726 0.171 ## 6 Urban Relative 0.227 0.0260 0.180 0.283 Después de tener la tabla con los valores a presentar en el gráfico, los códigos computacionales para realizar el gráfico es el siguiente: plot28_Ponde &lt;- ggplot( data = prop_ZonaH_Pobreza, aes( x = Poverty, y = prop, ymax = prop_upp, ymin = prop_low, fill = Zone)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar( position = position_dodge(width = 0.9), width = 0.3 ) + scale_fill_manual(values = colorZona) + theme_bw() plot28_Ponde Ahora bien, grafiquemos la proporción de hombres en condición de pobreza por región: prop_RegionH_Pobreza &lt;- sub_Hombre %&gt;% group_by(Region, pobreza) %&gt;% summarise( prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% data.frame() prop_RegionH_Pobreza Region pobreza prop prop_se prop_low prop_upp Norte 0 0.6315 0.0552 0.5171 0.7327 Norte 1 0.3685 0.0552 0.2673 0.4829 Sur 0 0.6134 0.0567 0.4970 0.7181 Sur 1 0.3866 0.0567 0.2819 0.5030 Centro 0 0.6453 0.0846 0.4666 0.7910 Centro 1 0.3547 0.0846 0.2090 0.5334 Occidente 0 0.6259 0.0439 0.5358 0.7080 Occidente 1 0.3741 0.0439 0.2920 0.4642 Oriente 0 0.5450 0.1012 0.3480 0.7289 Oriente 1 0.4550 0.1012 0.2711 0.6520 El gráfico de barras es el siguiente: plot29_Ponde &lt;- ggplot( data = prop_RegionH_Pobreza, aes( x = Region, y = prop, ymax = prop_upp, ymin = prop_low, fill = as.factor(pobreza))) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar( position = position_dodge(width = 0.9), width = 0.3 ) + theme_bw() plot29_Ponde "],["creando-mapas.html", "7.5 Creando mapas", " 7.5 Creando mapas Los mapas son una herramienta gráfica poderosa para la visualización de datos. Particularmente, para indicadores sociales-demográficos estos son una gran referencia visual para desagregaciones a nivel País, región, departamento, provincia, distrito, municipio, comuna, etc. R posee un sin fin de métodos de programación para representar dichos mapas. Para graficar mapas es necesario contar con información geoespacial, datos que contienen las coordenadas o delimitaciones geográficas de determinado país o región. Sitios web como http://www.diva-gis.org/gdata ofrecen de manera gratuita bases de datos o shapes que contienen los vectores asociados a las geografías correspondientes. Dichos conjuntos de datos poseen observaciones sobre la longitud y latitud lo cuál permite graficar en R un conjunto de puntos cuya unión en el gráfico formarán las formas los polígonos que dan forma a las áreas geográficas. Entre las distintas librería para realizar mapas en R están tmap y ggplot2. A continuación, se ilustra cómo se generan mapas, inicalmente con la librería tmap: Inicialmente, para realizar el mapa hay que contar con el archivo de shepefile el cual se carga de la siguiente manera:: library(sf) library(tmap) shapeBigCity &lt;- read_sf(&quot;Data/shapeBigCity/BigCity.shp&quot;) Una vez cargado el shape, el mapa se genera usando las funciones tm_shape y lo que se desea graficar en el mapa se incluye con la función tm_polygons. Para este ejemplo, solo grafiquemos las regiones en el mapa: tm_shape(shapeBigCity) + tm_polygons(col = &quot;Region&quot;) Si ahora el objetivo es graficar en las regiones el procentaje de probreza para hombres, inicialmente se debe agregar esa información a la base de datos con la que se graficará el mapa como sigue: shape_temp &lt;- tm_shape( shapeBigCity %&gt;% # shapefile left_join( # Agregando una variable prop_RegionH_Pobreza %&gt;% filter(pobreza == 1), # Filtrando el nivel de interés. by = &quot;Region&quot;)) Una vez generado la base de datos, se procede a crear el mapa. En este ejemplo, agregarán unos puntos de corte en el mapas que son definidos en el argumento brks como se muestra a continuación: brks &lt;- c(0, .2, .4, .6, 0.8, 1) shape_temp + tm_polygons( col = &quot;prop&quot;, breaks = brks, title = &quot;pobreza&quot;, palette = &quot;YlOrRd&quot;) A modo de otro ejemplo, se desea graficar ahora los coeficientes de variación de las estimaciones de los ingresos medios obtenidas por el diseño a nivel de región: prom_region &lt;- svyby(~Income, ~Region, diseno, svymean, na.rm = T, covmat = TRUE, vartype = c(&quot;cv&quot;)) prom_region Region Income cv Norte Norte 552.4 0.1002 Sur Sur 625.8 0.0997 Centro Centro 650.8 0.0945 Occidente Occidente 517.0 0.0894 Oriente Oriente 541.8 0.1323 brks &lt;- c(0, 0.2, 1) shape_temp &lt;- tm_shape( shapeBigCity %&gt;% left_join( prom_region, by = &quot;Region&quot;)) shape_temp + tm_polygons( &quot;cv&quot;, breaks = brks, title = &quot;cv&quot;, palette = c(&quot;#FFFFFF&quot;, &quot;#000000&quot;), ) + tm_layout(asp = 0) Ahora, realizar el mismo ejercicio anterior pero por zona y sexo: prom_region_Sex &lt;- diseno %&gt;% group_by(Region, Zone, Sex, pobreza) %&gt;% summarise(prop = survey_mean(vartype = &quot;cv&quot;)) %&gt;% filter(pobreza == 1, Zone == &quot;Rural&quot;, Sex == &quot;Female&quot;) shape_temp &lt;- tm_shape( shapeBigCity %&gt;% left_join( prom_region_Sex, by = &quot;Region&quot;)) shape_temp + tm_polygons( &quot;prop&quot;, title = &quot;Pobreza&quot;, ) + tm_layout(asp = 0) Como se comentó en la introducción de esta sección, los gráficos también se pueden hacer usando la librería ggplot2. Esta librería se apoya en las librerías biscale y cowplot. El procedimiento en R para hacer los mapas es muy similar al mostrado con la librería tmap y se realiza de la siguiente manera: library(biscale) library(cowplot) temp_shape &lt;- shapeBigCity %&gt;% left_join( prom_region_Sex, by = &quot;Region&quot;) k &lt;- 3 datos.RM.bi &lt;- bi_class(temp_shape, y = prop, x = prop_cv, dim = k, style = &quot;fisher&quot;) map.RM &lt;- ggplot() + geom_sf( data = datos.RM.bi, aes(fill = bi_class, geometry = geometry), colour = &quot;white&quot;, size = 0.1) + bi_scale_fill(pal = &quot;GrPink&quot;, dim = k) + bi_theme() + theme(legend.position = &quot;none&quot;) map.RM Ahora, para crear la leyenda del mapa se hace de la siguiente manera: legend1 &lt;- bi_legend( pal = &quot;GrPink&quot;, dim = k, xlab = &quot;Coeficiente de variaci&lt;U+00F3&gt;n&quot;, ylab = &quot;Pobreza&quot;, size = 8) mapa1 &lt;- ggdraw() + draw_plot(map.RM, 0, 0, 1, scale = 0.7) + draw_plot(legend1, 0.75, 0.4, 0.2, 0.2, scale = 1) + draw_text(&quot;Estimaciones directas de la pobreza en la mujer rural&quot;, vjust = -13, size = 18) mapa1 "],["modelos-lineales-generalizados-en-encuestas-de-hogares.html", "Capítulo 8 Modelos lineales generalizados en encuestas de hogares", " Capítulo 8 Modelos lineales generalizados en encuestas de hogares Los modelos lineales generalizados (MLGs) proporcionan una aproximación unificada a la mayoría de los procedimientos usados en estadística aplicada. El nombre se debe a que ellos generalizan los modelos lineales basados en el supuesto de distribución normal para la variable respuesta. Al igual que los modelos lineales clásicos, tratados en capítulos anteriores, los MLG tienen aplicación en todas las disciplinas del saber. Nelder &amp; Wedderburn (1972) presentaron por primera vez el término en un artículo que, sin lugar a dudas, es uno de los más importantes publicados en el área de estadística, por su gran impacto en la forma como se aplica esta disciplina. Desde entonces, poco a poco los modelos lineales generalizados se han ido conociendo y usando ampliamente. La genialidad de Nelder &amp; Wedderburn (1972) consistió en darse cuenta (y demostrar) que muchos de los métodos estadísticos ampliamente usados en la época, aparentemente desligados unos de otros, tales como la regresión lineal múltiple, el análisis probit, el análisis de datos provenientes de ensayos de dilución usando la distribución binomial (realizados por Fisher), los modelos logit para proporciones, los modelos log-lineales para conteos, los modelos de regresión para datos de sobrevivencia, entre otros, se podían tratar con un marco teórico unificado y que las estimaciones de máxima verosimilitud para los parámetros de esos modelos podían obtenerse por el mismo algoritmo conocido como mínimos cuadrados ponderados iterativos (MCPI). Los desarrollos teóricos en modelos lineales clásicos parten del supuesto que la variable respuesta tiene distribución normal, cuando un fenómeno en estudio genera datos para los cuales no es razonable la suposición de normalidad, como por ejemplo cuando la respuesta es categórica, una proporción o un conteo, obviamente la respuesta no es normal y no es recomendable analizar los datos suponiendo normalidad. Otro supuesto de los modelos lineales clásicos es el de homogeneidad de la varianza, situación que no se verifica cuando la respuesta es, por ejemplo, una variable aleatoria de poisson, distribución donde la media y la varianza son iguales, es decir, en este modelo un cambio en la media necesariamente implica cambio en la varianza. Los modelos lineales generalizados son excelentes para modelar datos en condiciones de no normalidad y varianza no constante. Específicamente, se debería considerar usar los MLGs cuando la variable respuesta es: conteos expresados como proporciones, conteos que no son proporciones, respuestas binarias, tiempos de sobrevida donde la varianza se incrementa con la media. Sin lugar a dudas, en las encuestas de hogares existen variables de tipo conteo, binomiales, etc que meritan su análisis usando modelos lineales generalizados. Es por esto que, este capítulo es de relevancia en este texto. Para ejemplificar los conceptos, inicialmente se cargan las librerías y la base de datos como sigue: options(digits = 4) options(tinytex.verbose = TRUE) library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(broom) library(jtools) library(modelsummary) library(patchwork) Cargue de las bases de datos, encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) data(&quot;BigCity&quot;, package = &quot;TeachingSampling&quot;) Por último, se define el diseño muestral, diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) Se generan nuevas variables en el diseño para ser utilizadas en los ejemplos, diseno &lt;- diseno %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0)) Como se ha definido en secciones y capítulos anteriores, con variables dicotómicas se pueden generar tablas de frecuencias teniendo en cuenta los factores de expansión del diseño. En R se hace usando la función svyby de la siguiente manera. Primero, se define la variable a la que se le requiere hacer la tabla (formula), luego se le indica cuál es la variable clasificadora (by). En este caso se quiere hacer una tabla de pobreza claificada por sexo. En tercer lugar se define la función que se quiere aplicar (FUN), en este caso, se quieren calcular totales por celda, por último, se define el diseño de muestreo (design) tab_pobreza_sexo &lt;- svyby(formula = ~factor(pobreza), by = ~Sex, FUN = svytotal, design = as.svrepdesign(diseno), se=F, na.rm=T, ci=T, keep.var=TRUE) tab_pobreza_sexo Sex factor(pobreza)0 factor(pobreza)1 se1 se2 Female Female 48366 30824 2411 2916 Male Male 43032 28044 2522 3095 Sin embargo para la estimación de tamaños, se puede emplear también la función svytable como sigue: tab &lt;- svytable(formula = ~pobreza + Sex, design = diseno) kable(tab) Female Male 0 48366 43032 1 30824 28044 Al hacer uso de la función svyby pero usando en el argumento FUN= svymean es posible estimar proporciones como se muestra a continuación: tab_pobreza_sexo &lt;- svyby(formula = ~factor(pobreza), by = ~Sex, FUN = svymean, design = as.svrepdesign(diseno), se=F, na.rm=T, ci=T, keep.var=TRUE) tab_pobreza_sexo Sex factor(pobreza)0 factor(pobreza)1 se1 se2 Female Female 0.6108 0.3892 0.0316 0.0316 Male Male 0.6054 0.3946 0.0366 0.0366 También se pueden hacer tablas de doble entrada para la proporción. En forma alternativa es posible usar la función prop.table del paquete base. kable(prop.table(tab, margin = 2)) Female Male 0 0.6108 0.6054 1 0.3892 0.3946 Estas diferentes formas de proceder son de mucha importancia al momento de hacer uso de pruebas de independencia en tablas cruzadas. "],["prueba-de-independencia-f.html", "8.1 Prueba de independencia F", " 8.1 Prueba de independencia F La prueba de independencia F de Fisher permite analizar si dos variables dicotómicas están asociadas cuando la muestra a estudiar es demasiado pequeña y no se cumplen las condiciones para aplicar la prueba \\(\\chi^{2}\\). Para utilizar esta técnica, tengamos en cuenta que la probabilidad estimada se escribe como: \\[ \\hat{\\pi}_{rc}=\\frac{n_{r+}}{n_{++}}\\times\\frac{n_{+c}}{n_{++}} \\] Teniendo en cuenta esta expresión, la estadística \\(\\chi{2}\\) de Pearson se define de la siguiente manera: \\[ \\chi_{pearsom}^{2}=n_{++}\\times\\sum_{r}\\sum_{c}\\left(\\frac{\\left(p_{rc}-\\hat{\\pi}_{rc}\\right)^{2}}{\\hat{\\pi}_{rc}}\\right) \\] y la estadística de razón de verosimilitud se define como: \\[ G^{2}=2\\times n_{++}\\times\\sum_{r}\\sum_{c}p_{cr}\\times\\ln\\left(\\frac{p_{rc}}{\\hat{\\pi}_{rc}}\\right) \\] donde, \\(r\\) es el número de filas y \\(c\\) representa el número de columnas, la prueba tiene \\((R-1)\\times (C-1)\\) grados de libertad. Como lo menciona Heeringa, Fay (1979, 1985) y Fellegi (1980) fueron de los primeros en proponer la corrección del estadístico chi-cuadrado de Pearson basada en un efecto de diseño generalizado (GDEFF, por sus siglas en inglés). Rao y Scott (1984) y más tarde Thomas y Rao (1987) ampliaron la teoría de las correcciones del efecto de diseño generalizado para estas pruebas estadísticas. El método de Rao-Scott requiere el cálculo de efectos de diseño generalizados que son analíticamente más complicados que el enfoque de Fellegi. Las correcciones de Rao-Scott son ahora el estándar en los procedimientos para el análisis de datos de encuestas categóricas en sistemas de software como Stata y SAS. Los estadísticos de prueba Rao-Scott Pearson ajustados por diseño y razón de verosimilitud chi-cuadrado se calculan de la siguiente manera: \\[ \\chi^2_{(R-S)} = \\chi^2_{(Pearson)}\\big/GDEFF \\] y, para la estadística basada en la razón de verosimilitud se calcula como: \\[ G^2_{(R-S)} = G^2\\big/GDEFF \\] donde el efecto generalizado del diseño (\\(GDEFF\\)) de Rao–Scott, está dado por \\[ GDEFF=\\frac{\\sum_{r}\\sum_{c}\\left(1-p_{rc}\\right)d^{2}\\left(p_{rc}\\right)-\\sum_{r}\\left(1-p_{r+}\\right)d^{2}\\left(p_{r+}\\right)-\\sum_{c}\\left(1-p_{+c}\\right)d^{2}\\left(p_{+c}\\right)}{\\left(R-1\\right)\\left(C-1\\right)} \\] Por tanto, la estadística F para independencia basada en la chi-cuadrado de Pearson se calcula como sigue: \\[ F_{R-S,Pearson}=\\chi_{R-S}^{2}\\big/\\left[\\left(R-1\\right)\\left(C-1\\right)\\right]\\sim F_{\\left(R-1\\right)\\left(C-1\\right),\\left(R-1\\right)\\left(C-1\\right)df} \\] y, la estadística F para independencia basada en la razón de verosimilitudes se calcula como sigue: \\[ F_{R-S,LRT}=G_{R-S}^{2}\\big/\\left(C-1\\right)\\sim F_{\\left(C-1\\right),df} \\] donde \\(C\\) es el número de columnas de la tabla cruzada. En R, el cálculo de las estadísticas chi-cuadrado y F se camculan usando la función summary como se muestra a continuación: summary(tab, statistic = &quot;Chisq&quot;) ## Sex ## pobreza Female Male ## 0 48366 43032 ## 1 30824 28044 ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## X-squared = 0.077, df = 1, p-value = 0.8 Basado en la estadística de Pearson, se puede concluir que el estado de pobreza y el sexo no están relacionados con una confianza del 95%. summary(tab, statistic = &quot;F&quot;) ## Sex ## pobreza Female Male ## 0 48366 43032 ## 1 30824 28044 ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 0.056, ndf = 1, ddf = 119, p-value = 0.8 Resultados similares se obtienen con la prueba F de independencia. "],["estadístico-de-wald.html", "8.2 Estadístico de Wald", " 8.2 Estadístico de Wald Este estadístico se aplica cuando ya se ha elegido un modelo estadístico ( regresión lineal simple, regresión logística, entre otros).El estadístico de prueba de Wald \\(\\chi^{2}\\) para la hipótesis nula de independencia de filas y columnas en una tabla de doble entrada se define de la siguiente manera: \\[ Q_{wald}=\\hat{\\boldsymbol{Y}^{t}}\\left(\\boldsymbol{H}\\hat{\\boldsymbol{V}}\\left(\\hat{\\boldsymbol{N}}\\right)\\boldsymbol{H}^{t}\\right)^{-1}\\hat{\\boldsymbol{Y}} \\] donde, \\[ \\hat{\\boldsymbol{Y}}=\\left(\\hat{N}-E\\right) \\] es un vector de \\(R\\times C\\) de diferencias entre los recuentos de celdas observadas y esperadas, esto es, \\(\\hat{N}_{rc}-E_{rc}\\). La matriz \\(\\boldsymbol{H}\\hat{\\boldsymbol{V}}\\left(\\hat{\\boldsymbol{N}}\\right)\\boldsymbol{H}^{t}\\), representa la matriz de varianza y covarianza estimada para el vector de diferencias. En el caso de un diseño de muestra complejo, la matriz de varianza-covarianza de los conteos de frecuencia ponderada, \\(\\hat{V}\\left(\\hat{N}\\right)\\), se estima utilizando métodos de remuestreo o aproximación de Taylor. La matriz \\(\\boldsymbol{H}\\) es la inversa de la matriz \\(\\boldsymbol{J}\\) dada por: \\[ \\boldsymbol{J}=-\\left[\\frac{\\delta^{2}\\ln PL\\left(\\boldsymbol{B}\\right)}{\\delta^{2}\\boldsymbol{B}}\\right] \\mid \\boldsymbol{B}=\\hat{\\boldsymbol{B}} \\] Bajo la hipótesis nula de independencia, el estadístico de wald se distribuye chi cuadrado con \\(\\left(R-1\\right)\\times\\left(C-1\\right)\\) grados de libertad, \\[ Q_{wald}\\sim\\chi_{\\left(R-1\\right)\\times\\left(C-1\\right)}^{2} \\] La transformación F del estadístico de Wald es: \\[ F_{wald}=Q_{wald}\\times\\frac{df-\\left(R-1\\right)\\left(C-1\\right)+1}{\\left(R-1\\right)\\left(C-1\\right)df}\\sim F_{\\left(R-1\\right)\\left(C-1\\right),df-\\left(R-1\\right)\\left(C-1\\right)+1} \\] En R, para calcular el estadístico de Wald se hace similarmente al cálculo de los estadísticos anteriores usando la función summary como sigue: summary(tab, statistic = &quot;Wald&quot;) ## Sex ## pobreza Female Male ## 0 48366 43032 ## 1 30824 28044 ## ## Design-based Wald test of association ## ## data: NextMethod() ## F = 0.056, ndf = 1, ddf = 119, p-value = 0.8 Se puede concluir que, con una confianza del 95% y basado en la muestra no hay relación entre el estado de pobreza y el sexo. En este mismo sentido, el estadístco de Wald ajustado en R se se calcula similarmente al anterior y los resultados fueron similares: summary(tab, statistic = &quot;adjWald&quot;) ## Sex ## pobreza Female Male ## 0 48366 43032 ## 1 30824 28044 ## ## Design-based Wald test of association ## ## data: NextMethod() ## F = 0.056, ndf = 1, ddf = 119, p-value = 0.8 "],["modelo-log-lineal-para-tablas-de-contingencia.html", "8.3 Modelo log lineal para tablas de contingencia", " 8.3 Modelo log lineal para tablas de contingencia El término modelo log-lineal, que básicamente describe el papel de la función de enlace que se utiliza en los modelos lineales generalizados. Iniciaremos esta sección con los modelos log-lineales en tablas de contingencia. El modelo estadístico es el siguiente: \\[ \\log(p_{ijk}) = \\mu + \\lambda_i^X + \\lambda_j^Y + \\lambda_k^Z + \\lambda_{ij}^{XY} , \\] donde: \\(p_{ijk}=\\) la proporción esperada en la celda bajo el modelo. \\(\\mu = \\log(p_{0})=\\frac{1}{\\#\\ de\\ celdas}\\) El modelo log-lineal en R se ajusta utilizando la función svyloglin como sigue: mod1 &lt;- svyloglin(formula = ~pobreza+Sex + pobreza:Sex, design = diseno) s1 &lt;- summary(mod1) s1 ## Loglinear model: svyloglin(formula = ~pobreza + Sex + pobreza:Sex, design = diseno) ## coef se p ## pobreza1 0.219673 0.06778 0.001192 ## Sex1 0.052843 0.01625 0.001145 ## pobreza1:Sex1 0.005583 0.02350 0.812175 En la salida anterior se puede observar que, con una confianza del 95% el estado de pobreza es independiente del sexo, como se ha mostrado con las pruebas anteriores. Ahora bien, puesto que en la salida anterior se pudo observar que la interacción es no significativa, entonces, ajustemos ahora el modelo sin interacción: mod2 &lt;- svyloglin(formula = ~pobreza+Sex, design = diseno) s2 &lt;- summary(mod2) s2 ## Loglinear model: svyloglin(formula = ~pobreza + Sex, design = diseno) ## coef se p ## pobreza1 0.21997 0.06752 0.0011230 ## Sex1 0.05405 0.01577 0.0006076 Por último, mediante un análisis de varianza es posible comparar los dos modelos como sigue: anova(mod1, mod2) ## Analysis of Deviance Table ## Model 1: y ~ pobreza + Sex ## Model 2: y ~ pobreza + Sex + pobreza:Sex ## Deviance= 0.07719 p= 0.8827 ## Score= 0.07719 p= 0.8827 De la anterior salida se puede concluir que, con una confianza del 95%, la interacción no es significativa en el modelo log-lineal ajustado. Modelo de regresión logistica Un modelo de regresion logística es un modelo matemático que puede ser utilizado para describir la relacion entre un conjunto de variables independientes y una variable dicotomica Y. El modelo logístico se describe a continuación: \\[ g(\\pi(x))=logit(\\pi(x)) \\] De aquí, \\[ z = \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right) = B_0 + B_1x_1+\\dots+B_px_p \\] Por tanto, la probabilidad estimada utilizando el modelo logístico es la siguiente: \\[ \\hat{\\pi}\\left(\\boldsymbol{x}\\right)=\\frac{\\exp\\left(\\boldsymbol{X\\hat{B}}\\right)}{1-\\exp\\left(\\boldsymbol{X\\hat{B}}\\right)}=\\frac{\\exp\\left(\\hat{B}_{0}+\\hat{B}_{1}x_{1}+\\cdots+\\hat{B}x_{p}\\right)}{1-\\exp\\left(\\hat{B}_{0}+\\hat{B}_{1}x_{1}+\\cdots+\\hat{B}x_{p}\\right)} \\] \\[ \\pi\\left(x_{i}\\right)=\\frac{\\exp\\left(x_{i}\\boldsymbol{B}\\right)}{1-\\exp\\left(x_{i}\\boldsymbol{B}\\right)} \\] La varianza de los parámetros estimados se calcula como sigue: \\[ var\\left(\\boldsymbol{\\hat{B}}\\right)=\\boldsymbol{J}^{-1}var\\left(S\\left(\\hat{\\boldsymbol{B}}\\right)\\right)\\boldsymbol{J}^{-1} \\] con, \\[ S\\left(B\\right)=\\sum_{h}\\sum_{a}\\sum_{i}w_{hai}\\boldsymbol{D}_{hai}^{t}\\left[\\left(\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)\\left(1-\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)\\right]^{-1}\\left(y_{hai}-\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)=0 \\] y, \\[ D_{hai} = \\frac{\\delta\\left(\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)}{\\delta B_{j}} \\] donde \\(j=0,\\dots,p\\) Prueba de Wald para los parámetros del modelo Para utilizar el estadístico de Wald en en la significancia de los parámetros del modelo se utiliza la razón de verosimilitud. En este caso se contrastan el modelo con todos los parámetros (modelo full) versus el modelo reducido, es decir, el modelo con menos parámetros (modelo reduced), \\[ G=-2\\ln\\left[\\frac{L\\left(\\hat{\\boldsymbol{\\beta}}_{MLE}\\right)_{reduced}}{L\\left(\\hat{\\boldsymbol{\\beta}}_{MLE}\\right)_{full}}\\right] \\] Dado que el modelo tiene enlace logaritmo, para construir los intervalos de confianza se debe aplicar el función exponencial a cada parámetro, \\[ \\hat{\\psi}=\\exp\\left(\\hat{B}_{1}\\right) \\] por ende, el intervalo de confianza es: \\[ CI\\left(\\psi\\right)=\\exp\\left(\\hat{B}_{j}\\pm t_{df,1-\\frac{\\alpha}{2}}se\\left(\\hat{B}_{j}\\right)\\right) \\] A continuación, se muestra el ajuste de un modelo logístico teniendo e cuenta el diseño muestral: mod_loglin &lt;- svyglm(formula = pobreza ~ Sex + Zone + Region, family = binomial, design=diseno) tidy(mod_loglin) term estimate std.error statistic p.value (Intercept) -0.4082 0.2640 -1.5464 0.1248 SexMale 0.0086 0.0915 0.0945 0.9249 ZoneUrban -0.4378 0.2418 -1.8106 0.0729 RegionSur 0.0063 0.3140 0.0201 0.9840 RegionCentro 0.1915 0.4279 0.4476 0.6553 RegionOccidente 0.2319 0.3144 0.7377 0.4622 RegionOriente 0.3699 0.4259 0.8686 0.3869 La función tidy muestra que ninguna de las covariables son significativas con una confianza del 95%. A continuación, se presentan los intervalos de confianza en los cuales se pueden concluir que en todos los parámetros el cero se encuentra dentro del intrevalo: confint(mod_loglin, level = 0.95) 2.5 % 97.5 % (Intercept) -0.9312 0.1148 SexMale -0.1727 0.1900 ZoneUrban -0.9169 0.0413 RegionSur -0.6159 0.6285 RegionCentro -0.6562 1.0392 RegionOccidente -0.3910 0.8549 RegionOriente -0.4738 1.2136 Para verificar de manera gráfica la distribución de los parámetros del modelo, se realizará un gráfico de estos usando la función plot_summs como se muestra a continuación, library(ggstance) plot_summs(mod_loglin, scale = TRUE, plot.distributions = TRUE) Se puede observar en el gráfico que el número cero se encuentra dentro del intervalo de confianza de cada uno de los parámetros, lo que confirma la no significancia al 95% de los parámetros del modelo. Por otro parte, el estadístico de Wald para el cada una de las variables del modelo se calcula a continuación con la función regTermTest para las variables del modelo: regTermTest(model = mod_loglin, ~Sex) ## Wald test for Sex ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno, ## family = binomial) ## F = 0.00893 on 1 and 113 df: p= 0.92 regTermTest(model = mod_loglin, ~Zone) ## Wald test for Zone ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno, ## family = binomial) ## F = 3.278 on 1 and 113 df: p= 0.073 regTermTest(model = mod_loglin, ~Region) ## Wald test for Region ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno, ## family = binomial) ## F = 0.3654 on 4 and 113 df: p= 0.83 Concluyendo que con una confianza del 95% no son significativas en el modelo como se había mencionado anteriormente. Como es tradicional en el ejuste de modelos de regresión ya sea, clásico o generalizado, se pueden realizar ajustes con interacciones. A continuación, se present cómo se ajustan modelos loglineales con interacción: mod_loglin_int &lt;- svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, family=binomial, design=diseno) tab_mod &lt;- tidy(mod_loglin_int) %&gt;% arrange(p.value) tab_mod term estimate std.error statistic p.value ZoneUrban -0.4248 0.2562 -1.6580 0.1002 (Intercept) -0.4289 0.2849 -1.5055 0.1351 SexMale:RegionSur 0.2871 0.2774 1.0348 0.3031 RegionOriente 0.3843 0.4279 0.8980 0.3712 RegionOccidente 0.3342 0.3783 0.8835 0.3790 SexMale:RegionOccidente -0.2302 0.2868 -0.8026 0.4240 RegionCentro 0.2466 0.4560 0.5408 0.5897 SexMale:RegionCentro -0.1162 0.2791 -0.4162 0.6781 RegionSur -0.1325 0.3464 -0.3825 0.7028 SexMale 0.0478 0.1994 0.2399 0.8109 SexMale:RegionOriente -0.0304 0.2878 -0.1057 0.9161 SexMale:ZoneUrban -0.0154 0.1872 -0.0824 0.9345 Observando que la interacción tampoco es significativa con una confianza del 95%. El gráfico de la distribución de los parámetros del modelo con intercepto y sin intercepto se presenta a continuación: plot_summs(mod_loglin_int, mod_loglin, scale = TRUE, plot.distributions = TRUE) El estadístico de Wald sobre los parámetros del modelo con intercepto son: regTermTest(model = mod_loglin_int, ~Sex) ## Wald test for Sex ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 0.05753 on 1 and 108 df: p= 0.81 regTermTest(model = mod_loglin_int, ~Zone) ## Wald test for Zone ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 2.749 on 1 and 108 df: p= 0.1 regTermTest(model = mod_loglin_int, ~Region) ## Wald test for Region ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 0.8999 on 4 and 108 df: p= 0.47 regTermTest(model = mod_loglin_int, ~Sex:Zone) ## Wald test for Sex:Zone ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 0.006789 on 1 and 108 df: p= 0.93 regTermTest(model = mod_loglin_int, ~Sex:Region) ## Wald test for Sex:Region ## in svyglm(formula = pobreza ~ Sex + Zone + Region + Sex:Zone + Sex:Region, ## design = diseno, family = binomial) ## F = 1.058 on 4 and 108 df: p= 0.38 Observándose que con una confianza del 95% ninguno de los parámetros del modelo es significativo. Ahora bien, como se ha explicado a los largo de los capítulos relacionado con modelos, se pueden ajustar modelos usando Q_Weighting. A continuación, se presenta cómo se ajusta el modelo usando estos pesos: fit_wgt &lt;- lm(wk ~ Sex + Zone + Region , data = encuesta) wgt_hat &lt;- predict(fit_wgt) encuesta %&lt;&gt;% mutate(wk2 = wk/wgt_hat) diseno_qwgt &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk2, nest = T ) Defiendo la variable pobreza dentro de la base de datos. diseno_qwgt &lt;- diseno_qwgt %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0)) Ajustando el modelo se tiene: mod_loglin_qwgt &lt;- svyglm(formula = pobreza ~ Sex + Zone + Region, family=quasibinomial, design=diseno_qwgt) tab_mod &lt;- tidy(mod_loglin_qwgt) tab_mod term estimate std.error statistic p.value (Intercept) -0.4644 0.2630 -1.7656 0.0802 SexMale 0.0241 0.0883 0.2726 0.7857 ZoneUrban -0.3445 0.2311 -1.4903 0.1389 RegionSur -0.0041 0.3116 -0.0130 0.9896 RegionCentro 0.1613 0.4270 0.3778 0.7063 RegionOccidente 0.2424 0.3147 0.7705 0.4426 RegionOriente 0.3937 0.4319 0.9115 0.3639 Concluyendo que, con una confianza del 95% y basado en la muestra, ninguno de los parámetros del modelo es significativo. Lo cual se puede corroborar con el gráfico de la distribución de los parámetros del modelo con los pesos ajustados y sin los pesos: plot_summs(mod_loglin, mod_loglin_qwgt, scale = TRUE, plot.distributions = TRUE) El Estadístico de Wald sobre los parámetros del modelo se obtiene de manera similar a lo visto anteriormente: regTermTest(model = mod_loglin_qwgt, ~Sex) ## Wald test for Sex ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno_qwgt, ## family = quasibinomial) ## F = 0.0743 on 1 and 113 df: p= 0.79 regTermTest(model = mod_loglin_qwgt, ~Zone) ## Wald test for Zone ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno_qwgt, ## family = quasibinomial) ## F = 2.221 on 1 and 113 df: p= 0.14 regTermTest(model = mod_loglin_qwgt, ~Region) ## Wald test for Region ## in svyglm(formula = pobreza ~ Sex + Zone + Region, design = diseno_qwgt, ## family = quasibinomial) ## F = 0.4156 on 4 and 113 df: p= 0.8 Concluyendo también que ninguna de las variables son significativas. "],["modelos-lineales-generalizados-introducción-a-la-pseudo-máxima-verosimilitud.html", "Capítulo 9 Modelos lineales generalizados: Introducción a la pseudo máxima verosimilitud", " Capítulo 9 Modelos lineales generalizados: Introducción a la pseudo máxima verosimilitud En la introducción de su excelente libro, Statistical Design for Researches, Leslie Kish afirma que el enunciado de la mayoría de libros de inferencia estadística abren con el siguiente enunciado: Dadas \\(n\\) variables aleatorias, seleccionadas de una población, independientes e idénticamente distribuidas y que cada palabra en el anterior enunciado es engañosa. ¿Quién le da a uno las muestras? ¿Existe algún sitio en dónde las repartan? Las muestras no son dadas, las muestras deben ser seleccionadas, asignadas o capturadas. El tamaño de la muestra no siempre es un número \\(n\\) fijo, en la mayoría de casos prácticos es una variable aleatoria. Los datos no siguen el supuesto de independencia ni de idéntica distribución; es más, en muchas ocasiones no existe una sola población, sino que la muestra seleccionada es el resultado de una selección de sub-poblaciones para las cuales se deben producir, no solo una sino muchas estimaciones. En la teoría de muestreo, se considera que las características de interés son parámetros y no constituyen realizaciones de variables aleatorias. Para reforzar esta idea haga lo siguiente: examine una moneda y obsérvela. Suponga que usted está observando la cara (o sello, da igual) de la moneda. Esa cara (o sello) no constituye una realización de una variable aleatoria. Para que se pueda hablar de una variable aleatoria, es necesario realizar un experimento, el cual induce el conjunto de todos los posibles resultados, el cual a su vez induce una sigma-álgebra que define a la variable aleatoria. Sería muy diferente si se crease un experimento con esa moneda. El más sencillo de todos sería lanzarla al aire y observar si la moneda cayó en cara o sello. De forma similar, es muy válido afirmar que, por ejemplo, el estado de la naturaleza de un individuo que está desempleado no constituye una realización de una variable aleatoria. Un ejemplo práctico se presenta a la hora de estimar la tasa de desempleo, se considera que, si un individuo está desempleado, pues está desempleado y punto. En otras palabras, el estado de la naturaleza del individuo al momento de la medición es “desempleado” y esta caracterización no corresponde a ninguna realización de algún evento aleatorio. Es por esto que, una vertiente de la inferencia en poblaciones finitas considera que el parámetro de interés será el número total de personas desempleadas dividido por el número total de personas en la fuerza laboral. Si se tuviese la oportunidad de medir a todos los integrantes de la fuerza laboral, mediante la realización de un censo, pues esa división correspondería al parámetro poblacional con el cual se tomarían decisiones y/o se cambiarían o reforzarían las políticas públicas de un país. El propósito de este capítulo es llevar a los lectores al correcto análisis de sus datos, preguntándose acerca del proceso de selección de la muestra. Más aún, en términos de muestreo, solo hay un único caso para el cual la teoría de la inferencia estadística es aplicable y se trata del muestreo aleatorio simple con reemplazo en donde si se tienen las propiedades de independencia y de idéntica distribución. Note que, en términos de selección de muestras, solo hay dos posibles escenarios generales. La selección con reemplazo y la selección sin reemplazo. "],["acerca-de-las-muestras-aleatorias-y-su-análisis.html", "9.1 Acerca de las muestras aleatorias y su análisis", " 9.1 Acerca de las muestras aleatorias y su análisis Hablemos primero de la selección sin reemplazo, en donde una muestra seleccionada está conformada por algunos elementos de la población que no se repiten. Para seleccionar una muestra sin reemplazo de tamaño \\(n=3\\), de una población de tamaño \\(N=5\\), el proceso de selección puede ser de la siguiente manera. Se escoge una unidad de las cinco posibles, luego se selecciona una unidad de las cuatro restantes, y por último, una unidad de las tres restantes. Esto hace que el proceso de selección de la muestra no se lleve a cabo de forma independiente. Por ejemplo, si el muestreo es aleatorio simple, la probabilidad de selección de la primera unidad es 1/5, la probabilidad de selección de la segunda unidad es 1/4 y así sucesivamente. Por otro lado, cuando el muestreo es con reemplazo, la selección se realiza de forma independiente puesto que se trata de realizar el mismo ensayo (seleccionar una unidad de cinco posibles) tres veces, sin importar que las unidades tengan diferentes probabilidades de selección. Por otra parte, es bien sabido que la teoría de muestreo establece que el valor de la característica de interés, \\(y_k\\), es eso, un valor; por tanto, no es aleatorio. Luego, es incorrecto decir que \\(y_k\\) es una variable aleatoria asociada con alguna distribución de probabilidad. Recuerde que en el muestreo lo único aleatorio en la inferencia es la muestra. Ahora, no significa que no podamos construir variables aleatorias en muestreo. Por ejemplo, construyamos la siguiente variable aleatoria \\(X_i\\) (\\(i=1,2,3\\)) definida como el valor de la característica de interés en el individuo \\(k\\)-ésimo, seleccionado en la \\(i\\)-ésima extracción. En este caso, existen tres variables aleatorias, puesto que la muestra es de tamaño tres. Si consideramos un muestreo aleatorio sin reemplazo, la primera variable aleatoria \\(X_1\\), podrá tomar cualquiera de los siguiente cinco valores: \\(y_1, y_2, y_3, y_4, y_5\\). La segunda variable aleatoria \\(X_2\\), solo podrá tomar cuatro valores, puesto que \\(X_1\\) ya fue realizada, y la tercera variable aleatoria \\(X_3\\) solo podrá tomar tres valores, puesto que \\(X_1\\) y \\(X_2\\) ya fueron realizadas. Esto hace que \\(X_1\\), \\(X_2\\) y \\(X_3\\) no constituya una sucesión de variables aleatorias independientes (puesto que la selección sin reemplazo no es un proceso independiente) ni idénticamente distribuidas (puesto que ni siquiera su espacio muestral es el mismo: \\(X_1\\) puede tomar cinco valores, \\(X_2\\) solo cuatro y \\(X_3\\) solo tres). Lo cual quiere decir que a partir de un muestreo sin reemplazo (ni siquiera el tan mencionado muestreo aleatorio simple) no es posible construir una muestra aleatoria, como las que aparecen en los libros de teoría estadística. Sin embargo, algo muy distinto sucede con el muestreo con reemplazo. Cuando construimos las variables aleatorias \\(X_1\\), \\(X_2\\) y \\(X_3\\), resulta ser que ellas sí conforman una sucesión de variables aleatorias independientes (puesto que el muestreo con reemplazo sí define un proceso de extracciones independientes) e idénticamente distribuidas (puesto que conservan el mismo espacio muestral y mantienen la probabilidad de selección). Es decir, \\(X_1\\) puede tomar los valores \\(y_1, \\ldots, y_5\\). La probabilidad de que \\(X_1=y_1\\) es \\(p_1\\), la probabilidad de selección del primer elemento; la probabilidad de que \\(X_1=y_2\\) es \\(p_2\\), la probabilidad de selección del segundo elemento y así sucesivamente hasta obtener que la probabilidad de que \\(X_1=y_5\\) es \\(p_5\\), la probabilidad de selección del primer elemento primer elemento. La misma distribución la tienen \\(X_2\\) y \\(X_3\\). Por lo tanto, \\(X_1\\), \\(X_2\\) y \\(X_3\\) conforman una muestra aleatoria, como las que aparecen en los libros clásicos de inferencia estadística. Entonces, hemos llegado a un punto sin retorno, en donde la conclusión es que, si la muestra fue seleccionada con reemplazo, entonces podemos inducir una muestra aleatoria. Sin embargo, existen muchas variantes en el muestreo con reemplazo. A continuación, vamos a dilucidar cuál de ellas es la indicada para analizar la muestra de acuerdo con la teoría de los libros de inferencia. En primera instancia, veamos que para que la esperanza (bajo el diseño de muestreo \\(p\\)) de cualquier variable aleatoria \\(X_i\\) sea igual a la media poblacional, es necesario que, para todos los individuos en la población, la probabilidad de selección sea idéntica e igual a \\(1/N\\), como se muestra a continuación: \\[ E_p(X_i)=\\sum_{k \\in U} y_k Pr(X_i = Y_k) = \\sum_{k \\in U} y_k p_k = \\frac{t_y}{N} = \\bar{y}_U=\\mu_N \\] De la misma manera, para que la varianza de cualquier variable aleatoria \\(X_i\\) sea igual a la varianza poblacional, se requiere la misma condición, puesto que: \\[ Var_p(X_i) = \\sum_{k \\in U} (y_k - \\bar{y}_U)^2 p_k = \\frac{1}{N}\\sum_{k \\in U} (y_k - \\bar{y}_U)^2 = S^2_{y_U} = \\sigma^2_N \\] Por lo tanto, la esperanza y la varianza de un estimador clásico como \\(\\bar{X}\\) solo coincidierón con los bien conocidos resultados de la inferencia clásica cuando el muestreo haya sido aleatorio simple con reemplazo. De otra forma, no se tienen las, bien conocidas, propiedades de esta estadística que implican que su esperanza es \\(E(\\bar{X}) = \\mu_N\\) y su varianza es \\(Var(\\bar{X}) = \\frac{\\sigma^2_N}{n}\\). Este razonamiento de aplicarse de la misma forma para pruebas de hipótesis, construcción de intervalos de confianza, modelos de regresión, y hasta diseño de experimentos. Ahora, para una encuesta cuyos datos no fueron extraídos de manera aleatoria simple con reemplazo, la manera correcta de analizarla confiadamente es incluir los pesos de muestreo en todas las técnicas y metodologías estadísticas, ya sean regresiones simples y logísticas o simples varianzas del promedio. Modelos de superpoblación Suponga que la estimación de máxima verosimilitud es apropiada para muestras aleatorias simples. Por ejemplo, modelos de regresión simple, múltiple, regresión logística, entre otros. Bajo este esquema, se asume que la función de densidad poblacional es \\(f(y | \\theta)\\) donde \\(\\theta\\) es el parámetro de interés. Con una réplica del ejemplo que David Binder utiliza en un artículo del año 2011 (una excelente lectura para quienes ha seguido el trabajo de Ken Brewer), se introducen algunos conceptos que son de utilidad. Finalmente, todos los resultados se van a plasmar en simulaciones de Monte Carlo, algunas veces anidadas. Suponga que se generaron \\(N=100\\) realizaciones de variables aleatorias independientes distribuidas Bernoulli con parámetro de interés \\(\\theta=0.3\\). Los datos que se obtienen se muestran a continuación: 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 En esta población finita, que fue generada a partir de un modelo probabilístico (llamado modelo de superpoblación), hay 28 éxitos. Primer proceso inferencial: el modelo En este apartado, es notable que la medida de probabilidad que rige la inferencia hasta el momento sea la inducida por la distribución binomial con parámetro 0.3. De esta manera, el estimador insesgado de mínima varianza (todas estas propiedades obtenidas con base en la distribución binomial) está dado por el promedio poblacional. Nótese que la inferencia utiliza todos los datos de la población. Ahora, para reproducirlo computacionalmente, basta con simular muchas poblaciones de 100 variables aleatorias independientes distribuidas Bernoulli con parámetro desconocido \\(\\theta\\)=0.3. Como es bien sabido, bajo la perspectiva de los modelos poblacionales y la inferencia estadística clásica, el estimador \\(\\bar{y}_U = \\frac{\\sum_U y_k}{N}\\) es insesgado. Para corroborarlo, es posible introducir la siguiente simulación de Monte Carlo. N = 100 theta = 0.3 nsim1 = 1000 Est0=rep(NA,nsim1) for(i in 1:nsim1){ y=rbinom(N, 1, theta) Est0[i]=mean(y) } Esp0 = mean(Est0) cbind(theta, Esp0) theta Esp0 0.3 0.30116 Segundo proceso inferencial: el muestreo En el primer proceso inferencial, se asume que las variables de estudio son realizaciones de variables aleatorias gobernadas por un modelo probabilístico. Sin embargo, un razonamiento muy válido es que en cualquier población finita en particular, los valores de la medición son fijos aunque desconocidos y no siguen ningún modelo probabilístico; es decir, no corresponden a realizaciones de variables aleatorias. Por ejemplo, suponga que para esa misma población del ejemplo anterior el dato uno corresponde a un individuo desempleado y el dato cero corresponde a un individuo empleado. Por otra parte, asuma que la población está subdividida en conglomerados, que pueden ser llamados hogares. De esta forma, nuestra población finita toma la siguiente caracterización, mediante una partición de \\(N_{I}=27\\) hogares: (1 1 0) (1 0) (0 0 0 0 0 0 1) (1 0) (0 0 0 0 0 0 1) (0 0 1) (0 0 0 0 0 0 0 1) (0 0 1) (0 0 0 1) (0 0 0 0 1) (0 0 0 0 0 0 0 1) (1 0) (1 0) (0 0 1) (1 0) (0 0 1) (1 0) (0 1) (0 0 0 1) (0 0 1) (1 1 0) (0 0 0 0 1) (0 1) (0 1) (0 0 0 0 0 0 0 0 0 1) (0 1) (0) El proceso de aglomeración en hogares es obviamente artificioso en este ejemplo, pero ilustra que en la vida real las poblaciones finitas siempre están aglomeradas. Suponga por otra parte que tomamos una muestra \\(S_{I}\\) de \\(n_{I}\\) hogares y en cada hogar seleccionado realizamos un censo; además la selección de los hogares se hará aleatoriamente, sin reemplazo y con probabilidades de inclusión \\(\\pi_{Ii}\\) proporcionales al tamaño del hogar \\(N_{i}\\). Siendo la característica de interés \\(y_{k}\\), el estado del individuo en la fuerza laboral (1, si está desempleado y 0, en otro caso); entonces es bien sabido que bajo este esquema de muestreo un estimador insesgado para la proporción de desempleados \\(\\bar{y}_{U}\\) es el siguiente: \\[ \\bar{y}_{\\pi S}=\\sum_{i\\in S_{I}}\\frac{t_{y_{i}}}{\\pi_{Ii}}=\\frac{\\sum_{i\\in S_{I}}\\bar{y}_{i}}{n_{I}} \\] En donde \\(\\bar{y}_{i}=\\frac{t_{y_{i}}}{N_{i}}\\) es la proporción de desempleados en el hogar \\(i\\)-ésimo, \\(t_{y_{i}}\\) es el total de desempleados en el hogar \\(i\\)-ésimo, \\(N_{i}\\) es el número de individuos en el hogar y \\(n_{I}\\) es el número de hogares seleccionados. Por otro lado, un estimador ingenuo, correspondiente a la proporción de desempleados en la muestra, que asume que el agrupamiento de los valores no interfiere en el proceso de inferencia e ignora el diseño de muestreo es el siguiente: \\[ \\bar{y}_{S}=\\frac{\\sum_{i\\in S_{I}}t_{y_{i}}}{\\sum_{i\\in S_{I}}N_{i}} \\] En términos generales el siguiente esquema trata de reproducir gráficamente este proceso de inferencia, en donde un gran número de muestras podrían haber sido extraídas siguiendo el diseño de muestreo. Con la siguiente simulación de Monte Carlo se comprueba fácilmente que es insesgado, mientras que es sesgado: library(TeachingSampling) N=100 theta=0.3 y=rbinom(N, 1, theta) theta_N=mean(y) nsim2=1000 Est1=Est2=rep(NA,nsim2) #-----Creación de los clusters--------- clus=c(0,which((y[-N]-y[-1])!=0)+1) NI=(length(clus)-1) Ind=matrix(0, nrow=N, ncol=NI) Tamaños=clus[-1]-clus[-(length(clus))] for(l in 1:(length(clus)-1)){ a=(clus[l]+1):clus[l+1] Ind[a,l]=a } #Tamaños nsim2=1000 nI=floor(NI*0.3) for(j in 1:nsim2){ res &lt;- S.piPS(nI,Tamaños) sam &lt;- res[,1] Ind.sam=Ind[,sam] Tamaños.sam=Tamaños[sam] #-------Espacio para las medias medias=matrix(NA) for(k in 1:ncol(Ind.sam)){ medias[k]=mean(y[Ind.sam[,k]]) } #------- Est1[j]=mean(medias) Est2[j]=sum(Tamaños.sam*medias)/sum(Tamaños) } Esp1=mean(Est1) Esp2=mean(Est2) cbind(theta_N, Esp1, Esp2) theta_N Esp1 Esp2 0.35 0.3551405 0.1380303 Nótese que el primer estimador es insesgado (su esperanza equivale al parámetro de la población finita) porque es función del inverso de la probabilidad de inclusión de los elementos que son inducidas por la medida de probabilidad definida por el plan de muestreo. El segundo estimador es sesgado porque no tiene en cuenta el diseño de muestreo. Inferencia doble: los modelos y el muestreo En último lugar, suponga que los valores de las variables de interés sí constituyen realizaciones de variables aleatorias que siguen un modelo probabilístico. Como una población finita está constituida por la realización particular de las variables aleatorias, condicionado a la realización de una población finita, se extrae una muestra aleatoria de elementos, mediante un diseño de muestreo complejo. Nótese que, en este tercer proceso inferencial, tanto el modelo como el diseño de muestreo como la medida de probabilidad que da origen a las superpoblaciones, constituyen dos medidas de probabilidad distintas que deben regir la inferencia del parámetro de interés. Al respecto, nótese que, dado que el diseño de muestreo es complejo, no es viable utilizar técnicas clásicas, como el método de máxima verosimilitud, puesto que los datos finales no constituyen una muestra aleatoria de variables independientes ni idénticamente distribuidas. Por lo anterior, la forma final de la función de verosimilitud, definida como la densidad conjunta de las variables en la muestra, será muy compleja, intratable e insoluble. Una solución a este problema de estimación es la técnica de máxima pseudo-verosimilitud, la cual induce estimadores que tienen en cuenta las ponderaciones del diseño de muestreo complejo. Para el ejemplo de las proporciones, el estimador \\(\\bar{y}_{\\pi S}\\) cumple la siguiente relación: \\[ E_{\\xi p}(\\bar{y}_{\\pi S})=E_{\\xi}E_{p}(\\bar{y}_{\\pi S}|Y)=E_{\\xi}(\\bar{y}_{U})=\\theta=0.3 \\] Con la siguiente simulación de Monte Carlo se comprueba fácilmente que \\(\\bar{y}_{\\pi S}\\) es insesgado, mientras que es \\(\\bar{y}_{S}\\) sesgado: library(TeachingSampling) N=100 theta=0.3 nsim1=100 Esp1=Esp2=rep(NA,nsim1) for(i in 1:nsim1){ y=rbinom(N, 1, theta) #-----Creación de los clusters--------- clus=c(0,which((y[-N]-y[-1])!=0)+1) NI=(length(clus)-1) Ind=matrix(0, nrow=N, ncol=NI) Tamaños=clus[-1]-clus[-(length(clus))] for(l in 1:(length(clus)-1)){ a=(clus[l]+1):clus[l+1] Ind[a,l]=a } Ind Tamaños nsim2=100 nI=floor(NI*0.3) Est1=Est2=rep(NA,nsim2) for(j in 1:nsim2){ res &lt;- S.piPS(nI,Tamaños) sam &lt;- res[,1] sam Ind.sam=Ind[,sam] Tamaños.sam=Tamaños[sam] #-------Espacio para las medias medias=matrix(0) for(k in 1:ncol(Ind.sam)){ medias[k]=mean(y[Ind.sam[,k]]) } Est1[j]=mean(medias) Est2[j]=sum(Tamaños.sam*medias)/sum(Tamaños) } Esp1[i]=mean(Est1) Esp2[i]=mean(Est2) } cbind(theta, mean(Esp1), mean(Esp2)) theta 0.3 0.3144898 0.1184895 Por supuesto que, dado que el proceso de inferencia es doble, entonces este ejercicio de Monte Carlo debe ser anidado. Es decir, muchas simulaciones dentro de una simulación. Nótese que en primer lugar se debe generar todas las poblaciones finitas y para cada una de ellas se debe generar las posibles muestras. Los métodos que se explicarán en este capítulo serán la estimación por Máxima Verosimilitud (MV) y Máxima Pseudo Verosimilitud (MPV) para modelos de regresión. El primer método se basa en estimar un parámetro desconocido suponiendo que las variables de interés constituyen una muestra aleatoria de variables independiente e idénticamente distribuidas (IID) para poder hacer inferencia sobre la población de interés. Por otra parte el método de Máxima Pseudo Verosimilitud sigue un razonamiento parecido, pero con la gran diferencia de que la variable de interés se rige por un diseño muestral específico, lo cual induce una probabilidad de inclusión del individuo que debe ser tenida en cuenta al momento de realizar cualquier tipo de inferencia. "],["método-de-máxima-verosimilitud.html", "9.2 Método de Máxima Verosimilitud", " 9.2 Método de Máxima Verosimilitud Uno de los métodos más utilizados en la estadística para estimar parámetros es el método de Máxima Verosimilitud, para utilizar este método debemos conocer la función de distribución de las variables de interés. Luego, si \\(y_{1},y_{2},\\ldots,y_{N}\\) una muestra aleatoria de las variable de interés que siguen una distribución \\(f(y;\\theta)\\). Por lo tanto, la función de verosimilitud está dada por: \\[ L(\\theta)=\\prod_{i=1}^{n}f(y_{i},\\theta) \\] Para un mejor manejo de esta función se sugiere aplicar propiedades de los logaritmos generando la siguiente función \\[ l(\\theta)=\\sum_{i=1}^{n}\\ln[f(y_{i},\\theta)] \\] Calculando las derivadas con respecto a \\(\\theta\\) e igualando a cero tenemos el siguiente sistema de ecuaciones \\[ \\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\theta}\\ln[f(y_{i},\\theta)]=0 \\] Ahora, definiendo a \\(u_{i}=\\frac{\\partial}{\\partial\\theta}\\ln[f(y_{i},\\theta)]\\), entonces el sistema de ecuaciones tendría la siguiente forma: \\[ \\sum_{i=1}^{N}u_{i}(\\theta)=0 \\] \\(u_{i}\\) es conocido el puntaje o de la unidad \\(i\\)-ésima. La solución de este sistema de ecuaciones, notada como \\(\\hat{\\theta}_{MV}\\), es conocida como el Estimador de Máxima Verosimilitud. Una bondad de este método es que podemos obtener una varianza asintótica del modelo \\(\\xi\\), de la siguiente manera \\[ V_{\\xi}(\\hat{\\theta}_{MV})\\cong[J(\\theta)]^{-1} \\] donde, \\[ J(\\theta)=\\sum_{i=1}^{N}\\partial u_{i}(\\theta)/\\partial\\theta \\] Como el anterior término depende del parámetro, un estimador consistente estaría dado por: \\[ \\hat{V}_{\\xi}(\\hat{\\theta}_{MV})=[J(\\hat{\\theta}_{MV})]^{-1} \\] donde, \\[ J(\\hat{\\theta}_{MV})=J(\\theta)\\mid_{\\theta=\\hat{\\theta}_{MV}} \\] MV para una distribución Bernoulli En el ejemplo introductorio que sirvió como punto de partida para esta discusión, se habló de que los datos de naturaleza \\(\\{0,1\\}\\) pueden ser modelados mediante una distribución Bernoulli, con parámetro de éxito \\(\\theta\\). De esta forma, la función de verosimilitud está dada por: \\[ L(\\theta) =\\prod_{i=1}^{N}\\theta^{y_{i}}(1-\\theta)^{1-y_{i}} \\] Luego, aplicando logaritmo, se tiene que: \\[ l(\\theta) =\\sum_{i=1}^{N}\\left[{y_{i}}\\ln(\\theta)+(1-y_{i})\\ln(1-\\theta)\\right] \\] Por lo tanto, las ecuaciones de verosimilitud, definidas en función de las variables de puntaje () son: \\[ \\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\theta}\\left[{y_{i}}\\ln(\\theta)+(1-y_{i})\\ln(1-\\theta)\\right]=\\sum_{i=1}^{N}u_{i}(\\theta) \\] En donde, \\(u_{i}(\\theta)=\\frac{y_{i}-\\theta}{\\theta(1-\\theta)}\\). Por tanto, igualando a cero, se obtiene que \\[ \\frac{\\partial}{\\partial\\theta}\\ln(\\theta)\\bar{y}_{U}+\\frac{\\partial}{\\partial\\theta}\\ln(1-\\theta)(n-\\bar{y}_{U})=0 \\] De lo cual se obtiene el estimador de máxima verosimilitud dado por: \\[ \\hat{\\theta}_{MV}=\\bar{y}_{U}=P_{d} \\] Con varianza estimada dada por: \\[ \\hat{V}_{\\xi}(\\hat{\\theta}_{MV})=[J(\\hat{\\theta}_{MV})]^{-1} \\] En donde, \\[ J(\\hat{\\theta}_{MV})=\\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\theta}u_{i}(\\theta)=\\frac{N}{\\bar{y}_{U}(1-\\bar{y}_{U})}=\\frac{N}{P_{d}(1-P_{d})} \\] Es decir que la estimación de la varianza para \\(\\hat{\\theta}_{MV}=P_{d}\\) es \\(\\hat{Var}_{\\xi}(\\hat{\\theta}_{MV})=P_{d}Q_{d}/N\\). En donde, \\(Q_{d}=1-P_{d}\\). MV para una distribución normal Ahora se ilustrará el método de Máxima Verosimilitud suponiendo la siguiente función de distribución de un variable aleatoria con distribución normal \\[ f(y;\\theta)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2}\\left(\\dfrac{y_{i}-\\theta^{2}}{\\sigma^{2}}\\right)\\right] \\] Conociendo la función de distribución llegamos a la probabilidad conjunta \\[ L(\\theta)=\\prod_{i=1}^{N}\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2}\\left(\\dfrac{y_{i}-\\theta^{2}}{\\sigma^{2}}\\right)\\right] \\] Con un poco de álgebra llegamos a esta expresión \\[ L(\\theta)=(2\\pi\\sigma^{2})^{-N/2}\\exp[(-\\dfrac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}(y_{i}-\\theta^{2})] \\] Aplicando logaritmos \\[ l(\\theta)=ln(2\\pi\\sigma^{2})^{-N/2}[-\\dfrac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}(y_{i}-\\theta^{2})] \\] Maximizando la anterior expresión llegamos a obtener el score \\(u_{i}\\) \\[ u_{i}=\\partial l(\\theta)/\\partial\\theta=\\dfrac{1}{\\sigma^{2}}\\sum_{i=1}^{N}(y_{i}-\\theta^{2})=0 \\] igualando a cero despejamos \\(\\theta\\) y tenemos \\[ \\theta=\\dfrac{\\sum_{i=1}^{N}y_{i}}{N}=\\bar{Y} \\] Llegamos a que una estimación por el método de Máxima Verosimilitud, para la función \\(\\theta\\) que sigue una función de distribución normal, es el promedio poblacional \\(\\bar{Y}\\). MV para una regresión lineal múltiple En un entorno matricial se puede tener en cuenta más de una variable predictora llevándonos a un modelo de regresión múltiple donde no solamente las variables \\(y_{i}\\) son continuas, sino que también pueden ser categóricas. A continuación, se presenta la estimación de parámetros del modelo. El modelo adopta la forma \\(X&#39;\\beta\\), disponemos de \\(X\\) como una matriz de dimensión \\(N\\times i\\), donde \\(n\\) es el tamaño de muestra e \\(i\\) es el número de variables predictoras, también se define un vector \\(Y\\) de tamaño \\(n\\) como la variable de interés y, por último, un vector \\(\\beta\\) de tamaño \\(i\\). Suponiendo que \\(X\\) sigue una distribución normal tenemos la siguiente función: \\[ f(Y;X\\beta)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2\\sigma^{2}}(Y-X\\beta)&#39;(Y-X\\beta)\\right] \\] Conociendo la anterior función de distribución, llegaremos a la probabilidad conjunta de \\(f\\) \\[ L(Y;X\\beta)=\\prod_{i=1}^{n}\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2\\sigma^{2}}(Y-X\\beta)&#39;(Y-X\\beta)\\right] \\] Con un poco de algebra matricial se llega a: \\[ L(Y;X\\beta)=(2\\pi\\sigma^{2})^{-n/2}-\\exp\\left[\\dfrac{1}{2\\sigma^{2}}(Y&#39;Y-Y&#39;X\\beta-(X\\beta)&#39;Y+(X\\beta)&#39;X\\beta)\\right] \\] Aplicando propiedades de logaritmos nos queda la siguiente expresión: \\[ l(Y;X\\beta)=ln(2\\pi\\sigma^{2})^{-n/2}-\\dfrac{1}{2\\sigma^{2}}(Y&#39;Y-Y&#39;X\\beta-(X\\beta)&#39;Y+(X\\beta)&#39;X\\beta) \\] Maximizando el anterior resultado podemos llegar al score \\(u_{i}\\): \\[ \\dfrac{\\partial l(Y;X\\beta)}{\\partial\\beta}=-\\dfrac{1}{2\\sigma^{2}}(-2X&#39;Y+2X&#39;X\\beta) \\] Igualando a cero la derivada y despejando llegamos a la estimación de \\(\\beta\\): \\[ \\beta=(X&#39;X)^{-1}(X&#39;Y) \\] El anterior resultado es la estimación general de \\(\\beta\\) en una regresión múltiple, obtenida bajo el método de Máxima Verosimilitud. "],["método-de-máxima-pseudo-verosimilitud.html", "9.3 Método de Máxima Pseudo-Verosimilitud", " 9.3 Método de Máxima Pseudo-Verosimilitud El anterior método tiene la particularidad de que \\(y_{i}\\) son IID, en la vida real muchas veces no es posible poder cumplir ese supuesto. En los procedimientos actuales se recurre a obtener una muestra compleja mediante la realización de conglomerados que tengan alguna relación particular de aglomeración para luego estratificarlos y llegar al individuo de interés que nos proporcione información sobre el estudio. Con esa muestra compleja cumplimos con que todos los individuos tienen una probabilidad de inclusión desigual sin la necesidad de utilizar un marco muestral. A partir de eso Pfeffermann(1993) discutió la posibilidad de hacer inferencia en la población partiendo de la información de una muestra, para esto se propuso crear un pseudo-parámetro que tenga en cuenta el diseño muestral, es decir, que el score \\(u_{i}\\) sea ponderado por el inverso de la probabilidad de inclusión que denominaremos \\(u_{i}\\). Este método es conocido como Máxima Pseudo Verosimilitud(MPV). \\[ L(\\theta)=\\prod_{i=1}^{n}w_{i}f(y_{i},\\theta) \\] Para un mejor manejo de esta función se sugiere aplicar propiedades de los logaritmos generando la siguiente función: \\[ l(\\theta)=\\sum_{i=1}^{n}\\ln[w_{i}f(y_{i},\\theta)] \\] Calculando las derivadas parciales de \\(L(\\theta)\\) con respecto a \\(\\theta\\) e igualando a cero tenemos un sistema de ecuaciones como sigue: \\[ \\dfrac{\\partial l(\\theta)}{\\partial\\theta}=\\sum_{i=1}^{n}w_{i}u_{i}(\\theta)=0 \\] donde \\(ui=\\partial\\ln[f(y_{i},\\theta)]/\\partial\\theta\\) es el vector de “score” de elementos \\(i,i\\in n\\) ponderado por \\(w_{i}\\), ahora definiremos \\(T\\) como: \\[ T=\\sum_{i=1}^{n}w_{i}u_{i}(\\theta)=0 \\] Mediante la linealización de Taylor y considerando los resultados de Binder(1983), podemos obtener una varianza asintóticamente insesgada de la siguiente forma: \\[ V_{p}(\\hat{\\theta}_{M}PV)\\cong[J(\\theta_{U})]^{-1}V_{p}(T)[J(\\theta_{U})]^{-1} \\] donde, \\[ J(\\theta_{U})=\\sum_{i\\in U}\\dfrac{\\partial u_{i}(\\theta)}{\\partial(\\theta)}\\mid_{\\theta=\\theta_{U}} \\] La estimación de la varianza anterior está definida por: \\[ \\hat{V}_{p}(\\hat{\\theta}_{MPV})\\cong[\\hat{J}(\\hat{\\theta}_{MPV})]^{-1}\\hat{V}_{p}(T)[\\hat{J}(\\hat{\\theta}_{MPV})]^{-1} \\] donde, \\[ \\hat{J}(\\theta_{MPV})=\\sum_{i\\in U}w_{i}\\dfrac{\\partial u_{i}(\\theta)}{\\partial(\\theta)}\\mid_{\\theta=\\theta_{MPV}} \\] MPV para una distribución Bernoulli Las ecuaciones de verosimilitud dadas anteriormente, conllevan a aplicar la técnica de pseudo-verosimilitud, para la cual, en primer lugar, se definen: \\[ u_{k}(\\theta)=\\frac{y_{k}-\\theta}{\\theta(1-\\theta)} \\] Luego, las ecuaciones de pseudo-verosimilitud son: \\[ \\sum_{k=1}^{n}w_{k}u_{k}(\\theta)=\\sum_{k=1}^{n}w_{k}\\frac{y_{k}-\\theta}{\\theta(1-\\theta)} \\] Por lo tanto, al igualar a cero, se tiene que: \\[ \\sum_{k=1}^{n}w_{k}y_{k}-\\theta\\sum_{k=1}^{n}w_{k}=0 \\] Por lo anterior, al despejar, se tiene que el estimador de máxima pseudo-verosimilitud, está dado por: \\[ \\hat{\\theta}_{MPV}=\\frac{\\sum_{k=1}^{n}w_{k}y_{k}}{\\sum_{k=1}^{n}w_{k}}=\\frac{\\hat{t}_{y,\\pi}}{\\hat{N}}=\\tilde{y}_{S}=\\tilde{p}_{d} \\] Luego, el estimador de la varianza de \\(\\hat{\\theta}_{MPV}\\) es: \\[ \\hat{V}_{p}(\\hat{\\theta}_{MPV})\\cong[\\hat{J}(\\hat{\\theta}_{MPV})]^{-1}\\hat{V}_{p}(\\hat{t}_{u\\pi})[\\hat{J}(\\hat{\\theta}_{MPV})]^{-1} \\] donde \\[ \\hat{J}(\\theta_{MPV})=\\sum_{i\\in U}w_{i}\\dfrac{\\partial u_{i}(\\theta)}{\\partial(\\theta)}\\mid_{\\theta=\\hat{\\theta_{MPV}}}=\\frac{\\hat{N}}{\\tilde{y}_{S}(1-\\tilde{y}_{S})}=\\frac{\\hat{N}}{\\tilde{p}_{d}(1-\\tilde{p}_{d})} \\] Por ejemplo, bajo un muestreo aleatorio simple sin reemplazo, se tiene que el estimador de máxima pseudo-verosimilitud es \\(\\hat{\\theta}_{MPV}=\\bar{y}_{S}\\). Además, la estimación de su varianza es: \\[ \\hat{V}_{MAS}(\\hat{t}_{u\\pi})=\\frac{N^{2}}{n}\\left(1-\\frac{n}{N}\\right)S_{\\hat{u}_{S}}^{2}=\\frac{N^{2}}{n}\\left(1-\\frac{n}{N}\\right)\\frac{1}{n-1}\\sum_{k=1}^{n}(\\hat{u}_{k}-\\bar{\\hat{u}})^{2} \\] Luego, teniendo en cuenta que bajo este diseño de muestreo, se tiene que \\(\\bar{\\hat{u}}=0\\) y que \\(\\hat{N}=N\\), entonces el estimador de la varianza de \\(\\hat{\\theta}_{MPV}\\) es: \\[ \\hat{V}_{MAS}(\\hat{\\theta}_{MPV})\\cong\\frac{1}{n}\\left(1-\\frac{n}{N}\\right)S_{y_{S}}^{2} \\] Nótese que la anterior expresión, coincide plenamente con la estimación de la varianza de la media muestral, es decir \\(\\hat{V}_{MAS}(\\hat{\\theta}_{MPV})=\\hat{V}_{MAS}(\\bar{y}_{S})\\). MPV para una distribución normal Siguiendo el mismo orden de la sección de Máxima Verosimilitud, se ilustrará el método de Máxima Pseudo Verosimilitud, suponga que \\(f(y;\\theta)\\) sigue una función de distribución normal. \\[ f(y;\\theta)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2}\\left(\\dfrac{y_{i}-\\theta^{2}}{\\sigma^{2}}\\right)w_{i}\\right] \\] Aplicaremos la productoria para llegar a la probabilidad conjunta: \\[ L(\\theta)=\\prod_{i=1}^{n}\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2}\\left(\\dfrac{y_{i}-\\theta^{2}}{\\sigma^{2}}\\right)w_{i}\\right] \\] Con algo de algebra llegamos a: \\[ L(\\theta)=(2\\pi\\sigma^{2})^{-n/2}\\exp[(-\\dfrac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(y_{i}-\\theta^{2})w_{i}] \\] Utilizamos logaritmos tenemos: \\[ l(\\theta)=ln(2\\pi\\sigma^{2})^{-n/2}[-\\dfrac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(y_{i}-\\theta^{2})w_{i}] \\] Maximizamos la anterior expresión con derivadas parciales tenemos: \\[ \\partial l(\\theta)/\\partial\\theta=\\dfrac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(y_{i}-\\theta^{2})w_{i}=0 \\] Despejando \\(\\theta\\), se llega a un resultado interesante: \\[ \\theta=\\dfrac{\\sum_{i=1}^{n}y_{i}}{\\sum_{i=1}^{n}w_{i}}=\\dfrac{\\hat{t}_{y\\pi}}{\\hat{N}}=\\tilde{Y} \\] Esto nos conlleva que, para la función \\(\\theta\\) una estimación es el promedio muestral ponderado. MPV para una regresión múltiple Con el modelo de la forma \\(X&#39;\\beta\\) se tiene una matriz \\(X\\) de dimensión \\(n\\times i\\), donde \\(n\\) es el tamaño de la muestra e \\(i\\) es el número de variables predictoras, también una matriz \\(W\\) diagonal, con los \\(w_{i}\\), de tamaño \\(n\\times n\\), y, por último, se define dos vectores, uno \\(Y\\) de tamaño \\(n\\) como la variable de interés y otro \\(\\beta\\) de tamaño \\(i\\). Con estas condiciones se puede definir una función de verosimilitud de la siguiente manera. Conociendo la función de distribución normal de \\(X\\) \\[ f(Y;X\\beta)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2\\sigma^{2}}(Y-X\\beta)&#39;W(Y-X\\beta)\\right] \\] Se halla la probabilidad conjunta matricialmente: \\[ L(Y;X\\beta)=\\prod_{i=1}^{n}\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2\\sigma^{2}}(Y-X\\beta)&#39;W(Y-X\\beta)\\right] \\] Simplificando la anterior expresión se llega a: \\[ L(Y;X\\beta)=(2\\pi\\sigma^{2})^{-N/2}-\\exp\\left[\\dfrac{1}{2\\sigma^{2}}(Y&#39;WY-Y&#39;WX\\beta-(X\\beta)&#39;WY+(X\\beta)&#39;WX\\beta)\\right] \\] Para poder derivar mejor, se aplica propiedades de los logaritmos: \\[ l(Y;X\\beta)=ln(2\\pi\\sigma^{2})^{-N/2}-\\dfrac{1}{2\\sigma^{2}}(Y&#39;WY-Y&#39;WX\\beta-(X\\beta)&#39;WY+(X\\beta)&#39;WX\\beta) \\] Maximizando el anterior resultado conoceremos el score \\(T\\): \\[ T=\\dfrac{\\partial l(Y;X\\beta)}{\\partial\\beta}=-\\dfrac{1}{2\\sigma^{2}}(-2X&#39;WY+2X&#39;WX\\beta) \\] Despejando \\(\\beta\\) tenemos el siguiente resultado: \\[ \\beta=(X&#39;WX)^{-1}(X&#39;Y) \\] Con este \\(\\beta\\) podemos estimar un modelo partiendo de una muestra probabilística compleja. A continuación, se ejemplifican los códigos computacionales para los modelos lineales generalizado. Inicialmente, se carga la base de datos como sigue: Cargue de las bases de datos, library(survey) library(srvyr) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) data(&quot;BigCity&quot;, package = &quot;TeachingSampling&quot;) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) Luego, se define la variable pobreza con los datos de la base como sigue: diseno &lt;- diseno %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0)) A continuación, se carga el tema de la CEPAL con el que se realizarán las gráficas: Para la ejemplificación de la metodología de este capítulo, se definirá primero el modelo Gamma. Modelo gamma para variable continua La función de enlace \\(g(\\cdot)\\) para el GLM con una variable dependiente distribuida por un modelo Gamma es el recíproco, \\(\\frac{1}{\\mu_{i}}\\). Eso significa que el valor esperado de \\(y_i\\) observado, (\\(E(y_i) = \\mu_i\\)), está relacionado con sus variables de entrada como, por ejemplo, \\[ \\frac{1}{\\mu_{i}} = B_0 + B_1x_1 \\] o, similarmente: \\[ \\mu_{i} = \\frac{1}{B_0 + B_1x_1} \\] Ahora bien, para el ajuste del modelo Gamma, primero se definen los pesos qweigth como sigue: mod_qw &lt;- lm(wk ~ Age + Sex + Region + Zone, data = encuesta) encuesta$wk2 &lt;- encuesta$wk/predict(mod_qw) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk2, nest = T) El modelo ajustado es el siguiente: modelo &lt;- svyglm(formula = Income ~ Age + Sex + Region + Zone, design = diseno, family = Gamma(link = &quot;inverse&quot;)) broom::tidy(modelo) term estimate std.error statistic p.value (Intercept) 0.0024054 0.0002192 10.9726242 0.0000000 Age -0.0000018 0.0000014 -1.2837541 0.2018776 SexMale -0.0000910 0.0000494 -1.8422509 0.0680837 RegionSur -0.0000528 0.0002279 -0.2315853 0.8172827 RegionCentro 0.0000305 0.0002202 0.1382979 0.8902533 RegionOccidente 0.0002363 0.0002317 1.0196070 0.3101123 RegionOriente 0.0000088 0.0002747 0.0319219 0.9745911 ZoneUrban -0.0009295 0.0001906 -4.8762260 0.0000036 De la tabla anterior, el intercepto, la variable edad y la zona rural resultaron significativo tomando una confianza del 95%. Por otro lado, es útil la estimación de la dispersión que ofrece svyglm de forma predeterminada dado que no tiene en cuenta la información especial sobre la dispersión que se puede calcular utilizando la distribución Gamma. No todos los GLM tienen una forma mejorada y específica del modelo para estimar. alpha = MASS::gamma.dispersion(modelo) alpha ## [1] 0.4830962 Luego, el parámetro de dispersión es: mod_s &lt;- summary(modelo, dispersion = alpha) mod_s$dispersion ## variance SE ## [1,] 0.59097 0.0876 Los coeficientes del modelo también se pueden obtener de la siguiente manera: mod_s$coefficients Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.0024054 0.0002192 10.9726242 0.0000000 Age -0.0000018 0.0000014 -1.2837541 0.2018776 SexMale -0.0000910 0.0000494 -1.8422509 0.0680837 RegionSur -0.0000528 0.0002279 -0.2315853 0.8172827 RegionCentro 0.0000305 0.0002202 0.1382979 0.8902533 RegionOccidente 0.0002363 0.0002317 1.0196070 0.3101123 RegionOriente 0.0000088 0.0002747 0.0319219 0.9745911 ZoneUrban -0.0009295 0.0001906 -4.8762260 0.0000036 Una vez estimado los coeficientes, se estiman los intervalos de confianza para la predicción como sigue: pred &lt;- data.frame(predict(modelo, type = &quot;response&quot;, se = T)) pred_IC &lt;- data.frame(confint(predict(modelo, type = &quot;response&quot;, se = T))) colnames(pred_IC) &lt;- c(&quot;Lim_Inf&quot;, &quot;Lim_Sup&quot;) pred &lt;- bind_cols(pred, pred_IC) pred$Income &lt;- encuesta$Income pred$Age &lt;- encuesta$Age pred %&gt;% slice(1:6L) response SE Lim_Inf Lim_Sup Income Age 456.8271 41.80366 374.8934 538.7607 409.87 68 434.3813 38.07031 359.7649 508.9977 409.87 56 423.5225 37.33444 350.3483 496.6966 409.87 24 441.2123 39.35438 364.0792 518.3455 409.87 26 416.6866 37.77570 342.6476 490.7257 409.87 3 436.1285 38.36471 360.9351 511.3219 823.75 61 A continuación, como se ha mostrado anteriormente, se realiza un Scaterplot de la predicción: pd &lt;- position_dodge(width = 0.2) ggplot(pred %&gt;% slice(1:100L), aes(x = Age , y = response)) + geom_errorbar(aes(ymin = Lim_Inf, ymax = Lim_Sup), width = .1, linetype = 1) + geom_point(size = 2, position = pd) + theme_bw() "],["modelo-multinomial.html", "9.4 Modelo multinomial", " 9.4 Modelo multinomial En capítulos anteriores de este libro se desarrolló la manera de cómo hacer el análisis de variables aleatorias provenientes de encuestas complejas de tipo binario. Sin embargo, en encuestas complejas también existen variables de tipo multinomial, es decir, que tengan más de dos niveles de respuesta. Para el análisis de este tipo de variables se utiliza el modelo multinomial, el cual se desarrollará en esta sección. El modelo de regresión logit multinomial es la extensión natural del modelo de regresión logística binomial simple para analizar variables con respuestas que tienen tres o más categorías distintas. Esta técnica es más apropiada para variables de encuesta con categorías de respuesta nominales. Para el ajuste de un modelo multinomial se debe tener en cuenta que: Su variable dependiente debe medirse en el nivel nominal. Tiene una o más variables independientes que son continuas, ordinales o nominales (incluidas las variables dicotómicas). Tener independencia de las observaciones y la variable dependiente debe tener categorías mutuamente excluyentes y exhaustivas No debe haber multicolinealidad. La multicolinealidad ocurre cuando tiene dos o más variables independientes que están altamente correlacionadas entre sí. Debe haber una relación lineal entre cualquier variable independiente continua y la transformación logit de la variable dependiente. No debe haber valores atípicos, valores de apalancamiento elevados o puntos influyentes. El modelo múltinomial está dado como: \\[ Pr\\left(Y_{ik}\\right)=Pr\\left(y_{i}=k\\mid\\boldsymbol{x}_{i}:\\boldsymbol{\\beta}_{1},\\dots\\boldsymbol{\\beta}_{m}\\right)=\\frac{\\exp\\left(\\beta_{0k}+\\boldsymbol{\\beta}_{k}\\boldsymbol{x}_{i}\\right)}{\\sum_{j=1}^{m}\\exp\\left(\\beta_{0j}+\\boldsymbol{\\beta}_{j}\\boldsymbol{x}_{i}\\right)} \\] donde \\(\\boldsymbol{\\beta}_k\\) es el vector de coeficiente de \\(\\boldsymbol{X}\\) para la k-ésima categoría. Para la estimación de los parámetros del modelo logístico se utilizará la máxima verosimilitud como técnica de estimación. Para ello, implica maximizar la siguiente versión multinomial de la función de pseudoverosimilitud (Heeringa): \\[ PL_{Mult}\\left(\\hat{\\beta}\\mid\\boldsymbol{X}\\right) = \\prod_{i=1}^{n}\\left\\{ \\prod_{i=1}^{k}\\hat{\\pi}_{k}\\left(x_{i}\\right)^{y_{i\\left(k\\right)}}\\right\\} ^{w_{i}} \\] donde, \\(y_{i\\left(k\\right)}=1\\) si \\(y = k\\) para la unidad muestreada \\(i\\), 0 en caso contrario. \\(\\hat{\\pi}_{k}\\left(x_{i}\\right)\\) es la probabilidad estimada de que \\(y_{i}=k\\mid x_{i}\\). \\(w_{i}\\) es el peso de la encuesta para la unidad muestreada \\(i\\). La maximización implica la aplicación del algoritmo de Newton-Raphson para resolver el siguiente conjunto de ecuaciones de estimación \\(\\left(K-1\\right)\\times\\left(p+1\\right)\\), suponiendo un diseño de muestra complejo con estratos indexados por \\(h\\) y conglomerados dentro de estratos indexados por \\(\\alpha\\): \\[ S\\left(\\boldsymbol{\\beta}\\right)_{Mult} = \\sum_{h}\\sum_{\\alpha}\\sum_{i}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}^{\\left(k\\right)}-\\pi_{k}\\left(\\boldsymbol{\\beta}\\right)\\right)x&#39;_{h\\alpha i}=0 \\] Donde, \\[ \\pi_{k}\\left(\\boldsymbol{\\beta}\\right)=\\frac{exp\\left(x&#39;\\boldsymbol{\\beta_{k}}\\right)}{1+{ \\sum_{k=2}^{K}exp\\left(x&#39;\\boldsymbol{\\beta_{k}}\\right)}} \\] y \\[ \\pi_{1\\left(referencia\\right)}\\left(\\boldsymbol{\\beta}\\right)=1-\\sum_{k=2}^{K}\\pi_{k}\\left(\\boldsymbol{\\beta}\\right) \\] Por último, la matriz de varianza-covarianza de los parámetros estimados, basada en la aplicación de Binder de la linealización de la serie de Taylor a las estimaciones derivadas utilizando la estimación de pseudomáxima verosimilitud es: \\[ \\hat{Var}\\left(\\hat{\\boldsymbol{\\beta}}\\right) = \\left(J^{-1}\\right)var\\left[S\\left(\\hat{\\boldsymbol{\\beta}}\\right)\\right]\\left(J^{-1}\\right) \\] Para ejemplificar el uso de los modelos multinomiales en encuestas de hogares utilizando R se utilizan los siguientes códigos computacionales. Inicialemente, para efectos de comparación, estimemos el porcentaje de personas por desempleo: diseno %&gt;% group_by(Employment) %&gt;% summarise(Prop = survey_mean(vartype = c(&quot;se&quot;, &quot;ci&quot;))) Employment Prop Prop_se Prop_low Prop_upp Unemployed 0.0322524 0.0057006 0.0209647 0.0435401 Inactive 0.2734145 0.0106850 0.2522572 0.2945718 Employed 0.4141869 0.0138767 0.3867096 0.4416642 NA 0.2801462 0.0139885 0.2524476 0.3078449 Ahora bien, se filtran aquellas personas mayores a 15 años y se vuelve a estimar: diseno %&gt;% filter(Age &gt;= 15)%&gt;% group_by(Employment) %&gt;% summarise(Prop = survey_mean(vartype = c(&quot;se&quot;, &quot;ci&quot;))) Employment Prop Prop_se Prop_low Prop_upp Unemployed 0.0448040 0.0077955 0.0293683 0.0602398 Inactive 0.3798195 0.0150308 0.3500571 0.4095820 Employed 0.5753764 0.0130688 0.5494988 0.6012540 Con lo anterior, se observa un aumento en el porcentaje de personas empleadas en los hogares. Ahora bien, se realiza la estimación usando el modelo multinomial con la función svy_vglm del paquete svyVGAM como se muestra a continuación: library(svyVGAM) diseno_15 &lt;- diseno %&gt;% filter(Age &gt;= 15) model_mul &lt;- svy_vglm( formula = Employment ~ Age + Sex + Region + Zone, design = diseno_15, crit = &quot;coef&quot;, family = multinomial(refLevel = &quot;Unemployed&quot;)) model_mul ## Stratified 1 - level Cluster Sampling design (with replacement) ## With (238) clusters. ## Called via srvyr ## Sampling variables: ## - ids: PSU ## - strata: Stratum ## - weights: wk2 ## Data variables: ## - HHID (chr), Stratum (chr), NIh (int), nIh (dbl), dI (dbl), PersonID (chr), ## PSU (chr), Zone (chr), Sex (chr), Age (int), MaritalST (fct), Income (dbl), ## Expenditure (dbl), Employment (fct), Poverty (fct), dki (dbl), dk (dbl), wk ## (dbl), Region (fct), CatAge (ord), wk2 (dbl) ## ## Call: ## vglm(formula = formula, family = family, data = surveydata, weights = .survey.prob.weights, ## crit = &quot;coef&quot;) ## ## ## Coefficients: ## (Intercept):1 (Intercept):2 Age:1 Age:2 ## 2.29035224 2.09308478 0.02474836 0.02073894 ## SexMale:1 SexMale:2 RegionSur:1 RegionSur:2 ## -2.21949718 -0.55633376 -0.43624259 -0.27905123 ## RegionCentro:1 RegionCentro:2 RegionOccidente:1 RegionOccidente:2 ## 0.37132724 0.25576105 0.25356331 0.09279505 ## RegionOriente:1 RegionOriente:2 ZoneUrban:1 ZoneUrban:2 ## 0.61759957 0.47055482 -0.23461616 0.05674419 ## ## Degrees of Freedom: 3758 Total; 3742 Residual ## Residual deviance: 2778.983 ## Log-likelihood: -1389.492 ## ## This is a multinomial logit model with 3 levels Algo a tener en cuenta es que la función broom::tidy(), que normalmente usamos para limpiar y estandarizar la salida del modelo, no puede ser empleada en este caso, sin embargo, en el link4 encuentra la función que utilizamos a continuación. tidy.svyVGAM &lt;- function( x, conf.int = FALSE, conf.level = 0.95, exponentiate = FALSE, ... ){ ret &lt;- as_tibble(summary(x)$coeftable, rownames = &quot;term&quot;) colnames(ret) &lt;- c(&quot;term&quot;, &quot;estimate&quot;, &quot;std.error&quot;, &quot;statistic&quot;, &quot;p.value&quot;) coefs &lt;- tibble::enframe(stats::coef(x), name = &quot;term&quot;, value = &quot;estimate&quot;) ret &lt;- left_join(coefs, ret, by = c(&quot;term&quot;, &quot;estimate&quot;)) if (conf.int){ ci &lt;- broom:::broom_confint_terms(x, level = conf.level, ...) ret &lt;- dplyr::left_join(ret, ci, by = &quot;term&quot;) } if (exponentiate){ret &lt;- broom:::exponentiate(ret)} ret %&gt;% tidyr::separate(term, into = c(&quot;term&quot;, &quot;y.level&quot;), sep = &quot;:&quot;) %&gt;% arrange(y.level) %&gt;% relocate(y.level, .before = term) } Utilizando el código anterior, obtenemos las estimaciones del modelo multinomial: tab_model &lt;- tidy.svyVGAM(model_mul, exponentiate = FALSE, conf.int = FALSE) tab_model A continuación, se grafica los intervalos de confianza para los coeficientes del modelo utilizando el formato definido para la CEPAL: library(dotwhisker) tab_model %&gt;% mutate(model = if_else( y.level == 1, &quot;Inactive&quot;, &quot;Employed&quot;, ), sig = gtools::stars.pval(p.value) ) %&gt;% dotwhisker::dwplot( dodge_size = 0.3, vline = geom_vline(xintercept = 1, colour = &quot;grey60&quot;, linetype = 2) ) + guides(color = guide_legend(reverse = TRUE)) + theme_bw() + theme(legend.position = &quot;top&quot;) Una forma aternativa de ajustar los modelos de regresión multinomial es usando la función svy_vglm. Sin embargo, presenta limitaciones para hacer las predicciones con el modelo, por lo tanto, se puede usar como alternativa la función svymultinom. Cabe aclarar que esta función no está en el repositorio de R y hay que instalarla desde un repositorio de github. A continuación, se muestra cómo se realiza la instalación y posteriormente el uso de la función: #devtools::install_github(&quot;BS1125/CMAverse&quot;) library(CMAverse) model_mul2 &lt;- svymultinom( formula = Employment ~ Age + Sex + Region + Zone, weights = diseno_15$variables$wk2, data = diseno_15$variables ) summary(model_mul2)$summarydf Estimate Std. Error t value Pr(&gt;|t|) Inactive:(Intercept) 2.2900932 0.5586694 4.0991924 0.0000432 Inactive:Age 0.0247522 0.0100125 2.4721273 0.0135199 Inactive:SexMale -2.2194689 0.3162459 -7.0181738 0.0000000 Inactive:RegionSur -0.4361437 0.4258014 -1.0242890 0.3058318 Inactive:RegionCentro 0.3715204 0.4910265 0.7566198 0.4493733 Inactive:RegionOccidente 0.2537092 0.4552794 0.5572605 0.5774164 Inactive:RegionOriente 0.6175485 0.5157976 1.1972692 0.2313540 Inactive:ZoneUrban -0.2346034 0.2906882 -0.8070620 0.4197338 Employed:(Intercept) 2.0928541 0.5427128 3.8562827 0.0001190 Employed:Age 0.0207428 0.0096497 2.1495766 0.0317171 Employed:SexMale -0.5563127 0.3053398 -1.8219460 0.0686234 Employed:RegionSur -0.2789895 0.4106178 -0.6794386 0.4969444 Employed:RegionCentro 0.2559417 0.4678575 0.5470505 0.5844096 Employed:RegionOccidente 0.0929049 0.4438767 0.2093035 0.8342342 Employed:RegionOriente 0.4704651 0.5050940 0.9314407 0.3517463 Employed:ZoneUrban 0.0567466 0.2791795 0.2032619 0.8389525 El hacer uso de esta función permite obtener de manera simple la predicción de las probabilidades como se muestra a continuación: library(tidyverse) tab_pred &lt;- predict(model_mul2, type = &quot;probs&quot;) %&gt;% data.frame() tab_pred %&gt;% slice(1:15) Unemployed Inactive Employed 0.0387205 0.2236639 0.7376156 0.0150598 0.5948109 0.3901293 0.0310297 0.5550689 0.4139014 0.0907946 0.1854493 0.7237561 0.0134342 0.6005134 0.3860523 0.0317299 0.5537176 0.4145525 0.0466669 0.2157326 0.7376005 0.0172653 0.5878100 0.3949247 0.0790766 0.1920709 0.7288525 0.0294856 0.2349694 0.7355450 0.0095218 0.6169869 0.3734913 0.0621464 0.2031545 0.7346991 0.0687482 0.1985744 0.7326773 0.0839204 0.1892482 0.7268314 0.0730133 0.1958009 0.7311858 Las predicciones del modelo se realizan de la siguiente manera: diseno_15$variables %&lt;&gt;% mutate(predicion = predict(model_mul2)) diseno_15 %&gt;% group_by(Employment) %&gt;% summarise(Prop = survey_mean(vartype = c(&quot;se&quot;, &quot;ci&quot;))) Employment Prop Prop_se Prop_low Prop_upp Unemployed 0.0448040 0.0077955 0.0293683 0.0602398 Inactive 0.3798195 0.0150308 0.3500571 0.4095820 Employed 0.5753764 0.0130688 0.5494988 0.6012540 De la anterior tabla se puede observar que, el porcentaje de personas en los hogares que están desempleados es del 4.48%, inactivos del 37.9% y empleados del 57%. https://tech.popdata.org/pma-data-hub/posts/2021-08-15-covid-analysis/↩︎ "],["modelos-multinivel-en-el-contexto-de-encuestas-de-hogares.html", "Capítulo 10 Modelos multinivel en el contexto de encuestas de hogares", " Capítulo 10 Modelos multinivel en el contexto de encuestas de hogares Los modelos multinivel, también conocidos como modelos de efectos mixtos o modelos jerárquicos, son una técnica estadística utilizada en el análisis de datos de encuestas de hogares que tienen una estructura jerárquica o multinivel. En estas encuestas, los datos se recopilan a nivel individual (por ejemplo, sobre la edad, el género y la educación de cada miembro del hogar) y a nivel del hogar (por ejemplo, sobre el ingreso del hogar, la propiedad de la vivienda y la ubicación geográfica). Los modelos multinivel permiten analizar cómo los factores a nivel del hogar y a nivel individual influyen en las respuestas a las preguntas de la encuesta. Por ejemplo, un modelo multinivel podría utilizarse para investigar cómo el ingreso del hogar y la edad de los miembros del hogar influyen en el consumo de alimentos saludables. En los modelos multinivel, se modelan los efectos aleatorios y fijos. Los efectos fijos representan las relaciones promedio entre las variables, mientras que los efectos aleatorios modelan la variación en estas relaciones entre los hogares. De esta manera, los modelos multinivel permiten tener en cuenta la heterogeneidad en la población y obtener estimaciones más precisas de los efectos de interés. En resumen, los modelos multinivel son una herramienta valiosa en el análisis de datos de encuestas de hogares al permitir analizar cómo los factores a nivel del hogar y a nivel individual influyen en las respuestas a las preguntas de la encuesta y al tener en cuenta la estructura jerárquica de los datos. algunas referencias bibliográficas relevantes sobre el uso de modelos multinivel en encuestas de hogares son: “Multilevel statistical models” de Harvey Goldstein (2011): Este libro es una referencia clásica para el análisis de datos multinivel, y aborda el uso de modelos jerárquicos en una variedad de contextos, incluyendo encuestas de hogares. El libro cubre tanto modelos de regresión como de varianza-covarianza, y discute temas como la selección de variables, la validación de modelos y la interpretación de resultados. “Data analysis using regression and multilevel/hierarchical models” de Andrew Gelman y Jennifer Hill (2006): Este libro también es una referencia popular para el análisis de datos multinivel, y ofrece una introducción accesible a la teoría y la práctica de los modelos jerárquicos. El libro cubre tanto modelos de regresión como de varianza-covarianza, y utiliza ejemplos de encuestas de hogares para ilustrar los conceptos. “Multilevel and longitudinal modeling using Stata” de Sophia Rabe-Hesketh y Anders Skrondal (2012): Este libro es una guía práctica para el análisis de datos multinivel y longitudinales utilizando el software estadístico Stata. El libro cubre tanto modelos de regresión como de varianza-covarianza, y utiliza ejemplos de encuestas de hogares para ilustrar los conceptos. “A comparison of Bayesian and likelihood-based methods for fitting multilevel models” de William J. Browne y David Draper (2006): Este artículo compara el rendimiento de los enfoques Bayesianos y basados en verosimilitud para ajustar modelos jerárquicos en el contexto de encuestas de hogares. Los autores concluyen que ambos enfoques pueden ser efectivos, pero que el enfoque Bayesiano puede ofrecer mayores ventajas en términos de flexibilidad y precisión. “A brief conceptual tutorial of multilevel analysis in social epidemiology: using measures of clustering in multilevel logistic regression to investigate contextual phenomena” de Juan Merlo y otros (2006): Este artículo presenta una introducción a los modelos jerárquicos en el contexto de la epidemiología social, y utiliza ejemplos de encuestas de hogares para ilustrar los conceptos. Los autores discuten cómo los modelos jerárquicos pueden ser utilizados para investigar fenómenos contextuales, como la segregación residencial y las desigualdades de salud. Para iniciar este capítulo se cargan las librerías necesarias, la base de datos y el tema de la Cepal para realizar los gráficos: Cargue de librerías: knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, error = FALSE) options(digits = 4) options(tinytex.verbose = TRUE) library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(broom) library(jtools) library(modelsummary) library(patchwork) library(ggplot2) Cargue de la base de datos: encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) Creando el tema de la CEPAL para generar los gráficos en este capítulo: theme_cepal &lt;- function(...) theme_light(10) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), legend.position=&quot;bottom&quot;, legend.justification = &quot;left&quot;, legend.direction=&quot;horizontal&quot;, plot.title = element_text(size = 20, hjust = 0.5), ...) Para efectos de ejemplificar los conceptos que se presentarán en este capítulo, definamos una muestra con 6 estratos como se muestra a continuación: encuesta_plot &lt;- encuesta %&gt;% dplyr::select(HHID, Stratum) %&gt;% unique() %&gt;% group_by(Stratum) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% dplyr::select(-n) %&gt;% slice(1:6L) %&gt;% inner_join(encuesta) %&gt;% filter(Expenditure &lt;700) %&gt;% dplyr::select(Income, Expenditure, Stratum, Sex, Region, Zone) encuesta_plot %&gt;% slice(1:10L) Income Expenditure Stratum Sex Region Zone 697.3 296.1 idStrt017 Male Norte Rural 697.3 296.1 idStrt017 Female Norte Rural 697.3 296.1 idStrt017 Male Norte Rural 697.3 296.1 idStrt017 Female Norte Rural 526.8 294.8 idStrt017 Male Norte Rural 526.8 294.8 idStrt017 Female Norte Rural 526.8 294.8 idStrt017 Female Norte Rural 526.8 294.8 idStrt017 Male Norte Rural 526.8 294.8 idStrt017 Male Norte Rural 526.8 294.8 idStrt017 Female Norte Rural A modo introductorio en este capítulo, se comenzará ajustando un modelo lineal cuya variable a modelar son los ingresos de los hogares y cuya variable explicativa son los gastos de los hogares sin considerar el efecto de los estratos del diseño muestral. A continuación, se muestra la gráfica: library(latex2exp) ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure)) + geom_jitter() + theme( legend.position=&quot;none&quot;, plot.title = element_text(hjust = 0.5)) + geom_smooth( formula = y ~ x, method = &quot;lm&quot;, se = F) + ggtitle( latex2exp::TeX(&quot;$Ingreso_{i}\\\\sim\\\\hat{\\\\beta}_{0}+\\\\hat{\\\\beta}_{1}Gasto_{i}+\\\\epsilon_{i}$&quot;)) + theme_cepal() Como se pudo observar en la gráfica anterior, el modelo lineal ajustado es común y se ajusta con los métodos estadísticos antes explicados. Como se ha explicado en capítulos anteriores, este modelo se basa en varios supuestos principales con respecto a la naturaleza de los datos en la población; más específicamente asume independencia de las observaciones a algunas variables de interés, por ejemplo, los estratos socioeconómicos. Naturalmente este supuesto no es válido más aún, cuando la selección de la muestra en muestreo probabilístico se hace por estrato muestral y además, el comportamiento de los estratos muestrales es diferente entre ellos. Teniendo en cuenta lo anterior, a continuación, se ajusta un modelo de regresión en donde el intercepto cambia de acuerdo con cada estrato. Es decir, se fija una pendiente común y se varía el intercepto, como se muestra a continuación: B1 &lt;- coef(lm(Income ~ Expenditure, data = encuesta_plot))[2] (coef_Mod &lt;- encuesta_plot %&gt;% group_by(Stratum) %&gt;% summarise(B0 = coef(lm(Income ~ 1))[1]) %&gt;% mutate(B1 = B1)) Stratum B0 B1 idStrt002 496.9 1.637 idStrt010 584.7 1.637 idStrt015 660.6 1.637 idStrt017 408.3 1.637 idStrt022 517.9 1.637 idStrt028 492.1 1.637 Ahora, se grafican cada uno de los modelos ajustados previamente: ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure, colour = Stratum)) + geom_jitter() + theme(legend.position=&quot;none&quot;, plot.title = element_text(hjust = 0.5)) + geom_abline(data = coef_Mod, mapping=aes(slope=B1, intercept=B0, colour = Stratum)) + ggtitle( latex2exp::TeX(&quot;$Ingreso_{ij}\\\\sim\\\\hat{\\\\beta}_{0j}+\\\\hat{\\\\beta}_{1}Gasto_{ij}+\\\\epsilon_{ij}$&quot;))+ theme_cepal() En la gráfica anterior Se puede observar que el ajuste no fue el mejor: sin embargo, dicho ajuste se puede ir mejorando a medida que el modelo se vaya afinando e incluyendo efectos adicionales. Por otro lado, se ajustará un modelo con pendiente aleatoria. Dicha pendiente se estimará para cada uno de los estratos definidos en el diseño muestral como se presenta a continuación: B0 &lt;- coef(lm(Income ~ Expenditure, data = encuesta_plot))[1] (coef_Mod &lt;- encuesta_plot %&gt;% group_by(Stratum) %&gt;% summarise( B1 = coef(lm(Income ~ -1 + Expenditure))[1]) %&gt;% mutate(B0 = B0)) Stratum B1 B0 idStrt002 1.727 29.56 idStrt010 2.303 29.56 idStrt015 1.837 29.56 idStrt017 1.672 29.56 idStrt022 1.478 29.56 idStrt028 1.495 29.56 Graficando el modelo con pendiente aleatoria definido anteriormente tenemos: ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure, colour = Stratum)) + geom_jitter() + theme(legend.position=&quot;none&quot;, plot.title = element_text(hjust = 0.5)) + geom_abline(data = coef_Mod, mapping=aes(slope=B1, intercept=B0, colour = Stratum)) + ggtitle( latex2exp::TeX(&quot;$Ingreso_{ij}\\\\sim\\\\hat{\\\\beta}_{0}+\\\\hat{\\\\beta}_{1j}Gasto_{ij}+\\\\epsilon_{ij}$&quot;))+ theme_cepal() A continuación, se genera un gráfico para un modelo con intercepto y pendientes aleatorias, ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure, colour = Stratum)) + geom_smooth( formula = y ~ x, method = &quot;lm&quot;, se = F) + geom_jitter() + theme(legend.position=&quot;none&quot;, plot.title = element_text(hjust = 0.5)) + ggtitle( latex2exp::TeX(&quot;$Ingreso_{ij}\\\\sim\\\\hat{\\\\beta}_{0j}+\\\\hat{\\\\beta}_{1j}Gasto_{ij}+\\\\epsilon_{ij}$&quot;))+ theme_cepal() Se puede observar que este modelo se ajusta mejor a los datos que el modelo anterior y que el modelo lineal clásico. Con los anteriores modelos se quería mostrar la importancia de realizar modelos multinivel cuando la naturaleza de los datos es anidadas. Para ejemplificar (Finch, W. H., et al, 2019), suponga que un investigador está interesado en medir el impacto que tendrá un nuevo método de enseñanza en el aprendizaje de los estudiantes. Para esto, planea un diseño muestral en donde selecciona escuelas de manera aleatoria y las ubica en un grupo de tratamiento o de control. En este sentido, si la escuela A se asigna en el grupo tratamiento y dado que el diseño muestral utilizado fue de conglomerados entonces, todos los estudiantes de la escuela también estarán en la condición de tratamiento. Es de tener en cuenta que, las escuelas (conglomerados) son las que se asignan a un grupo en particular y no directamente los estudiantes. Además, se sabe que, la escuela misma, más allá de la condición de tratamiento, tiene un impacto directo en el desempeño de los estudiantes. Este impacto se manifiesta como correlaciones en los puntajes de las pruebas de rendimiento entre las personas que asisten a esa escuela. Por lo tanto, si utilizáramos un ANOVA simple de una vía para comparar las medias de la prueba de rendimiento para los grupos de tratamiento y control con dichos datos muestreados por conglomerados, probablemente estaríamos violando la suposición de errores independientes porque un factor más allá de la condición de tratamiento (en este caso de la escuela) tendría un impacto adicional en la variable de resultado. Por lo general, nos referimos a la estructura de datos descrita anteriormente como anidada, lo que significa que los puntos de datos individuales en un nivel (por ejemplo, estudiante) aparecen solo en un nivel de una variable de nivel superior, como la escuela. Por lo tanto, los estudiantes están anidados dentro de la escuela. Dichos diseños pueden contrastarse con una estructura de datos cruzados en la que los individuos del primer nivel aparecen en múltiples niveles de la segunda variable. "],["modelos-multinivel-en-muestras-complejas.html", "10.1 Modelos multinivel en muestras complejas", " 10.1 Modelos multinivel en muestras complejas En el análisis de los modelos multinivel hay dos tipos de índices que son relevantes: Los coeficientes de regresión, generalmente denominados como los parámetros fijos del modelo. Las estimaciones de la varianza, generalmente denominadas parámetros aleatorios del modelo. Cualquier análisis de regresión multinivel siempre debe comenzar con la estimación de la varianza de los Niveles 1 y 2 para la variable dependiente. El primer paso recomendado en el análisis de regresión multinivel consiste en una descomposición de la varianza de la variable dependiente en los diferentes niveles. Por ejemplo, la varianza del ingreso se descompondrá en dos componentes: La varianza dentro del estrato la varianza entre los estratos. Estos dos componentes de varianza se pueden obtener en una regresión multinivel. Ahora bien, un modelo básico está dado por: \\[ y_{ij}=\\beta_{0j}+\\epsilon_{ij} \\] Con, \\[ \\beta_{0j}=\\gamma_{00}+\\tau_{0j} \\] Donde, \\(y_{ij}=\\) Los ingresos de la persona \\(i\\) en el estrato \\(j\\). \\(\\beta_{0j}=\\) El intercepto en el estrato \\(j\\). \\(\\epsilon_{ij}\\) El residual de la persona \\(i\\) en el estrato \\(j\\). \\(\\gamma_{00}=\\) El intercepto general. \\(\\tau_{0j}=\\) Efecto aleatorio para el intercepto. Para este modelo se asume que, \\[ \\tau_{0j}\\sim N\\left(0,\\sigma_{\\tau}^{2}\\right) \\] y, \\[ \\epsilon_{ij}\\sim N\\left(0,\\sigma_{\\epsilon}^{2}\\right) \\]. Para poder continuar con las características de este modelo, se introduce un concepto que se denomina correlación intra clásica la cual se calcula como: \\[ \\rho=\\frac{\\sigma_{\\tau}^{2}}{\\sigma_{\\tau}^{2}+\\sigma_{\\epsilon}^{2}} \\] La correlación intraclase (ICC, por sus siglas en inglés) hace referencia a la proporción de la varianza total de una variable que se explica por las diferencias entre los grupos (o niveles) en el modelo. En otras palabras, la ICC mide la similitud o correlación entre las observaciones dentro del mismo grupo o nivel en comparación con las observaciones de diferentes grupos. En un modelo multinivel, los datos están organizados en diferentes niveles, como, por ejemplo, estudiantes dentro de escuelas o pacientes dentro de hospitales. La ICC se calcula para cada nivel de agrupación en el modelo y ayuda a determinar qué tan importante es la variación entre los grupos en comparación con la variación dentro de los grupos. Una ICC alta indica que una gran proporción de la variación total de la variable se debe a las diferencias entre los grupos, lo que sugiere que los grupos son distintos entre sí y que los efectos de los grupos deben ser considerados en el modelo. Por otro lado, una ICC baja indica que la mayoría de la variación en la variable está dentro de los grupos y que los efectos de los grupos no son tan importantes para explicar la variabilidad en la variable. Aunque existe evidencia suficiente de que las ponderaciones de muestreo deben usarse en el modelado multinivel (MLM, por sus siglas en inglés) para obtener estimaciones insesgadas5, y también sobre cómo deben usarse estas ponderaciones en los análisis de un solo nivel, hay poca discusión en la literatura sobre qué y cómo usar pesos de muestreo en MLM. Actualmente, diferentes autores recomiendan diferentes enfoques sobre cómo usar los pesos de muestreo en modelos jerárquicos. Pfefermann et al. (1998) y Asparouhov (2006) aconsejan utilizar un enfoque de pseudomáxima verosimilitud para calcular estimaciones dentro y entre los diferentes niveles utilizando la técnica de maximización de mínimos cuadrados generalizados ponderados por probabilidad (PWGLS) para obtener estimaciones insesgadas. Rabe-Hesketh y Skrondal (2006) proporcionan técnicas de maximización de expectativas para maximizar la pseudoverosimilitud. Cai, T. (2013). Investigation of ways to handle sampling weights for multilevel model analyses. Sociological Methodology, 43(1), 178-219.↩︎ "],["estimación-de-pseudo-máxima-verosimilitud.html", "10.2 Estimación de pseudo máxima verosimilitud", " 10.2 Estimación de pseudo máxima verosimilitud EL método de máxima verosimilitud tiene como objetivo estimar los parámetros del modelo que maximizan la probabilidad de que se obtenga la muestra que de hecho se obtuvo. Es decir, los valores de los parámetros estimados deberían maximizar la probabilidad de elegir la muestra que se eligió para realizar el modelo . Lo anterior se realiza con la identificación de dichos valores muestrales mediante la comparación de los datos observados con los predichos por el modelo asociado a los valores de los parámetros. Cuanto más cerca estén entre sí los valores observados y predichos, mayor será la probabilidad de que los datos observados provengan de una población con parámetros cercanos a los utilizados para generar los valores predichos. Existe una variante del método de máxima verosimilitud (MLE, por sus siglas en inglés) y es la máxima verosimilitud restringida (RMLE, por sus siglas en inglés), que ha demostrado ser más precisa con respecto a la estimación de parámetros de varianza que MLE (Kreft y De Leeuw, 1998). En particular, los dos métodos difieren con respecto a cómo se calculan los grados de libertad en la estimación de las varianzas. En este sentido y como se ha definido en capítulos anteriores, la función de log-verosimilitud para la población está dada por: \\[ L_{U}\\left(\\theta\\right)=\\sum_{i\\in U}\\log\\left[f\\left(\\boldsymbol{y}_{i};\\theta\\right)\\right] \\] El estimador de máxima verosimilitud esta dada por: \\[ \\frac{\\partial L_{U}\\left(\\theta\\right)}{\\partial\\theta}=0 \\] La dificultad que se encuentra aquí es transferir los pesos muéstrales a los niveles inferiores, por ejemplo de UPMs a estratos. Pfeffermann et al. (1998) argumentaron que, debido a la estructura de datos agrupados, ya no se asume que las observaciones sean independientes y que la probabilidad logarítmica se convierta en una suma entre los elementos de nivel uno y dos en lugar de una simple suma de las contribuciones de los elementos. Pfeffermann et al. (1998) argumentaron que, debido a la estructura de datos agrupados, ya no se asume que las observaciones sean independientes y que la probabilidad logarítmica se convierta en una suma entre los elementos de nivel uno y dos en lugar de una simple suma de las contribuciones de los elementos. Para ajustar los modelos multinivel en R se usará la función lmer() de la librería lme4. A continuación se empezará a ejemplificar el ajuste de los modelos multinivel con encuestas complejas, iniciando con el ajuste de un modelo nulo. Modelo Nulo asuma que la información dentro del estrato está definida por el intercepto, \\[ Ingreso_{ij}=\\beta_{0j}+\\epsilon_{ij} \\] \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\tau_{1j} \\] Como se mostró en capítulos anteriores, para tener estimaciones consistentes se calculan los pesos Qweighted siguiendo los pasos mostrados, tomando en este caso como covariables la edad del entrevistado, el sexo, la región y la zona donde reside. Adicionalmente, se calculan también los senate-weight para el ajuste de los modelos (Wk3, en el ajuste del modelo en R). Al usar pesos para estimar los parámetros de los modelos, se debe tener en cuenta la naturaleza del diseño de muestreo. Particularmente, cuando se trata de estimaciones de parámetros multinivel, debe tener en cuenta no solo los pesos de la unidad de muestreo final sino también los primeros pesos de la unidad de muestreo. Por ejemplo, supongamos que tiene una muestra de estudiantes seleccionados de un marco nacional de escuelas. Luego, tenemos dos conjuntos de pesos, el primero con respecto a las escuelas (observe que una escuela seleccionada se representa así misma y a otros que no están en la muestra) y la segunda con respecto a los estudiantes. Para continuar con el ejemplo, supongamos también que en la población finita se tienen 10.000 estudiantes y 40 escuelas. Consideremos que se han seleccionado a 500 estudiantes asignados en 8 escuelas. En aras de la facilidad, pensemos que se usa una muestra aleatoria simple para seleccionar estudiantes. Si se tiene en cuenta solo los pesos del estudiante para que se ajusten al modelo multinivel, se encontrarán que está estimando parámetros con una muestra ampliada que representa a 10.000 estudiantes que se asignan en una muestra de solo ocho escuelas. Entonces, cualquier conclusión establecida será incorrecta. Por ejemplo, al realizar un análisis de varianza simple, el porcentaje de varianza explicado por las escuelas será extremadamente bajo. Ahora, si se tiene en cuenta ambos conjuntos de pesos (estudiantes y escuelas), se encontrará ajustando un modelo con muestras expandidas que representan 10.000 estudiantes y 40 escuelas, lo cual es lo correcto. A continuación, se presentan los pesos de muestreos usados para el ajuste del modelo. Primero, se ajustará con los pesos que se tienen directamente del diseño muestral (Wk), los segundos son los pesos q-weigth (wk2) y los senate weigth (wk3). mod_qw &lt;- lm(wk ~ Age + Sex + Region + Zone, data = encuesta) encuesta$wk2 &lt;- encuesta$wk/predict(mod_qw) n = nrow(encuesta) encuesta &lt;- encuesta %&gt;% mutate(wk3 = n*wk/sum(wk)) encuesta %&gt;% summarise(fep = sum(wk), q_wei = sum(wk2), fep2 = sum(wk3) ) fep q_wei fep2 150266 2602 2605 A continuación, se presenta un gráfico comparando los pesos q-weigth y los senate weigth que se usarán para el ajuste del modelo multinivel: ggplot(encuesta, aes(x = wk2, y = wk3)) + geom_point() + theme_bw() + labs(x = &quot;q-weighted&quot;, y = &quot;Alternativa&quot;) En este sentido, se realizarán los ajustes de los modelos utilizando los dos pesos mostrados anteriormente: library(lme4) mod_null &lt;- lmer( Income ~ ( 1 | Stratum ), data = encuesta, weights = wk2 ) mod_null2 &lt;- lmer( Income ~ ( 1 | Stratum ), data = encuesta, weights = wk3 ) Comparando los modelos obtenidos: coef_mod_null &lt;- bind_cols(coef( mod_null )$Stratum, coef(mod_null2 )$Stratum) colnames(coef_mod_null) &lt;- c(&quot;Intercept Mod 1&quot;, &quot;Intercept Mod 2&quot;) coef_mod_null %&gt;% slice(1:12) Intercept Mod 1 Intercept Mod 2 idStrt001 630.7 630.1 idStrt002 505.4 506.2 idStrt003 481.3 484.7 idStrt004 959.6 954.5 idStrt005 514.6 515.9 idStrt006 433.8 438.2 idStrt007 467.5 470.5 idStrt008 371.6 376.4 idStrt009 207.6 218.1 idStrt010 591.6 592.1 idStrt011 588.8 588.3 idStrt012 352.0 361.2 Se puede observar que las estimaciones de los interceptos son similares utilizando los dos factores de expansión. Sin embargo, se debe tener en cuenta las características de cada metolodología. Para efectos de ejemplificar el cálculo de la correlación intraclases, se utiliza la función icc de la librería performance. El cálculo es el siguiente: performance::icc(mod_null) ICC_adjusted ICC_unadjusted optional 0.3218 0.3218 FALSE se puede observar que la correlación intraclase, utilizando el modelo nulo es del 32%. Por otro lado, como el modelo que se está ajustando es el “modelo nulo”, la predicción del ingreso dentro de los estrato es constante, como se muestra a continuación: (tab_pred &lt;- data.frame(Pred = predict(mod_null), Income = encuesta$Income, Stratum = encuesta$Stratum)) %&gt;% distinct() %&gt;% slice(1:6L) # Son las pendientes aleatorias Pred Income Stratum 1 630.7 409.87 idStrt001 6 630.7 823.75 idStrt001 10 630.7 90.92 idStrt001 13 630.7 135.33 idStrt001 18 630.7 336.19 idStrt001 22 630.7 1539.75 idStrt001 Como es bien sabido, si la predicción es correcta se espera estar sobre la línea de 45°. Naturalmente, en este caso eso no se obtiene y se puede verificar en la siguiente gráfica: ggplot(data = tab_pred, aes(x = Pred, y = Income, colour = Stratum)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) Modelo con intercepto aleatoria Un modelo con pendiente aleatoria es un tipo de modelo de regresión que permite que la relación entre la variable independiente y la variable dependiente cambie según alguna otra variable explicativa. En otras palabras, permite que la pendiente de la relación entre las variables sea diferente en diferentes grupos o subconjuntos de datos. Por ejemplo, en un modelo de regresión lineal simple, la relación entre la variable independiente (X) y la variable dependiente (Y) se modela como una línea recta con una pendiente fija. Sin embargo, en un modelo con pendiente aleatoria, se permite que la pendiente varíe según otra variable explicativa, como el tiempo, la edad, el género, la ubicación geográfica, etc. En este tipo de modelos, la relación entre X e Y no se asume como lineal, sino que se puede ajustar a una curva con diferentes pendientes para diferentes subgrupos. Los modelos con pendiente aleatoria son útiles en situaciones donde se espera que la relación entre las variables cambie de manera no lineal o cuando se desea modelar diferencias en la pendiente entre subgrupos. Consideremos el siguiente modelo: \\[ Ingreso_{ij}=\\beta_{0}+\\beta_{1j}Gasto_{ij}+\\epsilon_{ij} \\] donde \\(\\beta_{1j}\\) esta dado como \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\tau_{1j} \\] Para este caso en particular, se va a variar la pendiente de acuerdo con los estratos socioeconómicos. Para ajustar el modelo se utiliza la función lmer como se muestra a continuación: mod_Int_Aleatorio &lt;- lmer( Income ~ Expenditure + (1 | Stratum), data = encuesta, weights = wk2) performance::icc(mod_Int_Aleatorio) ICC_adjusted ICC_unadjusted optional 0.1958 0.1022 FALSE Para cada estrato se tienen las siguientes estimaciones de \\(\\beta_{1j}\\) coef(mod_Int_Aleatorio)$Stratum %&gt;% slice(1:8L) (Intercept) Expenditure idStrt001 248.257 1.202 idStrt002 152.988 1.202 idStrt003 139.765 1.202 idStrt004 292.650 1.202 idStrt005 -42.165 1.202 idStrt006 46.766 1.202 idStrt007 2.841 1.202 idStrt008 103.346 1.202 Organizando los coeficientes en un gráfico se tiene: Coef_Estimado &lt;- inner_join( coef(mod_Int_Aleatorio)$Stratum %&gt;% add_rownames(var = &quot;Stratum&quot;), encuesta_plot %&gt;% dplyr::select(Stratum) %&gt;% distinct()) ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure, colour = Stratum)) + geom_jitter() + theme(legend.position=&quot;none&quot;, plot.title = element_text(hjust = 0.5)) + geom_abline(data = Coef_Estimado, mapping=aes(slope=Expenditure, intercept=`(Intercept)`, colour = Stratum))+ theme_cepal() Se puede observar que, la estimación de la pendiente varía de manera importante para cada uno de los estratos, por tanto, el ajuste del modelo con pendiente aleatoria es adecuado. Por otro lado, la predicción de los ingresos usando este modelo se muestra a continuación: (tab_pred &lt;- data.frame( Pred = predict(mod_Int_Aleatorio), Income = encuesta$Income, Stratum = encuesta$Stratum)) %&gt;% distinct() %&gt;% slice(1:6L) Pred Income Stratum 1 664.4 409.87 idStrt001 6 719.6 823.75 idStrt001 10 337.3 90.92 idStrt001 13 348.9 135.33 idStrt001 18 560.9 336.19 idStrt001 22 890.5 1539.75 idStrt001 Gráficamente se muestran las estimaciones versus los valores estimados de los ingresos y se logra observar que la predicción está más cerca a la línea de 45 grados que el modelo anterior. ggplot(data = tab_pred, aes(x = Pred, y = Income, colour = Stratum)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) Modelo con intercepto y pendiente aleatoria Los modelos con intercepto y pendiente aleatoria, también conocidos como modelos mixtos de efectos mixtos o modelos de regresión lineal mixta, son un tipo de modelo estadístico que permite modelar la relación entre una variable de respuesta y una o más variables predictoras, teniendo en cuenta tanto efectos fijos como efectos aleatorios. En los modelos con intercepto y pendiente aleatoria, los coeficientes de la regresión (es decir, la pendiente y el intercepto) se consideran aleatorios en lugar de fijos. Esto significa que se asume que estos coeficientes pueden variar entre las unidades de análisis, que pueden ser individuos, grupos, regiones geográficas, etc. Estas variaciones se modelan como efectos aleatorios que se incorporan en la ecuación de regresión. Los modelos con intercepto y pendiente aleatoria son útiles cuando se trabaja con datos que tienen una estructura jerárquica o de agrupamiento, donde las unidades de análisis están agrupadas en diferentes niveles. Por ejemplo, en un estudio sobre el rendimiento académico de los estudiantes, las unidades de análisis pueden ser los estudiantes y las escuelas a las que asisten. En este caso, se puede modelar tanto la variación entre los estudiantes como la variación entre las escuelas en la relación entre el rendimiento académico y los predictores. Estos modelos son ampliamente utilizados en diversas áreas, como la psicología, la medicina, la sociología, la economía, la biología y la ecología, entre otras. Para la ejemplificación, consideremos el siguiente modelo: \\[ Ingreso_{ij}=\\beta_{0j}+\\beta_{1j}Gasto_{ij}+\\epsilon_{ij} \\] donde, \\[ \\beta_{0j} = \\gamma_{00}+\\gamma_{01}Stratum_{j} + \\tau_{0j} \\] y, \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\tau_{1j} \\] El ajuste del modelo se realiza utilizando la función lmer como se presenta a continuación: mod_Pen_Aleatorio &lt;- lmer(Income ~ Expenditure + (1 + Expenditure| Stratum), data = encuesta, weights = wk2) performance::icc(mod_Pen_Aleatorio) ICC_adjusted ICC_unadjusted optional 0.6971 0.4598 FALSE Los coeficientes del modelo son: coef(mod_Pen_Aleatorio)$Stratum %&gt;% slice(1:10L) (Intercept) Expenditure idStrt001 -230.63 2.7761 idStrt002 31.07 1.6236 idStrt003 152.51 1.1621 idStrt004 230.80 1.3452 idStrt005 -97.05 1.2964 idStrt006 30.34 1.2039 idStrt007 37.63 1.0771 idStrt008 167.70 0.9018 idStrt009 30.46 0.7488 idStrt010 74.87 1.8971 Gráficamente, Coef_Estimado &lt;- inner_join( coef(mod_Pen_Aleatorio)$Stratum %&gt;% add_rownames(var = &quot;Stratum&quot;), encuesta_plot %&gt;% dplyr::select(Stratum) %&gt;% distinct()) ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure, colour = Stratum)) + geom_jitter() + theme(legend.position=&quot;none&quot;, plot.title = element_text(hjust = 0.5)) + geom_abline(data = Coef_Estimado, mapping=aes(slope=Expenditure, intercept=`(Intercept)`, colour = Stratum))+ theme_cepal() Como se pudo observar en la gráfica anterior y en el coeficiente de correlación intraclase, el ajuste del modelo con intercepto y pendiente aleatoria se ajusta mejor a los datos que los otros dos modelos mostrados anteriormente. A continuación, se realizan las predicciones de los ingresos con el modelo: (tab_pred &lt;- data.frame(Pred = predict(mod_Pen_Aleatorio), Income = encuesta$Income, Stratum = encuesta$Stratum)) %&gt;% distinct() %&gt;% slice(1:6L) # Son las pendientes aleatorias Pred Income Stratum 1 730.855 409.87 idStrt001 6 858.279 823.75 idStrt001 10 -25.002 90.92 idStrt001 13 1.982 135.33 idStrt001 18 491.663 336.19 idStrt001 22 1253.017 1539.75 idStrt001 Para poder ver qué tan buena son las predicciones, se realiza el siguiente gráfico: ggplot(data = tab_pred, aes(x = Pred, y = Income, colour = Stratum)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) Del anterior gráfico se logra observar que las predicciones del ingreso son mejores que las realizadas con los dos modelos anteriores. Lo anterior se debe a la naturaleza misma de los datos en una encuesta de hogares. Ahora bien, para robustecer el modelo, se ajusta nuevamente, pero agregando la variable zona como se muestra a continuación: \\[ Ingreso_{ij}=\\beta_{0j}+\\beta_{1j}Gasto_{ij}+\\beta_{2j}Zona_{ij} +\\epsilon_{ij} \\] Donde, \\[ \\beta_{0j} = \\gamma_{00}+\\gamma_{01}Stratum_{j} + \\gamma_{02}\\mu_{j} + \\tau_{0j} \\] y, \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\gamma_{12}\\mu_{j} + \\tau_{1j} \\] \\[ \\beta_{2j} = \\gamma_{20}+\\gamma_{21}Stratum_{j} + \\gamma_{12}\\mu_{j} + \\tau_{2j} \\] donde \\(\\mu_{j}\\) es el gasto medio de los hogares en el estrato \\(j\\). En R el ajuste se hace de la siguiente manera: media_estrato &lt;- encuesta %&gt;% group_by(Stratum) %&gt;% summarise(mu = mean(Expenditure)) encuesta &lt;- inner_join(encuesta, media_estrato, by = &quot;Stratum&quot;) mod_Pen_Aleatorio2 &lt;- lmer(Income ~ 1 + Expenditure + Zone + mu + (1 + Expenditure + Zone + mu | Stratum ), data = encuesta, weights = wk2) calculando las predicciones de los ingresos de los hogares por estrato: (tab_pred &lt;- data.frame(Pred = predict(mod_Pen_Aleatorio2), Income = encuesta$Income, Stratum = encuesta$Stratum)) %&gt;% distinct() %&gt;% slice(1:6L) Pred Income Stratum 1 728.163 409.87 idStrt001 6 854.365 823.75 idStrt001 10 -20.446 90.92 idStrt001 13 6.279 135.33 idStrt001 18 491.265 336.19 idStrt001 22 1245.318 1539.75 idStrt001 Por último, haciendo el gráfico de las predicciones con los datos observados observándose que el ajuste de este modelo es levemente mejor que el que no incluye la variable zona. ggplot(data = tab_pred, aes(x = Pred, y = Income, colour = Stratum)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) A continuación, se presentan todos los componentes que se requieren para escribir el modelo ajustado. Inicialmente, se calcula la matriz diseño: as.data.frame( model.matrix(mod_Pen_Aleatorio2)) %&gt;% distinct() (Intercept) Expenditure ZoneUrban mu 1 1 346.34 0 303.3 6 1 392.24 0 303.3 10 1 74.07 0 303.3 13 1 83.79 0 303.3 18 1 260.18 0 303.3 22 1 534.43 0 303.3 28 1 256.74 1 286.2 32 1 233.69 1 286.2 36 1 324.97 1 286.2 41 1 334.75 1 286.2 46 1 109.00 1 286.2 51 1 190.27 1 286.2 55 1 479.55 1 286.2 61 1 108.34 0 270.3 64 1 238.34 0 270.3 68 1 451.59 0 270.3 73 1 207.94 0 270.3 76 1 236.24 0 270.3 80 1 278.17 0 270.3 84 1 383.93 1 569.0 88 1 733.67 1 569.0 92 1 982.88 1 569.0 95 1 492.00 1 569.0 98 1 259.14 1 569.0 101 1 555.88 0 486.1 105 1 420.02 0 486.1 109 1 796.05 0 486.1 116 1 463.88 0 486.1 126 1 240.70 0 486.1 134 1 313.90 1 321.0 138 1 120.23 1 321.0 139 1 223.03 1 321.0 143 1 474.20 1 321.0 151 1 119.81 1 321.0 154 1 28.51 0 376.7 157 1 853.57 0 376.7 161 1 109.46 0 376.7 163 1 520.14 0 376.7 165 1 68.98 0 376.7 167 1 144.99 1 206.4 171 1 93.71 1 206.4 177 1 64.84 1 206.4 179 1 376.18 1 206.4 183 1 132.91 1 206.4 184 1 326.18 1 206.4 189 1 59.68 0 215.9 192 1 376.75 0 215.9 197 1 167.66 0 215.9 202 1 329.07 0 215.9 208 1 140.99 0 215.9 213 1 82.71 0 215.9 216 1 197.36 1 255.8 220 1 302.51 1 255.8 223 1 202.45 1 255.8 226 1 398.71 1 255.8 233 1 141.48 1 255.8 238 1 212.50 1 255.8 239 1 123.80 1 255.8 240 1 862.81 0 483.0 244 1 229.26 0 483.0 250 1 628.40 0 483.0 257 1 178.52 0 483.0 259 1 402.06 0 483.0 264 1 325.49 1 245.0 266 1 95.74 1 245.0 267 1 115.42 1 245.0 269 1 277.43 1 245.0 271 1 320.50 1 245.0 275 1 125.28 1 245.0 276 1 497.55 0 361.9 286 1 271.28 0 361.9 289 1 227.36 0 361.9 293 1 132.85 0 361.9 296 1 492.37 0 361.9 302 1 233.50 0 361.9 307 1 719.16 1 482.1 310 1 376.79 1 482.1 312 1 243.75 1 482.1 314 1 904.78 1 482.1 319 1 189.24 1 482.1 322 1 230.27 1 482.1 327 1 538.66 0 357.0 332 1 218.20 0 357.0 337 1 456.25 0 357.0 342 1 451.70 0 357.0 346 1 68.97 0 357.0 349 1 235.59 0 357.0 352 1 391.82 0 357.0 356 1 117.33 1 334.0 357 1 436.66 1 334.0 359 1 467.95 1 334.0 363 1 119.16 1 334.0 364 1 317.60 1 334.0 367 1 203.95 1 334.0 369 1 296.15 0 244.8 373 1 294.81 0 244.8 379 1 253.68 0 244.8 381 1 220.17 0 244.8 383 1 228.06 0 244.8 387 1 203.12 0 244.8 390 1 91.42 0 244.8 393 1 274.38 0 244.8 399 1 421.27 1 376.3 405 1 330.54 1 376.3 407 1 398.10 1 376.3 411 1 313.67 1 376.3 414 1 379.96 1 376.3 416 1 333.83 1 376.3 418 1 156.71 0 305.8 424 1 540.66 0 305.8 435 1 144.88 0 305.8 438 1 583.90 0 305.8 442 1 301.06 0 305.8 445 1 238.03 1 553.7 448 1 402.60 1 553.7 456 1 447.85 1 553.7 461 1 1348.56 1 553.7 465 1 141.40 0 127.2 467 1 119.50 0 127.2 471 1 74.83 0 127.2 474 1 96.00 0 127.2 475 1 192.33 0 127.2 478 1 125.17 0 127.2 480 1 543.36 1 524.0 484 1 123.76 1 524.0 487 1 76.31 1 524.0 489 1 541.50 1 524.0 493 1 180.18 1 524.0 495 1 712.67 1 524.0 498 1 1041.83 1 524.0 502 1 124.25 0 188.0 505 1 380.66 0 188.0 511 1 130.84 0 188.0 512 1 172.93 0 188.0 516 1 91.83 0 188.0 522 1 154.64 0 188.0 530 1 185.75 0 188.0 531 1 246.33 1 389.3 535 1 296.07 1 389.3 538 1 222.35 1 389.3 542 1 87.29 1 389.3 543 1 938.25 1 389.3 548 1 204.71 1 389.3 553 1 293.31 0 259.0 557 1 89.85 0 259.0 558 1 213.39 0 259.0 561 1 295.43 0 259.0 567 1 414.46 0 259.0 570 1 129.85 0 259.0 574 1 160.82 1 334.1 576 1 219.45 1 334.1 578 1 177.59 1 334.1 582 1 179.17 1 334.1 584 1 636.34 1 334.1 589 1 98.65 0 249.8 592 1 301.72 0 249.8 599 1 303.73 0 249.8 603 1 329.32 0 249.8 604 1 163.09 0 249.8 607 1 277.55 0 249.8 609 1 149.28 1 337.1 613 1 422.71 1 337.1 617 1 693.61 1 337.1 620 1 300.97 1 337.1 623 1 256.02 1 337.1 625 1 454.13 1 337.1 628 1 391.25 1 337.1 633 1 168.52 1 337.1 639 1 331.67 0 238.7 643 1 169.60 0 238.7 645 1 262.26 0 238.7 649 1 116.56 0 238.7 653 1 90.94 0 238.7 655 1 314.83 0 238.7 661 1 115.19 1 282.4 663 1 242.92 1 282.4 667 1 309.83 1 282.4 671 1 261.93 1 282.4 675 1 202.87 1 282.4 678 1 399.54 1 282.4 684 1 123.62 0 263.8 685 1 385.49 0 263.8 688 1 314.09 0 263.8 691 1 342.38 0 263.8 701 1 174.96 0 263.8 705 1 129.47 0 263.8 711 1 123.17 1 455.2 712 1 595.53 1 455.2 715 1 315.00 1 455.2 716 1 138.07 1 455.2 719 1 576.54 1 455.2 722 1 614.68 1 455.2 726 1 304.99 0 218.1 731 1 255.87 0 218.1 735 1 239.59 0 218.1 739 1 96.40 0 218.1 744 1 124.90 0 218.1 746 1 258.01 0 218.1 753 1 165.96 0 218.1 756 1 132.19 1 335.5 760 1 221.19 1 335.5 764 1 323.83 1 335.5 768 1 160.73 1 335.5 771 1 663.81 1 335.5 775 1 416.28 1 335.5 778 1 407.22 1 335.5 782 1 209.71 0 321.9 792 1 405.61 0 321.9 802 1 326.32 0 321.9 805 1 342.52 0 321.9 810 1 377.90 0 321.9 813 1 1530.50 1 686.4 818 1 142.61 1 686.4 819 1 935.17 1 686.4 822 1 273.74 1 686.4 824 1 77.81 1 686.4 825 1 245.40 1 686.4 830 1 555.77 1 686.4 836 1 220.67 0 402.2 839 1 525.84 0 402.2 845 1 254.35 0 402.2 849 1 215.09 0 402.2 853 1 172.29 0 402.2 856 1 568.83 0 402.2 867 1 492.63 1 505.9 871 1 287.21 1 505.9 872 1 526.77 1 505.9 874 1 639.26 1 505.9 879 1 667.66 1 505.9 884 1 272.24 1 505.9 887 1 233.79 1 505.9 889 1 3367.53 1 1311.6 892 1 312.13 1 1311.6 896 1 226.50 1 1311.6 898 1 71.05 0 342.6 900 1 440.57 0 342.6 904 1 516.79 0 342.6 906 1 166.56 0 342.6 908 1 402.71 0 342.6 912 1 321.24 0 342.6 916 1 418.53 1 370.9 918 1 560.13 1 370.9 923 1 282.76 1 370.9 927 1 242.68 1 370.9 931 1 91.33 1 370.9 933 1 516.67 1 370.9 937 1 171.67 1 370.9 938 1 567.89 0 517.1 943 1 380.40 0 517.1 948 1 437.62 0 517.1 952 1 308.07 0 517.1 956 1 912.84 0 517.1 960 1 322.27 1 359.9 965 1 380.67 1 359.9 967 1 349.11 1 359.9 971 1 371.38 1 359.9 976 1 454.20 1 359.9 983 1 172.72 1 359.9 985 1 282.56 1 359.9 987 1 223.55 0 224.4 995 1 147.35 0 224.4 999 1 288.59 0 224.4 1007 1 220.72 0 224.4 1011 1 41.26 0 224.4 1012 1 559.33 1 311.0 1016 1 217.71 1 311.0 1018 1 129.08 1 311.0 1019 1 484.33 1 311.0 1021 1 241.66 1 311.0 1035 1 152.06 0 364.2 1038 1 255.48 0 364.2 1043 1 604.25 0 364.2 1049 1 277.41 0 364.2 1052 1 616.32 1 1124.8 1055 1 1385.65 1 1124.8 1059 1 245.28 1 1124.8 1063 1 1791.50 1 1124.8 1069 1 137.27 0 199.8 1072 1 94.66 0 199.8 1074 1 181.66 0 199.8 1075 1 158.54 0 199.8 1077 1 449.20 0 199.8 1079 1 398.27 1 706.2 1084 1 364.26 1 706.2 1088 1 509.08 1 706.2 1095 1 1053.00 1 706.2 1099 1 1431.41 1 706.2 1103 1 305.82 0 301.7 1108 1 58.25 0 301.7 1110 1 183.36 0 301.7 1112 1 407.40 0 301.7 1120 1 336.90 0 301.7 1125 1 221.86 0 301.7 1129 1 112.09 1 213.0 1131 1 307.20 1 213.0 1135 1 231.25 1 213.0 1138 1 191.77 1 213.0 1141 1 162.08 1 213.0 1143 1 148.33 1 213.0 1144 1 264.78 0 367.2 1152 1 210.43 0 367.2 1156 1 984.08 0 367.2 1160 1 237.22 0 367.2 1165 1 274.55 0 367.2 1169 1 403.28 1 340.3 1172 1 290.73 1 340.3 1175 1 439.26 1 340.3 1180 1 112.06 1 340.3 1181 1 242.05 1 340.3 1182 1 236.34 1 340.3 1184 1 416.35 1 419.8 1188 1 452.00 1 419.8 1191 1 396.57 1 419.8 1192 1 377.49 1 419.8 1193 1 415.43 1 419.8 1197 1 776.23 1 527.1 1199 1 383.07 1 527.1 1201 1 259.07 1 527.1 1204 1 675.53 1 527.1 1208 1 262.04 1 386.3 1211 1 200.33 1 386.3 1212 1 371.04 1 386.3 1215 1 570.77 1 386.3 1217 1 433.46 1 386.3 1222 1 830.90 1 445.3 1225 1 257.00 1 445.3 1227 1 250.20 1 445.3 1231 1 268.25 1 346.4 1232 1 407.97 1 346.4 1236 1 381.56 1 346.4 1239 1 343.00 1 346.4 1244 1 343.99 1 346.4 1249 1 297.27 1 346.4 1254 1 281.26 0 233.9 1258 1 256.92 0 233.9 1260 1 380.08 0 233.9 1263 1 185.74 0 233.9 1266 1 105.79 0 233.9 1272 1 85.18 1 442.3 1274 1 285.13 1 442.3 1278 1 282.25 1 442.3 1283 1 622.50 1 442.3 1288 1 856.20 1 442.3 1291 1 344.24 0 290.0 1294 1 402.75 0 290.0 1299 1 233.12 0 290.0 1304 1 79.20 0 290.0 1310 1 303.00 0 290.0 1314 1 529.55 1 637.7 1318 1 315.60 1 637.7 1322 1 334.90 1 637.7 1328 1 209.62 1 637.7 1331 1 532.26 1 637.7 1339 1 1267.11 1 637.7 1348 1 891.51 1 648.7 1353 1 184.79 1 648.7 1361 1 812.17 1 648.7 1364 1 139.67 1 648.7 1367 1 295.20 0 393.9 1372 1 569.67 0 393.9 1383 1 256.05 0 393.9 1386 1 175.96 0 393.9 1389 1 446.70 0 393.9 1393 1 102.69 0 393.9 1395 1 726.16 1 429.5 1399 1 443.82 1 429.5 1401 1 475.36 1 429.5 1405 1 254.42 1 429.5 1409 1 80.52 1 429.5 1411 1 894.26 1 944.4 1413 1 514.84 1 944.4 1416 1 1412.17 1 944.4 1420 1 974.53 1 944.4 1428 1 799.96 1 944.4 1433 1 244.15 0 448.7 1437 1 132.73 0 448.7 1442 1 188.33 0 448.7 1444 1 442.43 0 448.7 1449 1 947.68 0 448.7 1456 1 312.87 0 448.7 1460 1 332.09 1 462.0 1463 1 471.45 1 462.0 1467 1 221.38 1 462.0 1472 1 480.97 1 462.0 1478 1 606.00 1 462.0 1488 1 330.08 1 558.1 1491 1 680.34 1 558.1 1496 1 636.47 1 558.1 1505 1 346.98 1 558.1 1508 1 377.67 0 378.3 1511 1 298.31 0 378.3 1515 1 468.38 0 378.3 1519 1 393.58 0 378.3 1522 1 338.80 0 378.3 1526 1 402.56 0 378.3 1529 1 292.17 1 278.0 1532 1 245.64 1 278.0 1536 1 274.80 1 278.0 1538 1 324.62 1 278.0 1540 1 300.50 0 244.4 1544 1 335.06 0 244.4 1547 1 86.95 0 244.4 1549 1 143.80 0 244.4 1553 1 295.93 0 244.4 1556 1 241.27 0 244.4 1558 1 262.50 0 244.4 1562 1 429.52 1 482.0 1567 1 709.45 1 482.0 1569 1 769.80 1 482.0 1573 1 258.00 1 482.0 1579 1 160.42 0 181.3 1583 1 140.52 0 181.3 1586 1 173.08 0 181.3 1588 1 122.07 0 181.3 1590 1 130.44 0 181.3 1592 1 196.96 0 181.3 1594 1 283.86 0 181.3 1598 1 425.13 1 312.4 1602 1 222.33 1 312.4 1610 1 138.00 1 312.4 1611 1 258.23 1 312.4 1614 1 244.34 1 312.4 1617 1 537.50 0 381.3 1623 1 365.90 0 381.3 1624 1 237.00 0 381.3 1629 1 123.50 0 381.3 1630 1 395.68 0 381.3 1634 1 289.63 1 368.1 1638 1 537.50 1 368.1 1643 1 374.15 1 368.1 1645 1 299.92 1 368.1 1653 1 447.18 0 312.9 1657 1 116.44 0 312.9 1660 1 249.21 0 312.9 1666 1 234.33 0 312.9 1670 1 645.88 0 312.9 1672 1 489.68 0 312.9 1675 1 88.65 0 312.9 1677 1 230.93 1 325.8 1681 1 461.33 1 325.8 1685 1 216.83 1 325.8 1687 1 576.55 1 325.8 1691 1 197.07 1 325.8 1699 1 337.60 1 325.8 1706 1 117.21 0 254.7 1710 1 210.67 0 254.7 1713 1 243.97 0 254.7 1715 1 594.27 0 254.7 1719 1 164.00 0 254.7 1723 1 157.14 0 254.7 1726 1 435.10 1 408.7 1731 1 262.00 1 408.7 1736 1 269.64 1 408.7 1740 1 265.89 1 408.7 1745 1 748.02 1 408.7 1752 1 382.56 1 408.7 1758 1 234.66 1 408.7 1760 1 216.20 0 202.1 1762 1 87.01 0 202.1 1764 1 177.01 0 202.1 1767 1 150.99 0 202.1 1771 1 349.33 0 202.1 1772 1 257.83 0 202.1 1778 1 365.42 1 586.4 1782 1 179.25 1 586.4 1783 1 766.09 1 586.4 1792 1 612.54 1 586.4 1796 1 516.71 1 586.4 1800 1 434.67 1 586.4 1801 1 278.90 0 205.6 1805 1 174.58 0 205.6 1808 1 87.54 0 205.6 1809 1 252.57 0 205.6 1813 1 112.95 0 205.6 1815 1 177.51 0 205.6 1818 1 233.89 1 343.1 1819 1 421.09 1 343.1 1823 1 388.14 1 343.1 1829 1 297.87 1 343.1 1834 1 123.09 1 343.1 1837 1 425.79 1 343.1 1842 1 123.97 0 316.0 1844 1 528.80 0 316.0 1848 1 322.14 0 316.0 1852 1 228.28 0 316.0 1856 1 306.89 0 316.0 1862 1 298.64 0 316.0 1867 1 295.33 1 323.7 1871 1 346.25 1 323.7 1875 1 91.89 1 323.7 1876 1 54.68 1 323.7 1877 1 286.72 1 323.7 1882 1 337.09 1 323.7 1887 1 430.50 1 323.7 1893 1 174.64 0 179.7 1897 1 64.58 0 179.7 1899 1 361.17 0 179.7 1902 1 113.19 0 179.7 1914 1 305.55 0 179.7 1918 1 287.53 1 457.6 1922 1 1122.09 1 457.6 1924 1 416.34 1 457.6 1930 1 89.09 1 457.6 1932 1 524.63 1 457.6 1937 1 270.68 0 237.1 1943 1 258.34 0 237.1 1954 1 249.76 0 237.1 1958 1 127.70 0 237.1 1961 1 157.92 0 237.1 1963 1 706.28 1 491.5 1969 1 119.92 1 491.5 1971 1 317.23 1 491.5 1974 1 701.09 1 491.5 1980 1 198.17 1 491.5 1982 1 145.12 1 491.5 1984 1 68.24 0 228.0 1988 1 108.86 0 228.0 1991 1 147.13 0 228.0 1993 1 526.50 0 228.0 1996 1 323.93 0 228.0 2000 1 160.00 0 228.0 2004 1 303.32 0 228.0 2006 1 94.48 1 418.9 2007 1 192.51 1 418.9 2011 1 902.79 1 418.9 2017 1 126.78 1 418.9 2019 1 333.77 1 418.9 2024 1 198.09 1 418.9 2027 1 306.30 0 187.1 2030 1 172.70 0 187.1 2035 1 189.83 0 187.1 2039 1 105.99 0 187.1 2042 1 191.91 0 187.1 2045 1 153.37 0 187.1 2047 1 157.02 1 222.7 2051 1 94.54 1 222.7 2054 1 158.63 1 222.7 2057 1 367.00 1 222.7 2060 1 209.60 1 222.7 2063 1 371.21 1 222.7 2066 1 355.19 0 195.9 2069 1 90.92 0 195.9 2071 1 112.20 0 195.9 2073 1 95.12 0 195.9 2074 1 328.00 0 195.9 2076 1 107.76 0 195.9 2079 1 317.76 1 379.2 2081 1 641.38 1 379.2 2084 1 459.73 1 379.2 2087 1 237.09 1 379.2 2091 1 294.93 1 379.2 2095 1 404.02 0 300.2 2098 1 76.07 0 300.2 2103 1 149.65 0 300.2 2108 1 520.16 0 300.2 2115 1 107.78 0 300.2 2120 1 326.53 1 289.5 2124 1 227.56 1 289.5 2127 1 376.33 1 289.5 2129 1 304.31 1 289.5 2133 1 94.33 1 289.5 2134 1 132.19 0 226.2 2139 1 313.16 0 226.2 2140 1 117.66 0 226.2 2142 1 380.93 0 226.2 2145 1 277.31 0 226.2 2148 1 220.55 0 226.2 2151 1 447.99 1 446.1 2154 1 249.19 1 446.1 2157 1 338.90 1 446.1 2160 1 790.23 1 446.1 2164 1 289.25 1 446.1 2167 1 317.20 0 239.6 2173 1 169.03 0 239.6 2176 1 334.50 0 239.6 2185 1 241.51 0 239.6 2190 1 105.40 0 239.6 2196 1 135.06 0 239.6 2199 1 424.40 1 338.3 2203 1 526.03 1 338.3 2208 1 304.45 1 338.3 2211 1 234.08 1 338.3 2215 1 196.95 1 338.3 2216 1 257.20 1 338.3 2220 1 238.98 1 338.3 2223 1 90.53 0 120.4 2226 1 83.24 0 120.4 2233 1 184.50 0 120.4 2236 1 146.50 0 120.4 2244 1 112.26 0 120.4 2248 1 115.67 0 120.4 2252 1 113.16 1 642.6 2255 1 377.33 1 642.6 2258 1 169.75 1 642.6 2265 1 381.46 1 642.6 2267 1 1437.12 1 642.6 2272 1 296.91 1 642.6 2282 1 202.09 0 533.3 2285 1 568.06 0 533.3 2292 1 190.82 0 533.3 2296 1 365.37 0 533.3 2301 1 116.77 0 533.3 2303 1 1797.58 0 533.3 2306 1 1166.35 1 707.5 2313 1 362.25 1 707.5 2316 1 417.98 1 707.5 2320 1 667.99 1 707.5 2325 1 297.33 1 707.5 2327 1 151.10 0 216.5 2330 1 111.08 0 216.5 2331 1 256.29 0 216.5 2335 1 159.94 0 216.5 2343 1 326.43 0 216.5 2349 1 134.71 0 216.5 2355 1 301.62 0 216.5 2360 1 264.17 1 253.6 2361 1 229.07 1 253.6 2365 1 174.67 1 253.6 2366 1 340.25 1 253.6 2369 1 196.44 1 253.6 2373 1 276.20 1 253.6 2379 1 294.95 0 332.9 2382 1 193.67 0 332.9 2385 1 166.44 0 332.9 2389 1 418.02 0 332.9 2396 1 167.79 0 332.9 2398 1 619.35 0 332.9 2402 1 226.12 0 332.9 2404 1 807.67 1 523.6 2409 1 408.00 1 523.6 2411 1 525.15 1 523.6 2415 1 101.90 1 523.6 2416 1 136.73 1 523.6 2418 1 176.89 0 180.5 2422 1 244.16 0 180.5 2431 1 128.21 0 180.5 2432 1 163.48 0 180.5 2446 1 445.75 1 330.0 2449 1 173.43 1 330.0 2451 1 273.75 1 330.0 2454 1 431.55 1 330.0 2458 1 262.24 1 330.0 2462 1 856.47 0 542.9 2466 1 128.53 0 542.9 2470 1 459.77 0 542.9 2478 1 89.47 0 542.9 2479 1 124.84 0 542.9 2482 1 1005.74 0 542.9 2488 1 800.53 1 625.2 2496 1 434.22 1 625.2 2504 1 791.03 1 625.2 2511 1 133.67 1 625.2 2512 1 380.70 1 625.2 2514 1 836.26 1 625.2 2520 1 360.85 1 625.2 2525 1 313.97 0 276.7 2529 1 245.33 0 276.7 2535 1 321.27 0 276.7 2540 1 215.69 0 276.7 2543 1 293.89 1 647.7 2545 1 168.45 1 647.7 2546 1 497.00 1 647.7 2552 1 575.38 1 647.7 2556 1 945.34 1 647.7 2564 1 151.50 0 293.3 2566 1 462.25 0 293.3 2572 1 485.27 0 293.3 2575 1 143.98 0 293.3 2576 1 214.24 0 293.3 2582 1 122.45 0 293.3 2586 1 391.67 1 354.7 2590 1 388.67 1 354.7 2594 1 474.78 1 354.7 2597 1 309.80 1 354.7 2603 1 229.54 1 354.7 Agregando la variable estrato y así poder incorporar las zonas a la matriz de coeficientes, (Coef_Estimado &lt;- inner_join( coef(mod_Pen_Aleatorio2)$Stratum %&gt;% add_rownames(var = &quot;Stratum&quot;), encuesta_plot %&gt;% dplyr::select(Stratum, Zone) %&gt;% distinct() )) Stratum (Intercept) Expenditure ZoneUrban mu Zone idStrt002 53.18 1.593 28.20 -0.1272 Urban idStrt010 97.11 1.984 145.69 -0.6724 Urban idStrt015 36.22 1.754 -149.89 -0.0381 Rural idStrt017 55.62 1.578 43.92 -0.1375 Rural idStrt022 41.14 1.133 27.03 0.2696 Urban idStrt028 53.87 1.568 -81.18 -0.0079 Urban Ahora bien, se agrega la media calculada para los estratos a la matriz de coeficientes: (Coef_Estimado&lt;- Coef_Estimado %&gt;% inner_join(media_estrato, by = &quot;Stratum&quot;)) Stratum (Intercept) Expenditure ZoneUrban mu.x Zone mu.y idStrt002 53.18 1.593 28.20 -0.1272 Urban 286.2 idStrt010 97.11 1.984 145.69 -0.6724 Urban 255.8 idStrt015 36.22 1.754 -149.89 -0.0381 Rural 357.0 idStrt017 55.62 1.578 43.92 -0.1375 Rural 244.8 idStrt022 41.14 1.133 27.03 0.2696 Urban 524.0 idStrt028 53.87 1.568 -81.18 -0.0079 Urban 337.1 Por último, el modelo para el estrato idStrt002 viene dado por: \\[ \\hat{y}_{ij}=51.1+1.59Expenditure_{ij}+28.98Zone_{ij}+\\left(-0.12\\right)\\mu_{j} \\] Escribiendo el anterior modelo en R se tiene: (Coef_Estimado %&gt;% dplyr::mutate(B0 = ifelse( Zone == &quot;Urban&quot;, `(Intercept)` + mu.y * mu.x + ZoneUrban,`(Intercept)` + mu.y * mu.x)) %&gt;% dplyr::select(Stratum, Zone, B0, Expenditure)) Stratum Zone B0 Expenditure idStrt002 Urban 44.97 1.593 idStrt010 Urban 70.82 1.984 idStrt015 Rural 22.61 1.754 idStrt017 Rural 21.97 1.578 idStrt022 Urban 209.46 1.133 idStrt028 Urban -29.97 1.568 Gráficamente, ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure, colour = Stratum)) + geom_jitter() + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) + facet_grid( ~ Zone) + geom_abline( data = Coef_Estimado, mapping = aes( slope = Expenditure, intercept = B0, colour = Stratum ) ) + theme_cepal() "],["introducción-a-los-modelos-logístico-multinivel..html", "10.3 Introducción a los modelos logístico multinivel.", " 10.3 Introducción a los modelos logístico multinivel. Los modelos logísticos multinivel son una extensión de los modelos logísticos simples, que se utilizan para predecir la probabilidad de un resultado binario en función de una o varias variables explicativas. Sin embargo, en muchas situaciones, los datos se recogen de individuos que están agrupados en diferentes niveles o unidades de análisis, como escuelas, ciudades o países. En estos casos, los modelos logísticos simples pueden no ser suficientes para capturar la estructura jerárquica de los datos y la variación en las respuestas entre los diferentes grupos. Los modelos logísticos multinivel resuelven este problema al permitir que los coeficientes del modelo varíen a través de los diferentes niveles de análisis. En otras palabras, se permite que la relación entre las variables predictoras y la respuesta varíe en función del grupo al que pertenece cada individuo. Además, los modelos logísticos multinivel permiten incluir tanto variables a nivel individual como variables a nivel de grupo, lo que aumenta la precisión de las estimaciones y la capacidad de explicar la variabilidad en las respuestas. También permiten estimar la varianza en las respuestas entre los diferentes grupos, lo que es útil para identificar las fuentes de variabilidad y para comparar la variabilidad entre grupos. En general, los modelos logísticos multinivel son una herramienta poderosa para analizar datos de respuestas binarias en contextos jerárquicos, y son ampliamente utilizados en muchas áreas de investigación, como la educación, la salud, las ciencias sociales y la psicología. Sea la variable \\(y_{ij} = 1\\) si el individuo \\(i\\) en el estrato \\(j\\) está por encima de la línea de pobreza y \\(y_{ij} = 0\\) en caso contrario, la variable \\(y_{ij}\\) se puede modelar mediante el modelo logístico: \\[ Pr\\left(y_{ij}\\right)=Pr\\left(y_{ij}=1\\mid x_{i}:\\boldsymbol{\\beta}\\right)=\\frac{1}{1+\\exp\\left(\\boldsymbol{-\\beta}_{j}\\boldsymbol{x}_{ij}\\right)} \\] ó \\[ \\log\\left(\\frac{\\pi_{ij}}{1-\\pi_{ij}}\\right)=\\boldsymbol{\\beta}_{j}\\boldsymbol{x}_{ij} \\] donde, \\[ \\pi_{ij}=Pr\\left(y_{ij}=1\\mid x_{i}:\\boldsymbol{\\beta}\\right) \\]. A modo de ejemplo, se ajustará un modelo logístico para la variable pobreza. Inicialmente, se crea la variable dicotómica en la base como se muestra a continuación: encuesta_plot &lt;- encuesta %&gt;% dplyr::select(Stratum,Expenditure) %&gt;% unique() %&gt;% group_by(Stratum) %&gt;% summarise(sd = sd(Expenditure)) %&gt;% arrange(desc(sd)) %&gt;% dplyr::select(-sd) %&gt;% slice(1:20L) %&gt;% inner_join(encuesta) %&gt;% dplyr::select(Poverty, Expenditure, Stratum, Sex, Region, Zone) encuesta_plot %&gt;% slice(1:15L) Poverty Expenditure Stratum Sex Region Zone NotPoor 3367.5 idStrt039 Male Sur Urban NotPoor 3367.5 idStrt039 Female Sur Urban NotPoor 3367.5 idStrt039 Male Sur Urban NotPoor 312.1 idStrt039 Female Sur Urban NotPoor 312.1 idStrt039 Female Sur Urban NotPoor 312.1 idStrt039 Female Sur Urban NotPoor 312.1 idStrt039 Male Sur Urban NotPoor 226.5 idStrt039 Male Sur Urban NotPoor 226.5 idStrt039 Female Sur Urban NotPoor 616.3 idStrt047 Female Sur Urban NotPoor 616.3 idStrt047 Female Sur Urban NotPoor 616.3 idStrt047 Female Sur Urban NotPoor 1385.7 idStrt047 Male Sur Urban NotPoor 1385.7 idStrt047 Female Sur Urban NotPoor 1385.7 idStrt047 Female Sur Urban Para poder observar la distribución la distribución de la variable pobreza, se presenta el siguiente gráfico: encuesta &lt;- encuesta %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0)) encuesta_plot %&lt;&gt;% mutate(pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0)) ggplot(data = encuesta, aes(y = pobreza, x = Expenditure)) + geom_point() + geom_smooth(formula = y~x, method = &quot;glm&quot;,se=FALSE, method.args = list(family=binomial(link = &quot;logit&quot;))) + theme_bw() El ajuste del modelo logístico se realiza con la función glm y la función link “logit”. Una vez se ajusta el modelo, se extraen los coeficientes del modelo y así poder calcular las probabilidades, como sigue a continuación: auxLogit &lt;- function(x,b0,b1){ 1/(1+exp(-(b0+b1*x))) } B0 = coef(glm(pobreza~1,data = encuesta_plot, family=binomial(link = &quot;logit&quot;))) (coef_Mod &lt;- encuesta_plot %&gt;% group_by(Stratum) %&gt;% summarise(B1 = coef(glm(pobreza ~ -1 + Expenditure, family=binomial(link = &quot;logit&quot;)))) %&gt;% mutate(B0 = B0)) %&gt;% slice(1:6L) Stratum B1 B0 idStrt007 -0.0189 -0.8782 idStrt020 -0.0010 -0.8782 idStrt022 -0.0057 -0.8782 idStrt024 -0.0020 -0.8782 idStrt036 -0.0009 -0.8782 idStrt039 -0.0976 -0.8782 A continuación, se grafican los diferentes modelos logísticos ajustados para cada uno de los estratos observándose que, hay una variación importante entre los estratos: pred_logit &lt;- coef_Mod %&gt;% mutate(Expenditure = list(seq(0,2000, length =100))) %&gt;% tidyr::unnest_legacy() pred_logit %&lt;&gt;% mutate(Prob = auxLogit(Expenditure,B0,B1)) ggplot(data = pred_logit, aes(y = Prob, x = Expenditure, colour = Stratum)) + geom_line() + theme_bw() + theme(legend.position = &quot;none&quot;) Un modelo logístico básico o nulo se escribe de la siguiente manera: \\[ logit( \\pi_{ij})=\\beta_{0j}+\\epsilon_{ij} \\] \\[ \\beta_{0j}=\\gamma_{00}+\\tau_{0j} \\] Donde los componentes son los siguientes: \\(\\pi_{ij}=Pr\\left(y_{ij}=1\\mid x_{i}:\\boldsymbol{\\beta}\\right)\\). \\(\\beta_{0j}=\\) El intercepto en el estrato \\(j\\). \\(\\epsilon_{ij}\\) El residual de la persona \\(i\\) en el estrato \\(j\\). \\(\\gamma_{00}=\\) El intercepto en general. \\(\\tau_{0j}=\\) Efecto aleatorio para el intercepto. con, \\(\\tau_{0j}\\sim N\\left(0,\\sigma_{\\tau}^{2}\\right)\\) y \\(\\epsilon_{ij}\\sim N\\left(0,\\sigma_{\\epsilon}^{2}\\right)\\). En este caso, la correlación intra clásica está dada por: \\[ \\rho=\\frac{\\sigma_{\\tau}^{2}}{\\sigma_{\\tau}^{2}+\\sigma_{\\epsilon}^{2}} \\] En R el modelo nulo se ajusta de la siguiente manera: library(lme4) mod_logist_null &lt;- glmer( pobreza ~ ( 1 | Stratum ), data = encuesta, weights = wk2, family = binomial(link = &quot;logit&quot;) ) coef( mod_logist_null )$Stratum %&gt;% slice(1:12) (Intercept) idStrt001 -0.8334 idStrt002 -0.0133 idStrt003 -2.6023 idStrt004 -2.7770 idStrt005 -1.0268 idStrt006 1.0100 idStrt007 -1.0134 idStrt008 0.2035 idStrt009 2.1966 idStrt010 -0.5948 idStrt011 -1.2986 idStrt012 0.2825 Las estadísticas resumen del modelo se presentan a continuación: library(sjstats) mod_logist_null ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: pobreza ~ (1 | Stratum) ## Data: encuesta ## Weights: wk2 ## AIC BIC logLik -2*log(L) df.resid ## 2966 2978 -1481 2962 2603 ## Random effects: ## Groups Name Std.Dev. ## Stratum (Intercept) 1.29 ## Number of obs: 2605, groups: Stratum, 119 ## Fixed Effects: ## (Intercept) ## -0.802 A continuación, se presenta la correlación intraclase, las predicciones del modelo y las variables observadas: performance::icc(mod_logist_null) ICC_adjusted ICC_unadjusted optional 0.3342 0.3342 FALSE (tab_pred &lt;- data.frame(Pred = predict(mod_logist_null, type = &quot;response&quot;), pobreza = encuesta$pobreza, Stratum = encuesta$Stratum)) %&gt;% distinct() %&gt;% slice(1:6L) # Son las pendientes aleatorias Pred pobreza Stratum 1 0.3029 0 idStrt001 10 0.3029 1 idStrt001 28 0.4967 1 idStrt002 36 0.4967 0 idStrt002 61 0.0690 0 idStrt003 84 0.0586 0 idStrt004 Para efecto de verificar qué tan buena fueron las predicciones del modelo, se estiman el porcentaje de pobreza de la variable observada y de las predicciones de modelo utilizando la función weighted.mean. Se logra observar que son muy similares las estimaciones: weighted.mean(encuesta$pobreza, encuesta$wk2) ## [1] 0.3859 weighted.mean(tab_pred$Pred, encuesta$wk2) ## [1] 0.385 Modelo con intercepto aleatoria EL modelo se define de la siguiente manera: \\[ logit(\\pi_{ij})=\\beta_{0}+\\beta_{1j}Gasto_{ij}+\\epsilon_{ij} \\] Donde, \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\tau_{1j} \\] Siguiendo las ideas de la sección anterior, el ajuste del modelo en R se realiza de la siguiente manera: mod_logit_Int_Aleatorio &lt;- glmer(pobreza ~ Expenditure + (1 | Stratum), data = encuesta, family = binomial(link = &quot;logit&quot;),weights = wk2) performance::icc(mod_logit_Int_Aleatorio) ICC_adjusted ICC_unadjusted optional 0.3151 0.1867 FALSE Los coeficientes estimados son: coef(mod_logit_Int_Aleatorio)$Stratum %&gt;% slice(1:10L) (Intercept) Expenditure idStrt001 0.9889 -0.0066 idStrt002 1.8837 -0.0066 idStrt003 -0.7463 -0.0066 idStrt004 -0.1484 -0.0066 idStrt005 1.7155 -0.0066 idStrt006 3.2456 -0.0066 idStrt007 0.5601 -0.0066 idStrt008 1.6848 -0.0066 idStrt009 3.9332 -0.0066 idStrt010 1.1207 -0.0066 Gráficamente, los modelos ajustados se muestran a continuación: dat_pred &lt;- encuesta %&gt;% group_by(Stratum) %&gt;% summarise(Expenditure = list(seq(min(Expenditure), max(Expenditure), len = 100))) %&gt;% tidyr::unnest_legacy() dat_pred &lt;- mutate(dat_pred,Proba = predict(mod_logit_Int_Aleatorio, newdata = dat_pred , type = &quot;response&quot;)) ggplot(data = dat_pred, aes(y = Proba, x = Expenditure, colour = Stratum)) + geom_line()+ theme_bw() + geom_point(data = encuesta, aes(y = pobreza, x = Expenditure))+ theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) Las predicciones del modelo se presentan a continuación: (tab_pred &lt;- data.frame(Pred = predict(mod_logit_Int_Aleatorio, type = &quot;response&quot;), pobreza = encuesta$pobreza, Stratum = encuesta$Stratum, wk2 = encuesta$wk2)) %&gt;% distinct() %&gt;% slice(1:6L) Pred pobreza Stratum wk2 0.2149 0 idStrt001 0.7770 0.2149 0 idStrt001 0.7501 0.2149 0 idStrt001 0.7463 0.2149 0 idStrt001 0.7717 0.2149 0 idStrt001 0.7438 0.1682 0 idStrt001 0.7507 Como se indicó anteriormente, para verificar la calidad del modelo se realizan las estimaciones de las predicciones y de las variables observadas, teniendo estimaciones similares: tab_pred %&gt;% summarise(Pred = weighted.mean(Pred, wk2), pobreza = weighted.mean(pobreza,wk2)) Pred pobreza 0.3855 0.3859 Modelo con intercepto y pendiente aleatoria Los modelos logísticos con intercepto y pendiente aleatoria son un tipo de modelo logístico multinivel que permiten que tanto el intercepto como la pendiente varíen aleatoriamente entre los diferentes grupos de observación. En los modelos logísticos básicos, la relación entre las variables predictoras y la variable de respuesta se modela mediante una función logística, donde la respuesta es la probabilidad de que el resultado binario ocurra. En los modelos con intercepto y pendiente aleatoria, la función logística se ajusta para cada grupo de observación, y tanto el intercepto como la pendiente son variables aleatorias que varían de un grupo a otro. Esto permite que los coeficientes del modelo, que representan la relación entre las variables predictoras y la respuesta, varíen según el grupo de observación. La incorporación de coeficientes aleatorios en los modelos logísticos multinivel permite capturar la heterogeneidad en la relación entre las variables predictoras y la respuesta en diferentes grupos de observación, y mejora la precisión de las estimaciones. Además, estos modelos también permiten la inclusión de variables a nivel individual y a nivel de grupo, lo que permite una mejor comprensión de la estructura jerárquica de los datos. El modelo se define de la siguiente manera: \\[ logit(\\pi_{ij})=\\beta_{0j}+\\beta_{1j}Gasto_{ij}+\\epsilon_{ij} \\] con, \\[ \\beta_{0j} = \\gamma_{00}+\\gamma_{01}Stratum_{j} + \\tau_{0j} \\] donde, \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\tau_{1j} \\] En R, el ajuste se hace de la siguiente manera: mod_logit_Pen_Aleatorio &lt;- glmer(pobreza ~ Expenditure + (1 + Expenditure| Stratum), data = encuesta, weights = wk2, binomial(link = &quot;logit&quot;)) performance::icc(mod_logit_Pen_Aleatorio) ICC_adjusted ICC_unadjusted optional 0.8859 0.6534 FALSE Los coeficientes del modelo son: coef(mod_logit_Pen_Aleatorio)$Stratum %&gt;% slice(1:10L) (Intercept) Expenditure idStrt001 5.244 -0.0271 idStrt002 11.059 -0.0394 idStrt003 -1.614 -0.0060 idStrt004 1.655 -0.0153 idStrt005 9.055 -0.0289 idStrt006 -1.354 0.0100 idStrt007 1.035 -0.0136 idStrt008 1.473 -0.0056 idStrt009 4.050 -0.0048 idStrt010 4.310 -0.0214 Gráficamente el ajuste de los modelo se muestra a continuación: dat_pred &lt;- encuesta %&gt;% group_by(Stratum) %&gt;% summarise(Expenditure = list(seq(min(Expenditure), max(Expenditure), len = 100))) %&gt;% tidyr::unnest_legacy() dat_pred &lt;- mutate(dat_pred,Proba = predict(mod_logit_Pen_Aleatorio, newdata = dat_pred , type = &quot;response&quot;)) ggplot(data = dat_pred, aes(y = Proba, x = Expenditure, colour = Stratum)) + geom_line()+ theme_bw() + geom_point(data = encuesta, aes(y = pobreza, x = Expenditure))+ theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) Las predicciones se muestran a continuación: (tab_pred &lt;- data.frame( Pred = predict(mod_logit_Pen_Aleatorio, type = &quot;response&quot;), pobreza = encuesta$pobreza, Stratum = encuesta$Stratum, wk2 = encuesta$wk2)) %&gt;% distinct() %&gt;% slice(1:6L) Pred pobreza Stratum wk2 0.0154 0 idStrt001 0.7770 0.0154 0 idStrt001 0.7501 0.0154 0 idStrt001 0.7463 0.0154 0 idStrt001 0.7717 0.0154 0 idStrt001 0.7438 0.0045 0 idStrt001 0.7507 La calidad de la predicción del modelo es muy buena como se muestra a continuación: tab_pred %&gt;% summarise(Pred = weighted.mean(Pred, wk2), pobreza = weighted.mean(pobreza,wk2)) Pred pobreza 0.3845 0.3859 Por otro lado, se ajusta un modelo agregando ahora la variable zona. La idea es entonces medir el porcentaje de pobreza discriminando por zona. El modelo es el siguiente: \\[ logit(\\pi_{ij})=\\beta_{0j}+\\beta_{1j}Gasto_{ij}+\\beta_{2j}Zona_{ij} +\\epsilon_{ij} \\] Donde, \\[ \\beta_{0j} = \\gamma_{00}+\\gamma_{01}Stratum_{j} + \\gamma_{02}\\mu_{j} + \\tau_{0j} \\] con, \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\gamma_{12}\\mu_{j} + \\tau_{1j} \\] y, \\[ \\beta_{2j} = \\gamma_{20}+\\gamma_{21}Stratum_{j} + \\gamma_{12}\\mu_{j} + \\tau_{2j} \\] donde \\(\\mu_{j}\\) es el gasto medio en el estrato \\(j\\). El ajuste del modelo es el siguiente: mod_logit_Pen_Aleatorio2 &lt;- glmer( pobreza ~ 1 + Expenditure + Zone + mu + (1 + Expenditure + Zone + mu | Stratum ), data = encuesta, weights = wk2, binomial(link = &quot;logit&quot;)) performance::icc(mod_logit_Pen_Aleatorio2) ## [1] NA Se grafican los modelos ajustados anteriormente: dat_pred &lt;- encuesta %&gt;% group_by(Stratum, Zone, mu) %&gt;% summarise( Expenditure = list(seq(min(Expenditure), max(Expenditure), len = 100))) %&gt;% tidyr::unnest_legacy() dat_pred$Proba = predict(mod_logit_Pen_Aleatorio2, newdata = dat_pred , type = &quot;response&quot;) ggplot(data = dat_pred, aes(y = Proba, x = Expenditure, colour = Stratum)) + geom_line()+ theme_bw() +facet_grid(.~Zone)+ geom_point(data = encuesta, aes(y = pobreza, x = Expenditure))+ theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) Se logra observar que, hay una variación importante en el ajuste de los modelos para cada zona. Ahora bien, las predicciones del porcentaje de pobreza por zona se calculan a continuación: (tab_pred &lt;- data.frame( Pred = predict(mod_logit_Pen_Aleatorio2, type = &quot;response&quot;), pobreza = encuesta$pobreza, Stratum = encuesta$Stratum, Zone = encuesta$Zone, wk2 = encuesta$wk2)) %&gt;% distinct() %&gt;% slice(1:6L) Pred pobreza Stratum Zone wk2 0.0123 0 idStrt001 Rural 0.7770 0.0123 0 idStrt001 Rural 0.7501 0.0123 0 idStrt001 Rural 0.7463 0.0123 0 idStrt001 Rural 0.7717 0.0123 0 idStrt001 Rural 0.7438 0.0035 0 idStrt001 Rural 0.7507 Por último, se verifica la calidad de las predicciones, obteniendo, como en los modelos anteriores, unas predicciones de buena calidad haciendo las comparaciones con las estimaciones de la variable observada para cada una de las zonas. tab_pred %&gt;% group_by(Zone) %&gt;% summarise(Pred = weighted.mean(Pred, wk2), pobreza = weighted.mean(pobreza,wk2)) Zone Pred pobreza Rural 0.4283 0.4298 Urban 0.3408 0.3437 "],["imputación-múltiple-en-encuestas-de-hogares.html", "Capítulo 11 Imputación múltiple en encuestas de hogares", " Capítulo 11 Imputación múltiple en encuestas de hogares La no respuesta en encuestas de hogares es un fenómenos que desde siempre ha sucedido, más ahora después de la pandemia del Covid -19. Esto sucede por muchas razones, por ejemplo, la longitud de los cuestionarios en las encuestas, el no conocer algunas características particulares del hogar, la renuencia por entregar información sensible, entre otras. Si la no respuesta en algún indicador del estudio es muy alto, puede poner en riesgo la calidad de las estimaciones que se obtienen utilizando los estimadores clásicos. Debido a este problema, se ha avanzado en la literatura especializada metodologías que permiten atacar este problema, el cual es una realidad en todas las encuestas de hogares en los distintos países. 427 En este sentido, alguno de los avances en la teoría de los métodos en el contexto de muestras comlejas son: imputación múltiple (MI; Van Buuren, 2012; Carpenter y Kenward, 2013; Berglund y Heeringa, 2014; Stata Corp, 2015; Raghunathan, 2016). También se encuentra en la literatura métodos como el de imputación fraccional (FI; Kim y Fuller, 2004; Kim y Shao, 2014), el Bootstrap Bayesiano de Población Finita (Zhou et al., 2016a,b), y métodos de máxima verosimilitud como los de Chambers et al., (2012). El objetivo principal de este capítulo es abordar el problema de la falta de datos en encuestas de hogares, revisar las posibles causas, el impacto que tiene en la estimación de los indicadores, y mostrar algunas soluciones a la falta de datos en las encuesta. Siguiendo las ideas anteriores, sea \\(\\boldsymbol{X}_{n \\times p} = x_{ij}\\) una matriz completa (sin valores perdidos) de tal forma que \\(X_{ij}\\) es el valor de la variable \\(j\\) con \\(j=1, \\dots, p\\) e \\(i\\) con \\(i=1, \\dots, n\\). Adicionalmente, se define \\(\\boldsymbol{M}_{n \\times p} = m_{ij}\\) una matriz indicadoradonde \\(m_{ij} = 1\\) si el valor de \\(x_{ij}\\) es un dato perdido y \\(m_{ij}=0\\) si \\(x_{ij}\\) está presente. Ahora bie, note que la matriz \\(M\\) describe el patrón de missing (datos faltantes), y su media marginal de columna puede ser interpretada como la probabilidad de que \\(x_{ij}\\) sea missing. A continuación, se describen alguna de las particularidades de la matriz \\(\\boldsymbol{M}_{n \\times p}\\): La matriz \\(\\boldsymbol{M}_{n \\times p}\\) presenta un comportamiento completamente al azar (MCAR): si la probabilidad de respuesta es independiente de las variables observadas y de las no observadas completamente. El mecanismo de pérdida es ignorable tanto para inferencias basadas en muestreo como en máxima verosimilitud. Los valores de la matriz \\(\\boldsymbol{M}_{n \\times p}\\) son al azar (MAR): si la probabilidad de respuesta es independiente de las variables no observadas completamente y no de las observadas. El mecanismo de pérdida es ignorable para inferencias basadas en máxima verosimilitud. Los datos no están perdidos al azar (MNAR): si la probabilidad de respuesta no es independiente de las variables no observadas completamente y posiblemente, también, de las observadas. El mecanismo de pérdida es no ignorable. En las dos figuras siguientes, se ilustran los casos de observaciones perdidas de manera aleatoria y con un patrón identificado: Como se ha venido trabajando en los capítulos anteriores, primero carguemos la base de datos con la muestra seleccionada y con el fin de poder ejemplificar el tratamiento de datos faltantes, se incluiran manualmente “valores perdidos”. En este sentido, la lectura de la base se hará a continuación: encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) Se filtran encuestados mayores a 15 años y se calcula la proporción de la población desempleada, inactiva y empleada antes de generar los valores faltantes. Pero antes de eso, se cargan todas las librerías que se utilizarán en este capítulo: knitr::opts_chunk$set(cache = FALSE, warning = FALSE, message = FALSE, error = FALSE) #options(digits = 4) options(tinytex.verbose = TRUE) library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(broom) library(jtools) library(modelsummary) library(patchwork) library(ggplot2) Cálculo de las proprociones de personas desempleadas, inactivas y empleados: encuesta&lt;- encuesta |&gt; filter(Age &gt;= 15) (tab_antes &lt;- prop.table(table(encuesta$Employment))) Unemployed Inactive Employed 0.0409792 0.373603 0.5854178 También se calcula el promedio de ingresos en al muestra: (med_antes &lt;- mean(encuesta$Income, na.rm = TRUE)) ## [1] 604.2494 Luego de los conteos anteriores, se genera un 20% de valores faltantes siguiendo un esquema MCAR como sigue: set.seed(1234) encuesta_MCAR &lt;- sample_frac(encuesta, 0.8 ) dat_plot &lt;- bind_rows( list(encuesta_MCAR = encuesta_MCAR, encuesta = encuesta), .id = &quot;Caso&quot; ) Ahora bien, para poder ver el efecto de la inclusión de datos faltantes de manera gráfica por zona y sexo para la variable ingreso, se realizan las siguientes gráficas: p1 &lt;- ggplot(dat_plot, aes(x=Zone, y = Income)) + geom_boxplot() + facet_grid(.~Caso) + theme_bw()+ geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot, aes(x=Sex, y = Income)) + geom_boxplot() + facet_grid(.~Caso) +theme_bw()+ geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) library(patchwork) p1|p2 Como se puede observar en las gráficas anteriores, la distribución de los ingresos por Zona y Sexo se mantiene similar con o sin presencia de la no respuesta. Esto se debe a que la no respuesta que se incluyó no depende de la variable de estudio. Ahora bien, analizando la variable de interés se tiene que tampoco hay cambios distribucionales notables entre las distribuciones con y sin datos faltantes por sexo, como se puede observar a continuación: p1 &lt;- ggplot(dat_plot, aes(x = Income, fill = Caso)) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot, aes(x = Income, fill = Caso)) + geom_density(alpha = 0.3) + facet_grid(.~Sex) + theme_bw()+ geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) + theme(legend.position = &quot;none&quot;) (p1/p2) Si graficamos ahora la varibale gastos, se tienen los mismos resultados que para ingresos. p1 &lt;- ggplot(dat_plot, aes(x = Expenditure, fill = Caso)) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot, aes(x = Expenditure, fill = Caso)) + geom_density(alpha = 0.3) + facet_grid(.~Sex) + theme_bw()+ geom_vline(xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) + theme(legend.position = &quot;none&quot;) (p1/p2) Por otro lado, simulemos ahora una pérdida de información MAR como sigue: library(TeachingSampling) set.seed(1234) temp_estrato &lt;- paste0(encuesta$Zone, encuesta$Sex) table(temp_estrato) RuralFemale RuralMale UrbanFemale UrbanMale 481 428 531 439 sel &lt;- S.STSI(S = temp_estrato, Nh = c(481,428,531,439), nh = c(20, 380, 20,280)) encuesta_MAR &lt;- encuesta[-sel,] dat_plot2 &lt;- bind_rows( list(encuesta_MAR = encuesta_MAR, encuesta = encuesta), .id = &quot;Caso&quot; ) El código anterior utiliza la librería TeachingSampling para realizar un muestreo aleatorio estratificado. Primero, se establece la semilla aleatoria en 1234 para asegurarse de que los resultados sean reproducibles. A continuación, se crea una variable llamada temp_estrato que combina dos variables de la encuesta “Zone” y “Sex” utilizando la función “paste0” para crear grupos de estratos. La función table se usa para mostrar la frecuencia de cada estrato. Luego, se realiza el muestreo estratificado utilizando la función S.STSI que toma los siguientes argumentos: “S”: el vector de estratos creado anteriormente “Nh”: el número de unidades en cada estrato (en este caso, 469, 411, 510 y 390) “nh”: el tamaño de muestra deseado para cada estrato (en este caso, 20, 380, 20 y 280) El resultado del muestreo estratificado es un vector de índices de fila que corresponden a las observaciones seleccionadas para la muestra. Luego, se crea un nuevo conjunto de datos llamado “encuesta_MAR” que excluye las observaciones seleccionadas en la muestra. Finalmente, se usa la función bind_rows del paquete dplyr para unir los dos conjuntos de datos, “encuesta” y “encuesta_MAR”, en un solo conjunto de datos llamado “dat_plot2”, con una nueva variable “Caso” que indica el caso de cada observación en el conjunto de datos. Observemos gráficamente el efecto de la perdida de dinformación en una encuesta en un esquema MAR: p1 &lt;- ggplot(dat_plot2, aes(x= Caso, y = Expenditure)) + geom_hline(yintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone~Sex) + theme_bw() p1 En el gráfico anterior se logra observar un cambio en la distribución de los datos en las distintas desagregaciones cuando en la encuesta no se tiene pérdida de información y cuando sí se tiene con un esquema MAR. Naturalmente, esto afectaría en las estimaciones finales que se hagan de los parámetros estudiados. Con mayor claridad, se puede ver el cambio distribucional en la siguiente gráfica: p1 &lt;- ggplot(dat_plot2, aes(x = Income, fill = Caso)) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot2, aes(x = Income, fill = Caso)) + facet_grid(.~Sex) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;none&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) p1/p2 Este comportamiento es natural que suceda en un esquema MAR de datos faltantes puesto que, como se mencionó anteriormente, cuando se dice que los datos faltantes están “missing at random” (MAR), significa que la probabilidad de que los datos estén ausentes está relacionada con los valores observados en otras variables del conjunto de datos. En otras palabras, la probabilidad de que un valor esté ausente no está relacionada con el valor real del dato en sí mismo, sino que depende de la distribución de los datos en otras variables. La ventaja que tienen los mecanismo de missing MAR es que se puede estimar el valor real de los datos faltantes utilizando la información de otras variables disponibles en el conjunto de datos. Esto puede mejorar la calidad de los resultados de los análisis y evitar la necesidad de descartar observaciones con datos faltantes. Otra gráfica en donde se evidencia el cambio de distribución de los gastos entre hombres y mujeres. p1 &lt;- ggplot(dat_plot2, aes(x = Expenditure, fill = Caso)) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline( xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot2, aes(x = Expenditure, fill = Caso)) + facet_grid(.~Sex) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;none&quot;) + geom_vline( xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) p1/p2 Ahora bien, para seguir con la ejemplificación de los esquemas de datos faltantes, generemos ahora un esquema de pérdida de información en una encuesta NMAR (siglas en inglés de “Not Missing at Random”). Como se mencionó al inicio de este capítulo, en este tipo de esquema, la probabilidad de que un dato falte está relacionada con el propio valor de ese dato, es decir, la probabilidad de que falte un dato no es aleatoria y depende de alguna característica o variable del propio dato. En otras palabras, en un esquema aleatorio NMAR, la probabilidad de que falte un dato no es independiente del valor de ese dato, sino que está influenciada por algún factor que puede estar relacionado con el fenómeno que se está estudiando. Esto puede llevar a que los datos faltantes introduzcan un sesgo en los resultados del análisis estadístico, lo que hace que el manejo adecuado de los datos faltantes en este tipo de esquemas sea particularmente importante en la investigación. encuesta_MNAR &lt;- encuesta %&gt;% arrange((Income)) %&gt;% slice(1:1300L) dat_plot3 &lt;- bind_rows( list(encuesta_MNAR = encuesta_MNAR, encuesta = encuesta), .id = &quot;Caso&quot; ) El código anterior tiene como objetivo crear un nuevo conjunto de datos llamado encuesta_MNAR que contiene las primeras 1300 observaciones del conjunto de datos original encuesta, ordenadas por la variable Income. Luego, el código une el conjunto de datos original encuesta con el conjunto de datos encuesta_MNAR usando la función bind_rows(), y crea una nueva variable llamada “Caso” que indica la fuente de los datos. Ahora bien, para ver el efecto que tiene en una encuesta el tener datos faltante con esquema NMAR, se ilustran los siguientes gráficos: p1 &lt;- ggplot(dat_plot3, aes(x = Income, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline( xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline( xintercept = mean(encuesta_MNAR$Income), col = &quot;blue&quot;) p1 Como se puede observar en la gráfica anterior, la distribución de los ingresos cambia notablemente cuando se tienen datos faltantes con esquema NMAR, lo mismo sucede con la variable gastos, como se puede observar en la siguiente gráfica: p1 &lt;- ggplot(dat_plot3, aes(x = Expenditure, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline( xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) + geom_vline( xintercept = mean(encuesta_MNAR$Expenditure), col = &quot;blue&quot;) p1 Para ver más al detalle el impacto que tiene la no respuesta con un esquema NMAR, a continuación se muestra una gráfica del ingreso discriminada por sexo y por zona. También se nota un cambio en la distribución de los ingresos significativos. p1 &lt;- ggplot(dat_plot3, aes(x= Caso, y = Income)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone~Sex) + theme_bw() p1 Para efectos de ejemplificar la solución del problema a los datos faltantes en una encuesta de hogares, generemos la siguiente base de datos: encuesta &lt;- full_join( encuesta, encuesta_MCAR %&gt;% dplyr::select(HHID, PersonID, Income, Employment) %&gt;% mutate( Income_missin = Income, Employment_missin = Employment, Employment = NULL, Income = NULL ) ) El código anterior utiliza la función full_join() de la librería dplyr de ´R´ para combinar dos conjuntos de datos: encuesta y encuesta_MCAR. La combinación se realiza mediante la unión completa (o full join), que devuelve todas las filas de ambas tablas, uniendo las filas con valores coincidentes y rellenando con valores faltantes para las columnas que no tienen una coincidencia en ambas tablas. La segunda Base de datos, encuesta_MCAR, se transforma previamente con las siguientes operaciones: Se seleccionan las columnas HHID, PersonID, Income y Employment mediante la función dplyr::select(). Se agregan dos nuevas columnas, Income_missin y Employment_missin, utilizando la función mutate(). Los valores de estas columnas son idénticos a los de las columnas Income y Employment, respectivamente. Las columnas Income y Employment se eliminan del conjunto de datos utilizando la función mutate() y asignando el valor NULL a ambas columnas. Finalmente, el resultado de la unión completa se asigna a la variable encuesta. Ahora bien, para tener como referencia el porcentaje de datos faltantes, se ejecuta el siguiente comando: encuesta %&gt;% group_by(Zone) %&gt;% summarise(Income = sum(is.na(Income_missin) / n())) Zone Income Rural 0.2079208 Urban 0.1927835 encuesta %&gt;% group_by(Sex) %&gt;% summarise(Income = sum(is.na(Income_missin) / n())) Sex Income Female 0.1837945 Male 0.2191465 "],["imputación-por-la-media-no-condicional..html", "11.1 Imputación por la media no condicional.", " 11.1 Imputación por la media no condicional. La imputación por la media no condicional consiste en reemplazar los valores faltantes con la media aritmética de la variable completa, sin tener en cuenta ninguna otra variable. Es decir, la media se calcula a partir de todos los valores disponibles en la variable en cuestión, independientemente de las características de los demás datos. Este método es bastante simple y rápido, y puede ser útil en ciertas situaciones, especialmente cuando la variable en cuestión no tiene una distribución muy sesgada o cuando los valores faltantes son relativamente pocos en comparación con el tamaño de la muestra. Sin embargo, el método de imputación por la media no condicional también tiene limitaciones y puede no ser adecuado en todas las situaciones, especialmente cuando hay sesgos o patrones en los datos faltantes o cuando los datos están altamente correlacionados. Adicionalmente, este método no afecta el promedio, pero si afecta la variabilidad, el sesgo y los percentiles. A continuación, se ejemplifica con los datos de ejemplo este método: promedio &lt;- mean(encuesta$Income_missin, na.rm = TRUE) encuesta %&lt;&gt;% dplyr::mutate( Income_imp = ifelse(is.na(Income_missin), promedio, Income_missin)) sum(is.na(encuesta$Income_imp)) ## [1] 0 En el código anterior la imputación se realiza utilizando la media aritmética de los valores no faltantes en Income_missin y se almacena en una nueva variable llamada Income_imp. A continuación, se describen cada una de las líneas del código: promedio &lt;- mean(encuesta$Income_missin, na.rm = TRUE): esta línea calcula la media aritmética de los valores no faltantes en la columna Income_missin de la base de datos encuesta y la almacena en una variable llamada promedio. El argumento na.rm = TRUE se utiliza para excluir los valores faltantes en el cálculo de la media. encuesta %&lt;&gt;%: este operador de magrittr (%&lt;&gt;%) se utiliza para asignar el resultado de la siguiente operación al objeto encuesta. Es equivalente a utilizar encuesta &lt;- encuesta %&gt;%. mutate: esta función de dplyr se utiliza para crear una nueva columna en la base de datos encuesta con la imputación de los valores faltantes. Income_imp = ifelse(is.na(Income_missin), promedio, Income_missin): esta línea utiliza la función ifelse para asignar el valor imputado a la columna Income_imp en la base de datos encuesta. Si un valor en la columna Income_missin es NA (es decir, faltante), se reemplaza con el valor de promedio. Si no es NA, se mantiene el valor original. sum(is.na(encuesta$Income_imp)): esta línea cuenta el número de valores faltantes en la nueva columna Income_imp de la base de datos encuesta utilizando la función sum y is.na. ## Ordenando la base para gráfica dat_plot4 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone,Sex,Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;, -Zone,-Sex) p1 &lt;- ggplot(dat_plot4, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline( xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline( xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 En la gráfica anterior se puede observar que, a pesar de ser imputada la variable ingresos utilizando la media incondicional, la distribución real y la imputada cambia de manera significativa lo que demuestra que este método, para este conjunto de datos no es el más apropiado dado lo sesgado de la distribución de la variable ingresos. Un caso similar al anterior ocurre si graficamos la variable ingreso por zona y sexo. A continuación, se muestra de manera gráfica los boxplot para revisar la distribución de los datos, arrojando nuevamente las conclusiones obtenidas con el gráfico anterior: p1 &lt;- ggplot(dat_plot4, aes(x= Caso, y = Income2)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone~Sex) + theme_bw() p1 "],["imputación-por-la-media-condicional.html", "11.2 Imputación por la media condicional", " 11.2 Imputación por la media condicional El método de imputación por la media condicional es una técnica utilizada en el análisis de datos para tratar valores faltantes o perdidos en una variable numérica. A diferencia del método de imputación por la media no condicional, el método de imputación por la media condicional tiene en cuenta otras variables en el conjunto de datos al calcular la media. La imputación por la media condicional se basa en la idea de que la media de una variable puede variar en función de los valores de otras variables. Por lo tanto, en lugar de simplemente reemplazar los valores faltantes con la media aritmética de la variable completa, se utiliza la media de la variable para grupos de observaciones que tienen valores similares en otras variables. Por ejemplo, si se tiene una variable numérica llamada “Ingreso” y una variable categórica llamada “Educación”, se podría utilizar la media de ingresos para cada nivel de educación para imputar los valores faltantes en la variable “Ingreso”. De esta manera, se tiene en cuenta la relación entre la educación y el ingreso al realizar la imputación. El método de imputación por la media condicional puede ser más preciso que el método de imputación por la media no condicional en situaciones en las que las variables están correlacionadas o cuando hay patrones de valores faltantes en los datos. Sin embargo, también puede ser más complicado y requiere más tiempo y recursos computacionales para implementar. A continuación, se ejemplifica la técnica de imputación utilizando la variable estrato (Stratum en la base de datos) para hacer el cálculo de los promedios por cada uno de los estratos y así poder imputar los datos faltantes. Asumiendo que hay una relación directa entre los estratos y los ingresos de los hogares: encuesta %&lt;&gt;% group_by(Stratum) %&gt;% mutate( Income_imp = ifelse(is.na(Income_missin), mean(Income_missin, na.rm = TRUE), Income_missin)) %&gt;% data.frame() sum(is.na(encuesta$Income_imp)) ## [1] 0 encuesta %&lt;&gt;% mutate( Income_imp = ifelse(is.na(Income_imp), promedio, Income_imp)) sum(is.na(encuesta$Income_imp)) ## [1] 0 A continuación, se decribe el código computacional utilizado: encuesta %&lt;&gt;% group_by(Stratum) %&gt;%: este código utiliza la función group_by de dplyr para agrupar las observaciones de la base de datos encuesta por los niveles de la variable Stratum. El operador %&gt;% se utiliza para concatenar este código con el siguiente código en una sola cadena de operaciones. mutate: esta función de dplyr se utiliza para crear una nueva columna en la base de datos encuesta con la imputación de los valores faltantes. Income_imp = ifelse(is.na(Income_missin), mean(Income_missin, na.rm = TRUE), Income_missin)): esta línea utiliza la función ifelse para asignar el valor imputado a la columna Income_imp en la base de datos encuesta. Si un valor en la columna Income_missin es NA (es decir, faltante), se reemplaza con la media aritmética de los valores no faltantes en Income_missin dentro del grupo correspondiente. Si no es NA, se mantiene el valor original. sum(is.na(encuesta$Income_imp)): esta línea cuenta el número de valores faltantes en la nueva columna Income_imp de la base de datos encuesta utilizando la función sum y is.na. encuesta %&lt;&gt;% mutate( Income_imp = ifelse(is.na(Income_imp), promedio, Income_imp)): este código utiliza la función mutate para crear una nueva columna en la base de datos encuesta y reemplazar cualquier valor faltante en la columna Income_imp con la media aritmética de los valores no faltantes en Income_missin. El valor de la media aritmética se almacena en una variable llamada promedio. Si un valor en la columna Income_imp no es NA, se mantiene el valor original. sum(is.na(encuesta$Income_imp)): esta línea cuenta el número de valores faltantes en la nueva columna Income_imp de la base de datos encuesta utilizando la función sum y is.na. A continuación, se calculan las medias y desviaciones estándar tanto para los datos imputados como los originales y así poder comparar le efecto de la imputación realizada: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 611.545 488.7209 Para poder comparar los resultados, calculemos el sesgo relativo de la imputación el cual se calcula como sigue: \\[ BR=\\frac{Income-Income_{imp}}{Income}\\times100\\% \\] 100*(604.2494- 611.545)/604.2494 ## [1] -1.207382 Como se puede observar, el sesgo relativo para el primedio de los ingresos es menor al 1.5%. Ahora bien, siguiendo la misma idea, el sesgo relativo para la desviación es: 100*(513.1078- 488.7209)/513.1078 ## [1] 4.752783 Lo que generó un sesgo relativo para la desviación estándar inferior al 5%. Ahora bien, si se realiza la imputación utilizando la media condicional agrupando por la variable zona, se tienen los siguientes resultados: encuesta %&gt;%group_by(Zone) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Zone Income_ Income_sd Income_imp_ Income_imp_sd Rural 469.1217 336.5861 477.9042 305.5101 Urban 730.8793 609.0304 736.7815 585.6550 Realizando el mismo ejercicio anterior, se obtienen sesgos relativos para la media de los ingresos para la zona rural de 1.87%: 100*(469.1217- 477.9042)/469.1217 ## [1] -1.872115 y para la zona urbana de 0.8%.: 100*(730.8793- 736.7815)/730.8793 ## [1] -0.8075478 En ambos casos se observa una buena imputación de los ingresos. Ejercicio similar se puede realizar para sexo (Se le deja al lector realizar el cálculo del sesgo relativo). encuesta %&gt;%group_by(Sex) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Sex Income_ Income_sd Income_imp_ Income_imp_sd Female 589.2330 504.3041 600.3235 486.5818 Male 621.7771 522.9428 624.6431 491.1625 Ahora bien, para observar la distribución de los datos imputados en comparación con los no imputado se realizan las siguientes gráficas: dat_plot5 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone,Sex,Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;, -Zone,-Sex) p1 &lt;- ggplot(dat_plot5, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline( xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline( xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 Se puede observar que de manera general, la distribución de las observaciones imputadas y originales tienen un comportamiento mejor que con la media no condicional. Si se observa ahora la distribunción de los datos por zona y sexo, se puede observar también una buena imputación de las observaciones. p1 &lt;- ggplot(dat_plot5, aes(x= Caso, y = Income2)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone~Sex) + theme_bw() p1 "],["imputación-por-hot-deck-y-cold-deck.html", "11.3 Imputación por Hot-deck y Cold-deck", " 11.3 Imputación por Hot-deck y Cold-deck La imputación Hot deck consiste en reemplazar los valores faltantes de una o más variables para un no encuestado (llamado receptor) con valores observados de un encuestado (el donante) que es similar al no encuestado con respecto a las características observadas en ambos casos. La técnica se basa en la idea de que las observaciones similares pueden tener valores similares para las variables de interés. En el enfoque por hot-deck, se selecciona una observación donante que sea similar a la observación receptora en términos de características relevantes (por ejemplo, edad, género, ubicación geográfica, etc.), y se utiliza su valor observado para imputar el valor faltante en la observación receptora. El término “hot-deck” se refiere a una tarjeta perforada que se utilizaba en los primeros sistemas informáticos para almacenar y recuperar datos. En el enfoque por hot-deck, las observaciones se organizan en una “pila” o “mazo” de tarjetas, y las observaciones similares se seleccionan de esta pila para imputar los valores faltantes. La imputación por hot-deck es una técnica relativamente simple y eficaz para imputar valores faltantes en conjuntos de datos pequeños o medianos, y se utiliza comúnmente en encuestas y estudios de investigación social. Sin embargo, puede ser menos efectiva en conjuntos de datos grandes o complejos, donde puede ser difícil encontrar observaciones similares o donde las características relevantes son difíciles de definir o medir de manera confiable. Por otro lado, el método llamado Cold-deck por analogía con Hot-deck consiste en reemplazar el valor faltante por valores de una fuente no relacionada con el conjunto de datos en consideración. El método de imputación Cold-deck es una técnica de imputación de datos faltantes que se basa en la sustitución de los valores faltantes por valores observados de una fuente externa, tal como un conjunto de datos históricos, registros administrativos u otras fuentes de datos secundarios. A diferencia del método de imputación por hot-deck, que utiliza información de la propia muestra para imputar los valores faltantes, el método de imputación cold-deck se basa en la utilización de información externa para sustituir los valores faltantes. La fuente de datos externa se utiliza para imputar los valores faltantes en la muestra de estudio. El término “cold-deck” hace referencia a la tarjeta perforada que se utilizaba en los primeros sistemas informáticos para almacenar y recuperar datos. En el método de imputación cold-deck, los valores faltantes se imputan a partir de los datos de la “pila fría” o “mazo frío” de tarjetas perforadas, que corresponden a los datos históricos o a la fuente externa de datos. El método de imputación cold-deck se utiliza a menudo cuando la muestra de estudio es pequeña o no hay suficientes observaciones similares para aplicar el método de imputación por hot-deck. Sin embargo, el método de imputación cold-deck tiene algunas limitaciones, como la posibilidad de que los datos externos no sean representativos de la muestra de estudio y la posibilidad de introducir errores en los datos imputados si la fuente de datos externa no es fiable o no es adecuada para la variable de interés. Imputación por hot-deck Iniciamos los ejemplos en esta sección con el método hot-deck. A continuación, se presenta un código computacional que ejemplifica, para los datos que estamos usando en el capítulo, el uso del método hot-deck. donante &lt;- which(!is.na(encuesta$Income_missin)) receptor &lt;- which(is.na(encuesta$Income_missin)) encuesta$Income_imp &lt;- encuesta$Income_missin set.seed(1234) for(ii in receptor){ don_ii &lt;- sample(x = donante, size = 1) encuesta$Income_imp[ii] &lt;- encuesta$Income_missin[don_ii]} sum(is.na(encuesta$Income_imp)) ## [1] 0 El código mostrado anteriormente se describe a continuación: La primera línea del código selecciona las observaciones que no tienen valores faltantes en la variable “Income_missin” utilizando la función which y el operador ! (not) donante &lt;- which(!is.na(encuesta$Income_missin)) La segunda línea selecciona las observaciones que tienen valores faltantes en la variable “Income_missin” utilizando la función which y el operador is.na. receptor &lt;- which(is.na(encuesta$Income_missin)) La tercera línea crea una nueva variable “Income_imp” en la encuesta, que se utilizará para almacenar los valores imputados encuesta\\(Income_imp &lt;- encuesta\\)Income_missin La cuarta línea utiliza la función set.seed para establecer una semilla aleatoria para asegurar la reproducibilidad del proceso de imputación. set.seed(1234) Se utiliza un bucle for para iterar a través de cada observación receptora. Dentro del bucle, se utiliza la función sample para seleccionar una observación donante aleatoria de entre las observaciones que no tienen valores faltantes en la variable “Income_missin”. Una vez realizada la imputación, se calcula la media y la desviación de los datos completos e imputados: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 618.2937 528.2157 Como en los métodos de imputación anterior, calculemos el sesgo relativo de la imputación el cual fue de 2.3%: 100*(604.2494-618.2937)/604.2494 ## [1] -2.324256 Ahora bien, haciendo el mismo ejercicio, pero esta vez desagregada por zona tenemos: encuesta %&gt;%group_by(Zone) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Zone Income_ Income_sd Income_imp_ Income_imp_sd Rural 469.1217 336.5861 503.7127 368.9137 Urban 730.8793 609.0304 725.6691 623.9875 El sesgo relativo de la estimación en la zona rural es de 7.4% y en al zona urbana es de 0.7%. 100*(469.1217-503.7127)/469.1217 ## [1] -7.373566 100*(730.8793-725.6691)/730.8793 ## [1] 0.7128674 El mismo ejercicio se puede realizar por sexo: encuesta %&gt;%group_by(Sex) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Sex Income_ Income_sd Income_imp_ Income_imp_sd Female 589.2330 504.3041 602.8075 503.0951 Male 621.7771 522.9428 636.3699 555.8522 Como en los ejercicios anteriores, a continuación, se muestra la gráfica de la distribución de los datos tanto los completos como los imputados observándose que la distribución de los datos imputados es muy similar a la de los datos no imputados: dat_plot6 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone,Sex,Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;, -Zone,-Sex) p1 &lt;- ggplot(dat_plot6, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline( xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline( xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 El mismo ejercicio anterior se realiza por zona y sexo obteniendo resultados similares a los abtenidos en el gráfico anterior: p1 &lt;- ggplot(dat_plot6, aes(x= Caso, y = Income2)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone~Sex) + theme_bw() p1 A continuación, se implementa el método de imputación pero para la variable empleado. Los códigos computacionales son similares a los empleados con la variable ingresos: donante &lt;- which(!is.na(encuesta$Income_missin)) receptor &lt;- which(is.na(encuesta$Income_missin)) encuesta$Employment_imp &lt;- encuesta$Employment_missin (prop &lt;- prop.table( table(na.omit(encuesta$Employment_missin)))) Unemployed Inactive Employed 0.0425815 0.3739188 0.5834997 set.seed(1234) imp &lt;- sample(size = length(receptor), c(&quot;Unemployed&quot;, &quot;Inactive&quot;,&quot;Employed&quot;), prob = prop, replace = TRUE ) encuesta$Employment_imp[receptor] &lt;- imp sum(is.na(encuesta$Employment_imp)) ## [1] 0 Se le deja al lector realizar los ejercicios de comprobación gráfica, como se mostró a lo largo de esta sección. "],["imputación-por-regresión.html", "11.4 Imputación por regresión", " 11.4 Imputación por regresión La imputación por regresión es una técnica de análisis de datos que se utiliza para imputar valores faltantes en un conjunto de datos. Esta técnica se basa en la construcción de un modelo de regresión a partir de las variables \\(X\\) disponibles en el conjunto de datos, que se utiliza para predecir los valores faltantes \\(Y\\). Cuando se habla de predicción no se refiere a dar un valor futuro, se refiere a dar un valor a la información faltante. Para llevar a cabo la imputación por regresión, se selecciona una variable objetivo que tenga valores faltantes y se identifican las variables predictoras que tienen una correlación significativa con la variable objetivo. Se ajusta un modelo de regresión utilizando las variables predictoras y la variable objetivo disponible, y se utilizan los coeficientes del modelo para predecir los valores faltantes de la variable objetivo. Es importante destacar que la imputación por regresión es una técnica estadística avanzada que requiere conocimientos sólidos de análisis de datos y modelado estadístico. Además, su aplicación puede verse limitada por la calidad y la cantidad de los datos disponibles y por la distribución de los valores faltantes en el conjunto de datos. Por lo tanto, es importante utilizarla con precaución y tener en cuenta sus limitaciones. Para ejemplificar, imputemos la variable ingreso y la variable empleados tomadno como covariables las variable zona, sexo y empleamiento. Para la primera variable se utiliza un modelo de regresión líneal múltiple y para el segundo. se utiliza un modelo multinomial (dada la naturaleza de la variable) como se muestra a continución: require(nnet) ## Loading required package: nnet encuesta$Income_imp &lt;- encuesta$Income_missin encuesta$Employment_imp &lt;- encuesta$Employment_missin encuesta_obs &lt;- filter(encuesta, !is.na(Income_missin)) encuesta_no_obs &lt;- filter(encuesta, is.na(Income_missin)) mod &lt;- lm(Income~Zone + Sex +Expenditure, data = encuesta_obs) mod.mult &lt;- multinom(Employment~Zone + Sex +Expenditure, data = encuesta_obs) ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1182.110113 ## final value 1132.682019 ## converged Una vez ajustado los modelos tanto para las variable ingreso como para empleados, se realiza el proceso de predicción como se muestra a continuación: imp &lt;- predict(mod, encuesta_no_obs) imp.mult &lt;- predict(mod.mult, encuesta_no_obs, type = &quot;class&quot;) encuesta_no_obs$Income_imp &lt;- imp encuesta_no_obs$Employment_imp &lt;- imp.mult encuesta &lt;- bind_rows(encuesta_obs,encuesta_no_obs) A continuación, se presenta el porcentaje de datos faltantes en la variable empleado: prop.table(table(encuesta$Employment_missin, useNA = &quot;a&quot;)) Unemployed Inactive Employed NA 0.0340607 0.2990953 0.4667376 0.2001064 Se puede observar que hay un 20% de datos faltantes. Una vez se realiza la imputación, se redistribuyen esas observaciones en las demás categorías arrojando los siguientes resultados: prop.table(table(encuesta$Employment_imp, useNA = &quot;a&quot;)) Unemployed Inactive Employed NA 0.0340607 0.3858435 0.5800958 0 A modo de ejercicio, se realiza el cálculo del porcentaje de los valores faltante para la variable empleados por zona, antes y después de imputar, reconociendo que, los porcentajes marginales por zona no varían: library(printr) library(kableExtra) kable(prop.table( table(encuesta$Zone, encuesta$Employment_missin, useNA = &quot;a&quot;)) %&gt;% addmargins()) Unemployed Inactive Employed NA Sum Rural 0.0117084 0.1506120 0.2208622 0.1005854 0.483768 Urban 0.0223523 0.1484832 0.2458755 0.0995210 0.516232 NA 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 Sum 0.0340607 0.2990953 0.4667376 0.2001064 1.000000 kable( prop.table( table(encuesta$Zone, encuesta$Employment_imp,useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Rural 0.0117084 0.2006386 0.2714210 0 0.483768 Urban 0.0223523 0.1852049 0.3086748 0 0.516232 NA 0.0000000 0.0000000 0.0000000 0 0.000000 Sum 0.0340607 0.3858435 0.5800958 0 1.000000 El mismo ejercicio anterior se realiza por sexo arrojando los siguientes resultados: kable( prop.table( table(encuesta$Sex, encuesta$Employment_missin, useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Female 0.0106440 0.2277807 0.2011708 0.0989888 0.5385844 Male 0.0234167 0.0713145 0.2655668 0.1011176 0.4614156 NA 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 Sum 0.0340607 0.2990953 0.4667376 0.2001064 1.0000000 kable( prop.table( table(encuesta$Sex, encuesta$Employment_imp,useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Female 0.0106440 0.3145290 0.2134114 0 0.5385844 Male 0.0234167 0.0713145 0.3666844 0 0.4614156 NA 0.0000000 0.0000000 0.0000000 0 0.0000000 Sum 0.0340607 0.3858435 0.5800958 0 1.0000000 Para finalizar y como se ha realizado con los métodos anteriores, se hace el cálculo de la variable ingreso completa e imputada: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 611.7477 498.3293 Teniendo un sesgo relativo de 1.2%. 100*(604.2494 - 611.7477)/604.2494 ## [1] -1.240928 Haciendo el mismo ejercicio por zona tenemos: encuesta %&gt;%group_by(Zone) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Zone Income_ Income_sd Income_imp_ Income_imp_sd Rural 469.1217 336.5861 476.1361 317.3999 Urban 730.8793 609.0304 738.8311 594.5319 Con sesgos relativos para la zona rural de 1.5% y para urbano de 1%. 100*(469.1217 - 476.1361)/469.1217 ## [1] -1.49522 100*(730.8793 - 738.8311)/730.8793 ## [1] -1.087977 Para la variable sexo, se puede realizar el mismo ejercicio anterior. Se le deja al lector hacer los cálculos pertinentes: encuesta %&gt;%group_by(Sex) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Sex Income_ Income_sd Income_imp_ Income_imp_sd Female 589.2330 504.3041 598.6515 488.3917 Male 621.7771 522.9428 627.0341 509.5410 Por último, los ejercicios gráficos se realizan a continuación: dat_plot7 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone,Sex,Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;, -Zone,-Sex) p1 &lt;- ggplot(dat_plot7, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline( xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline( xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 p1 &lt;- ggplot(dat_plot7, aes(x= Caso, y = Income2)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone~Sex) + theme_bw() p1 Obteniendo buenos resultados en el proceso de imputación como se pudo observar en las gráficas anteriores. "],["imputación-por-el-vecino-más-cercano.html", "11.5 Imputación por el vecino más cercano", " 11.5 Imputación por el vecino más cercano La imputación por el vecino más cercano (K-nearest neighbor imputation en inglés) es una técnica de análisis de datos utilizada para estimar los valores faltantes en un conjunto de datos. En esta técnica, los valores faltantes se reemplazan por valores de otras observaciones que son similares a la observación con valores faltantes. La imputación por el vecino más cercano se basa en la idea de que los registros similares tienden a tener valores similares para una determinada variable. La técnica consiste en encontrar los \\(k\\) registros más similares a la observación con valores faltantes en función de las variables disponibles en el conjunto de datos y utilizar los valores de estas observaciones para estimar el valor faltante. Para calcular la similitud entre observaciones, se pueden utilizar diferentes medidas de distancia, como la distancia euclidiana o la distancia de Manhattan. La técnica también permite ajustar el valor de \\(k\\), que representa el número de vecinos más cercanos utilizados para estimar el valor faltante. Es importante destacar que la imputación por el vecino más cercano es una técnica relativamente simple y fácil de implementar. Sin embargo, su eficacia puede verse limitada por la cantidad y la calidad de los datos disponibles, así como por la elección de los parámetros (como el valor de \\(k\\) y la medida de distancia) que pueden afectar significativamente los resultados obtenidos. Por lo tanto, es importante evaluar cuidadosamente la calidad de los datos y los resultados obtenidos antes de utilizar esta técnica. Teniendo en cuenta lo anterior, se presentan 3 pasos a tener en cuenta al momento de utilizar esta técnica: Paso 1: Definir una magnitud de distancia (Distancia euclidiana, k-media, K-Medioides, entre otras). Paso 2: Para la \\(i\\)-ésimo elemento identificar el donante, cual será el más cercano al receptor según la magnitud de distancia previamente definida. Paso 3: Se imputa el valor faltante con la información del donante identificado previamente. Para ejemplificar esta metología, se va a imputar la variable ingresos y empleado utilizando como variable de apoyo los gastos del individuo. Se utilizará como distancia la euclidiana. encuesta$Income_imp &lt;- encuesta$Income_missin encuesta$Employment_imp &lt;- encuesta$Employment_missin encuesta_obs &lt;- filter(encuesta, !is.na(Income_missin)) encuesta_no_obs &lt;- filter(encuesta, is.na(Income_missin)) for(ii in 1:nrow(encuesta_no_obs)){ Expen_ii &lt;- encuesta_no_obs$Expenditure[[ii]] don_ii &lt;- which.min(abs(Expen_ii - encuesta_obs$Expenditure)) encuesta_no_obs$Income_imp[[ii]] &lt;- encuesta_obs$Income_missin[[don_ii]] encuesta_no_obs$Employment_imp[[ii]] &lt;- encuesta_obs$Employment_missin[[don_ii]] } encuesta &lt;- bind_rows(encuesta_obs,encuesta_no_obs) Como se hizo la revisión anterior, se calculan los porcentajes de datos faltantes y se revisa nuevamente la distribución de la imputación en las categorías de la variable empleado. prop.table(table(encuesta$Employment_missin, useNA = &quot;a&quot;)) Unemployed Inactive Employed NA 0.0340607 0.2990953 0.4667376 0.2001064 prop.table(table(encuesta$Employment_imp, useNA = &quot;a&quot;)) Unemployed Inactive Employed NA 0.0436402 0.3650878 0.591272 0 Haciendo el mismo ejercicio por zona se tiene: kable( prop.table( table(encuesta$Zone, encuesta$Employment_missin, useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Rural 0.0117084 0.1506120 0.2208622 0.1005854 0.483768 Urban 0.0223523 0.1484832 0.2458755 0.0995210 0.516232 NA 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 Sum 0.0340607 0.2990953 0.4667376 0.2001064 1.000000 kable( prop.table( table(encuesta$Zone, encuesta$Employment_imp,useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Rural 0.0127728 0.1873337 0.2836615 0 0.483768 Urban 0.0308675 0.1777541 0.3076104 0 0.516232 NA 0.0000000 0.0000000 0.0000000 0 0.000000 Sum 0.0436402 0.3650878 0.5912720 0 1.000000 Al igual que en el caso anterior, la distribución marginal por zona no se altera. Ahora por sexo la distribución es la siguiente: kable( prop.table( table(encuesta$Sex, encuesta$Employment_missin, useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Female 0.0106440 0.2277807 0.2011708 0.0989888 0.5385844 Male 0.0234167 0.0713145 0.2655668 0.1011176 0.4614156 NA 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 Sum 0.0340607 0.2990953 0.4667376 0.2001064 1.0000000 kable( prop.table( table(encuesta$Sex, encuesta$Employment_imp,useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Female 0.0159659 0.2527940 0.2698244 0 0.5385844 Male 0.0276743 0.1122938 0.3214476 0 0.4614156 NA 0.0000000 0.0000000 0.0000000 0 0.0000000 Sum 0.0436402 0.3650878 0.5912720 0 1.0000000 Ahora, haciendo el cálculo del promedio de los ingresos para los datos completos y estimados se tienen los siguientes resultados: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 610.505 513.6812 Se observa que hay una diferencia de 6 unidades monetarias entre el promedio real y el estimado. Realizando este mismo ejercicio por zona tenemos: encuesta %&gt;%group_by(Zone) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Zone Income_ Income_sd Income_imp_ Income_imp_sd Rural 469.1217 336.5861 477.9160 344.1316 Urban 730.8793 609.0304 734.7559 607.0266 Obteniéndose diferencias de 7 unidades monetarias en el ingreso para la zona rural y de 4 para la urbana. Este mismo ejercicio se realiza por sexo teniendo los siguientes resultados: encuesta %&gt;%group_by(Sex) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Sex Income_ Income_sd Income_imp_ Income_imp_sd Female 589.2330 504.3041 597.8052 504.5971 Male 621.7771 522.9428 625.3287 523.9882 Obteniéndose diferencias pequeñas entre el ingreso real y el estimado en ambos sexos. dat_plot8 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone,Sex,Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;, -Zone,-Sex) p1 &lt;- ggplot(dat_plot8, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline( xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline( xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 Se puede observar que la distribución de los datos imputados son muy próximos que los datos reales. Haciendo este mismo ejercicio pero por zona y sexo se obtienen resultados similares a los anteriores: p1 &lt;- ggplot(dat_plot8, aes(x= Caso, y = Income2)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone~Sex) + theme_bw() p1 "],["imputación-por-el-vecino-más-cercano-con-regresión.html", "11.6 Imputación por el vecino más cercano con regresión", " 11.6 Imputación por el vecino más cercano con regresión An las secciones anteriores se realizaron las descripciones delas técnicas: vecino más cercano e imputación vía regresión, a continuación, se presentan los pasos que se deben tener en cuenta para realizar la imputación utilizando el vecino más cernado mediante una regresión: Paso 1: Ajustar un modelo de regresión. Paso 2: Realizar la predicción de los valores observados y no observados. Paso 3: Comparar las predicciones obtenidas para los valores observados y no observados. Paso 4: Para la \\(i\\)-ésima observación identificar el donante con la menor distancia al receptor. Paso 5: Reemplazar el valor faltante con la información proveniente del donante. NOTA Se toma es la información observada en el donante. A continuación, se ejemplifica la técnica imputando los ingresos en los hogares realizando un modelo en el cual se toman como covariables el sexo, la zona y los gastos: encuesta$Income_imp &lt;- encuesta$Income_missin encuesta$Employment_imp &lt;- encuesta$Employment_missin encuesta_obs &lt;- filter(encuesta, !is.na(Income_missin)) encuesta_no_obs &lt;- filter(encuesta, is.na(Income_missin)) mod &lt;- lm(Income ~ Zone + Sex + Expenditure, data = encuesta_obs) Luego, se predicen los valores observados y no observados con el modelo ajustado anteriormente y se imputa el valor faltante calculando las diferencias entre las predicciones de los datos observados y no observados: pred_Obs &lt;- predict(mod, encuesta_obs) pred_no_Obs &lt;- predict(mod, encuesta_no_obs) for(ii in 1:nrow(encuesta_no_obs)){ don_ii &lt;- which.min(abs(pred_no_Obs[ii] - pred_Obs)) encuesta_no_obs$Income_imp[[ii]] &lt;- encuesta_obs$Income_missin[[don_ii]] encuesta_no_obs$Employment_imp[[ii]] &lt;- encuesta_obs$Employment_missin[[don_ii]] } encuesta &lt;- bind_rows(encuesta_obs,encuesta_no_obs) Una vez imputada la información, se puede chequear el pocentaje de datos faltantes que habían y una vez impuatado, cómo cambia la distribución: kable( prop.table(table(encuesta$Employment_missin, useNA = &quot;a&quot;)) ) Var1 Freq Unemployed 0.0340607 Inactive 0.2990953 Employed 0.4667376 NA 0.2001064 kable( prop.table(table(encuesta$Employment_imp, useNA = &quot;a&quot;)) ) Var1 Freq Unemployed 0.0399148 Inactive 0.3736030 Employed 0.5864822 NA 0.0000000 El mismo ejercicio se puede realizar por zona: kable( prop.table( table(encuesta$Zone, encuesta$Employment_missin, useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Rural 0.0117084 0.1506120 0.2208622 0.1005854 0.483768 Urban 0.0223523 0.1484832 0.2458755 0.0995210 0.516232 NA 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 Sum 0.0340607 0.2990953 0.4667376 0.2001064 1.000000 kable( prop.table( table(encuesta$Zone, encuesta$Employment_imp,useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Rural 0.0138371 0.1905269 0.2794039 0 0.483768 Urban 0.0260777 0.1830761 0.3070782 0 0.516232 NA 0.0000000 0.0000000 0.0000000 0 0.000000 Sum 0.0399148 0.3736030 0.5864822 0 1.000000 Y por sexo: kable( prop.table( table(encuesta$Sex, encuesta$Employment_missin, useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Female 0.0106440 0.2277807 0.2011708 0.0989888 0.5385844 Male 0.0234167 0.0713145 0.2655668 0.1011176 0.4614156 NA 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 Sum 0.0340607 0.2990953 0.4667376 0.2001064 1.0000000 kable( prop.table( table(encuesta$Sex, encuesta$Employment_imp,useNA = &quot;a&quot;)) %&gt;% addmargins() ) Unemployed Inactive Employed NA Sum Female 0.0122406 0.2730176 0.2533262 0 0.5385844 Male 0.0276743 0.1005854 0.3331559 0 0.4614156 NA 0.0000000 0.0000000 0.0000000 0 0.0000000 Sum 0.0399148 0.3736030 0.5864822 0 1.0000000 Por último, se calcula la media de los ingresos y su desviación estándar para los datos completos e imputado como se ha realizado anteriormente: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 608.2746 515.5829 De la anterior imputación se puede observar que, la diferencia entre los datos reales y los imputados es cercano a 4 unidades monetarias. El mismo ejercicio realizado por zona arroja los siguientes resultados: encuesta %&gt;%group_by(Zone) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Zone Income_ Income_sd Income_imp_ Income_imp_sd Rural 469.1217 336.5861 476.0558 342.6870 Urban 730.8793 609.0304 732.1786 611.0504 y se puede observar también que la diferencia entre los ingresos reales y los estimados en las dos zonas con inferiores a 6 unidades monetarias. Por sexo, los resultados son los siguientes: encuesta %&gt;%group_by(Sex) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp)) Sex Income_ Income_sd Income_imp_ Income_imp_sd Female 589.2330 504.3041 592.9922 508.4371 Male 621.7771 522.9428 626.1128 523.5303 Teniendo como diferencia máxima para la variable ingreso de 5 unidades monetarias. A continuación, se presentan la gráfica distribucional del ingreso real y del ingreso imputado por el método del vecino más cercano mediante un modelo. Se puede observar que, las dos distribuciones con muy similares. dat_plot9 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone,Sex,Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;, -Zone,-Sex) p1 &lt;- ggplot(dat_plot9, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline( xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline( xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 Realizando ahora unos boxplot por sexo y zona para la variable ingreso tanto la completa como la imputada se tiene: p1 &lt;- ggplot(dat_plot9, aes(x= Caso, y = Income2)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone~Sex) + theme_bw() p1 Se puede observar que los boxplot son muy similares. "],["introducción-a-la-imputación-múltiple..html", "11.7 Introducción a la imputación múltiple.", " 11.7 Introducción a la imputación múltiple. La imputación múltiple, también conocida como “multiple imputations” en inglés, es una técnica estadística utilizada para tratar los datos faltantes o incompletos en un conjunto de datos. La imputación múltiple consiste en crear múltiples copias del conjunto de datos, donde los valores faltantes en cada copia son imputados utilizando modelos estadísticos. Estos modelos se basan en las relaciones entre las variables en el conjunto de datos y se utilizan para estimar los valores faltantes de manera plausible. Luego de que se han creado múltiples copias completas del conjunto de datos, se realizan análisis separados en cada copia para generar resultados. Los resultados de cada análisis se combinan para obtener un único resultado final que refleje la incertidumbre causada por la imputación de los valores faltantes. La imputación múltiple es una técnica poderosa para lidiar con los datos faltantes, ya que proporciona resultados más precisos y menos sesgados en comparación con otros métodos que simplemente eliminan las observaciones con valores faltantes. Sin embargo, la imputación múltiple es un proceso computacionalmente intensivo y requiere un conocimiento sólido de la teoría estadística para su implementación efectiva. En este sentido, suponga entonces que existe un conjunto de \\(n\\) datos que relaciona dos variables \\(X\\), \\(Y\\), a través del siguiente modelo de regresión simple: \\[y_i = \\beta x_i + \\varepsilon_i\\] Para todo individuo \\(i = 1, \\ldots, n.\\), de tal manera que los errores tienen distribución normal con \\(E(\\varepsilon) = 0\\) y \\(Var(\\varepsilon) = \\sigma ^2\\). Ahora bien, se debe tener en cuenta, para utilizar la metodología, los siguientes atributos: Sea \\(Y_{Obs}\\) los valores observados para un conjunto de individuos de tamaño \\(n_1\\). Sea \\(Y_{NoObs}\\) los valores NO observados de la variable \\(Y\\) de tamaño \\(n_0\\), es decir, \\(n_1 + n_0 = n\\). Suponga que sí fue posible observar los valores de la covariable \\(X\\) para todos los individuos en la muestra. A modo de ejemplificar la teoría, realicemos un ejercicio de simulación de la siguiente manera. Simular un conjunto de \\(n = 500\\) datos con una pendiente \\(\\beta = 10\\) y con una dispersión de \\(\\sigma = 2\\). A su vez, el conjunto de datos tendrá \\(n_0 = 200\\) valores faltantes en la variable respuesta. El algoritmo de simulación es el siguiente: generar &lt;- function(n = 500, n_0 = 200, beta = 10, sigma = 2){ x &lt;- runif(n) mu &lt;- beta * x y &lt;- mu + rnorm(n, mean = 0, sd = sigma) datos &lt;- data.frame(x = x, y = y) faltantes &lt;- sample(n, n_0) datos$faltantes &lt;- &quot;No&quot; datos$faltantes[faltantes] &lt;- &quot;Si&quot; datos$y.per &lt;- y datos$y.per[faltantes] &lt;- NA return(datos) } El código anterior realiza lo siguiente: generar &lt;- function(n = 500, n_0 = 200, beta = 10, sigma = 2){:Esta línea de código define la función “generar” con cuatro argumentos opcionales: n, n_0, beta y sigma. x &lt;- runif(n): Esta línea genera una secuencia de n números aleatorios uniformemente distribuidos en el intervalo [0,1] y los asigna a la variable “x”. mu &lt;- beta * x: Esta línea de código calcula la media condicional de “y” (la variable de respuesta) dada “x” utilizando el parámetro de pendiente “beta” y lo asigna a la variable “mu”. y &lt;- mu + rnorm(n, mean = 0, sd = sigma): Esta línea genera una variable de respuesta “y” a partir de la media condicional “mu” y agrega un error aleatorio generado a partir de una distribución normal con media cero y desviación estándar “sigma”. datos &lt;- data.frame(x = x, y = y): Esta línea combina las variables “x” e “y” en un data frame llamado “datos”. faltantes &lt;- sample(n, n_0): Esta línea genera una muestra aleatoria de “n_0” valores únicos desde un rango de 1 hasta “n” (la longitud de los datos) y los asigna a la variable “faltantes”. datos\\(faltantes &lt;- &quot;No&quot;*, *datos\\)faltantes[faltantes] &lt;- “Si”: Esta línea crea una nueva columna en el data frame “datos” llamada “faltantes” y la inicializa con valores “No” para todas las observaciones. Luego, establece los valores de esta columna en “Si” para las filas seleccionadas por la muestra aleatoria anterior. datos\\(y.per &lt;- y*, *datos\\)y.per[faltantes] &lt;- NA: Esta línea crea una nueva columna en el data frame “datos” llamada “y.per” y la inicializa con los mismos valores que “y”. Luego, establece los valores de esta columna en “NA” para las filas seleccionadas por la muestra aleatoria anterior, lo que simula valores faltantes en los datos de “y”. Una vez creada la función anterior, se genera la población usando una semilla para poder reproducirla: set.seed(1234) datos &lt;- generar() head(datos,12) x y faltantes y.per 0.1137034 2.0108953 No 2.010895 0.6222994 8.3432419 No 8.343242 0.6092747 6.9971281 No 6.997128 0.6233794 7.5601916 Si NA 0.8609154 6.3364067 No 6.336407 0.6403106 5.6621110 No 5.662111 0.0094958 3.0488967 No 3.048897 0.2325505 -0.1223024 Si NA 0.6660838 7.1769744 Si NA 0.5142511 5.9525170 No 5.952517 0.6935913 8.8875196 No 8.887520 0.5449748 4.7519949 No 4.751995 A continuación, se grafican la relación entre “x” y “y” y “x” y “y.per” notando que, la distribución entre las dos relaciones es muy similar. library(patchwork) p1 &lt;- ggplot(data = datos, aes(x = x, y = y)) + geom_point() + geom_smooth(formula = y~x , method = &quot;lm&quot;) p2 &lt;- ggplot(data = datos, aes(x = x, y = y.per)) + geom_point() + geom_smooth(formula = y~x , method = &quot;lm&quot;) p1 | p1 Ahora, dado el 40% de valores faltantes, es necesario imputar dichos valores. Para esto, utilizaremos la técnica de imputación múltiple propuesta por Rubin (1987). La idea consiste en generar \\(M &gt; 1\\) conjuntos de valores para los datos faltantes. Al final, el valor imputado corresponderá al promedio de esos \\(M\\) valores. Hay varias maneras de realizar la imputación, a continuación, se hace un listado: Ingenua: Esta clase de imputación carece de aleatoriedad y por tanto, la varianza de \\(\\beta\\) va a ser subestimada. Bootstrap: Se seleccionan \\(m\\) muestras bootstrap, y para cada una se estiman los parámetros \\(\\beta\\) y \\(\\sigma\\) para generar \\(\\dot{y}_i\\). Al final se promedian los \\(m\\) valores y se imputa el valor faltante. Bayesiana: Se definen las distribuciones posteriores de \\(\\beta\\) y \\(\\sigma\\) para generar \\(M\\) valores de estos parámetros y por tanto \\(M\\) valores de \\(\\dot{y}_i\\). Al final se promedian los \\(M\\) valores y se imputa el valor faltante. Dado que el interés es la estimación de la pendiente de la regresión lineal simple \\(\\beta\\), entonces la esperanza estimada al utilizar la metodología de imputación múltiple está dada por: \\[E(\\hat{\\beta} | Y_{obs}) = E(E(\\hat{\\beta} | Y_{obs}, Y_{mis}) | Y_{obs})\\] Esta expresión es estimada por el promedio de las \\(M\\) estimaciones puntuales de \\(\\hat{\\beta}\\) sobre las \\(M\\) imputaciones, dado por: \\[\\bar{\\hat{\\beta}} = \\frac{1}{M} \\sum_{m = 1} ^ M \\hat{\\beta}_m\\] La varianza estimada al utilizar la metodología de imputación múltiple está dada por la siguiente expresión: \\[ V(\\hat{\\beta} | Y_{obs}) = E(V(\\hat{\\beta} | Y_{obs}, Y_{mis}) | Y_{obs}) + V(E(\\hat{\\beta} | Y_{obs}, Y_{mis}) | Y_{obs}) \\] La primera parte de la anterior expresión se estima como el promedio de las varianzas muestrales de \\(\\hat{\\beta}\\) sobre las \\(M\\) imputaciones, dado por: \\[\\bar{U} = \\frac{1}{M} \\sum_{m = 1} ^ M Var(\\beta)\\] El segundo término se estima como la varianza muestral de las \\(M\\) estimaciones puntuales de \\(\\hat{\\beta}\\) sobre las \\(M\\) imputaciones, dada por: \\[B = \\frac{1}{M-1} \\sum_{m = 1} ^ M (\\hat{\\beta}_m - \\bar{\\hat{\\beta}})\\] Es necesario tener en cuenta un factor de corrección (puesto que \\(M\\) es finito). Por tanto, la estimación del segundo término viene dada por la siguiente expresión: \\[ (1 + \\frac{1}{M}) B \\] Por tanto, la varianza estimada es igual a: \\[\\hat{V}(\\hat{\\beta} | Y_{obs}) = \\bar{U} + (1 + \\frac{1}{M}) B\\] A modo de ejemplo, a continuación, se crea una función que haga la estimación de los valores faltantes usando bootstrap: im.bootstrap &lt;- function(datos, M = 15){ library(dplyr) n &lt;- nrow(datos) datos1 &lt;- na.omit(datos) n1 &lt;- nrow(datos1) n0 &lt;- n - n1 Ind &lt;- is.na(datos$y.per) faltantes.boot &lt;- NULL beta1 &lt;- NULL sigma1 &lt;- NULL for (m in 1:M){ datos.m &lt;- dplyr::sample_n(datos1, n1, replace = TRUE) model1 &lt;- lm(y ~ 0 + x, data = datos.m) beta &lt;- model1$coeff sigma &lt;- sqrt(anova(model1)[[&quot;Mean Sq&quot;]][2]) faltantes.boot &lt;- rnorm(n0, datos$x[Ind] * beta, sd = sigma) datos$y.per[Ind] &lt;- faltantes.boot model.input &lt;- lm(y.per ~ 0 + x, data = datos) beta1[m] &lt;- model.input$coeff sigma1[m] &lt;- summary(model.input)$coeff[2] } beta.input &lt;- mean(beta1) u.bar &lt;- mean(sigma1 ^ 2) B &lt;- var(beta1) beta.sd &lt;- sqrt(u.bar + B + B/M) result &lt;- list(new = datos, beta = beta.input, sd = beta.sd) } La función anterior realiza internamente lo siguiente: En la primera sección del código, desde library(dplyr) hasta sigma1 &lt;- NULL se calcula el número total de observaciones “n”, se eliminan las filas con valores faltantes y se calculan las longitudes de los datos “datos1” sin valores faltantes, “n1”, y los datos faltantes “n0”. También se crea una variable de índice “Ind” que indica qué observaciones tienen valores faltantes, y se inicializan varias variables que se usarán más adelante en la función. Luego, en la sección del bucle for se realiza el proceso de imputación múltiple. Se itera “M” veces, donde en cada iteración se muestran “n1” filas de los datos sin valores faltantes “datos1” con reemplazo, y se ajusta un modelo de regresión lineal con “y” como respuesta y “x” como variable explicativa. Luego se calculan los coeficientes de la regresión y la desviación estándar de los residuos. A continuación, se generan valores de imputación aleatorios para las filas faltantes, utilizando la media condicional y la desviación estándar estimadas en la muestra. Estos valores de imputación se asignan a las filas faltantes en el data frame “datos”. Finalmente, se ajusta un modelo de regresión lineal con la variable de respuesta imputada “y.per” como respuesta y “x” como variable explicativa. Se guardan los coeficientes y las desviaciones estándar de los residuos para cada iteración en las variables “beta1” y “sigma1”, respectivamente. En la última sección del código se calcula el coeficiente de regresión promedio para las iteraciones, “beta.input”, y se calcula la desviación estándar de “beta1” y por último, la estimación de la desviación estándar del beta. Al aplicar la función sobre el conjunto de datos creado, se obtienen las siguientes salidas: datos &lt;- generar() im.bootstrap(datos)$beta ## [1] 10.29741 im.bootstrap(datos)$sd ## [1] 0.2222246 head(im.bootstrap(datos)$new) x y faltantes y.per 0.2173207 0.2872457 Si 0.4423490 0.2953090 1.5860882 Si 1.8694104 0.9609104 11.2309918 Si 11.8596299 0.3119669 5.0508837 No 5.0508837 0.0520595 -0.7714404 Si -0.5380997 0.1597426 3.9523667 No 3.9523667 Nótese que existe una buena dispersión en los valores imputados. nuevos &lt;- im.bootstrap(datos)$new ggplot(data = nuevos, aes(x = x, y = y.per, color = faltantes)) + geom_point() Por otro lado, se ejemplificará la técnica de imputación múltiple para los datos de la encuesta que se utiliza de ejemplo en este texto: encuesta$Income_imp &lt;- encuesta$Income_missin encuesta$Employment_imp &lt;- encuesta$Employment_missin encuesta_obs &lt;- filter(encuesta, !is.na(Income_missin)) encuesta_no_obs &lt;- filter(encuesta, is.na(Income_missin)) n0 &lt;- nrow(encuesta_no_obs) n1 &lt;- nrow(encuesta_obs) Inicialmente, se extraen los datos a imputar y se calculan los tamaños de los datos observados y no observados. M = 10 set.seed(1234) for (ii in 1:M) { vp &lt;- paste0(&quot;Income_vp_&quot;,ii) vp2 &lt;- paste0(&quot;Employment_vp_&quot;,ii) encuesta_temp &lt;- encuesta_obs %&gt;% sample_n(size = n1, replace = TRUE) mod &lt;- lm(Income~ Zone + Sex + Expenditure, data = encuesta_temp) mod.mult &lt;- multinom(Employment~Zone + Sex +Expenditure,data = encuesta_temp) encuesta_no_obs[[vp]] &lt;- predict(mod, encuesta_no_obs) encuesta_obs[[vp]] &lt;- encuesta_obs$Income encuesta_no_obs[[vp2]] &lt;- predict(mod.mult, encuesta_no_obs,type = &quot;class&quot;) encuesta_obs[[vp2]] &lt;- encuesta_obs$Employment } ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1147.241384 ## final value 1127.485787 ## converged ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1177.374646 ## final value 1161.652154 ## converged ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1104.676334 ## final value 1090.350507 ## converged ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1157.667055 ## final value 1138.773189 ## converged ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1145.090973 ## final value 1094.863541 ## converged ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1139.350146 ## final value 1127.134266 ## converged ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1169.810267 ## final value 1141.798357 ## converged ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1165.750155 ## final value 1125.933563 ## converged ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1117.370321 ## final value 1093.345603 ## converged ## # weights: 15 (8 variable) ## initial value 1651.214270 ## iter 10 value 1143.115259 ## final value 1129.053716 ## converged El código anterior realiza lo siguiente: Se crea una cadena de caracteres “vp” y “vp2” mediante la función paste0() concatenando la cadena “Income_vp_” y “Employment_vp_” respectivamente con el número de la iteración actual “ii”. Se crea una muestra aleatoria con reemplazo de tamaño n1 a partir de la base de datos “encuesta_obs” utilizando la función sample_n() del paquete dplyr. Se ajusta un modelo de regresión lineal con la variable “Income” como respuesta y las variables “Zone”, “Sex” y “Expenditure” como covariables utilizando la función lm(). Se ajusta un modelo de regresión multinomial con la variable “Employment” como respuesta y las variables “Zone”, “Sex” y “Expenditure” como covariables utilizando la función multinom() del paquete nnet. Se utiliza el modelo ajustado en el punto 3 para predecir los valores de la variable “Income” en la base de datos “encuesta_no_obs” mediante la función predict(). Se guarda en la base de datos “encuesta_obs” los valores verdaderos de la variable “Income”. Se utiliza el modelo ajustado en el punto 4 para predecir los valores de la variable “Employment” en la base de datos “encuesta_no_obs” mediante la función predict(). Se guarda en la base de datos “encuesta_obs” los valores verdaderos de la variable “Employment”. Se repite el proceso para las siguientes iteraciones. Al finalizar el bucle se obtendrán 20 nuevas variables en las bases de datos “encuesta_no_obs” y “encuesta_obs” correspondientes a las predicciones de “Income” y “Employment” respectivamente, para cada una de las 10 iteraciones del bucle. Una vez corrido el código anterior, se seleccionan las variables de ingresos y sus 10 valores plausibles como se muestra a continuación: dplyr::select(encuesta_no_obs, Income, matches(&quot;Income_vp_&quot;))[1:10,1:4] Income Income_vp_1 Income_vp_2 Income_vp_3 409.87 550.2195 566.0432 567.8296 409.87 561.1194 529.3457 541.8310 90.92 210.5682 225.7715 163.9913 90.92 221.4682 189.0739 137.9927 90.92 210.5682 225.7715 163.9913 135.33 222.6937 237.9191 178.4083 135.33 222.6937 237.9191 178.4083 1539.75 784.8579 801.1103 846.8098 336.00 507.8955 472.7913 439.7467 685.48 593.0111 558.0623 540.9473 A continuación, se grafica la distribución de los ingresos y los 10 valores plausibles observándose que, las distribuciones son muy similares: encuesta &lt;- bind_rows(encuesta_obs, encuesta_no_obs) dat_plot10 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone,Sex,matches(&quot;Income_vp_&quot;)), key = &quot;Caso&quot;, value = &quot;Income2&quot;, -Zone,-Sex) p1 &lt;- ggplot(dat_plot10, aes(x = Income2, col = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_density(data = encuesta ,aes(x = Income), col = &quot;black&quot;, size = 1.2) p1 En el siguiente gráfico se presentan las frecuencias de los valores plausibles para la variable empleado. Se observa también que son muy aproximadas a la variable completa: dat_plot11 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone,Sex, Employment,matches(&quot;Employment_vp_&quot;)), key = &quot;Caso&quot;, value = &quot;Employment2&quot;, -Zone,-Sex) %&gt;% group_by(Caso,Employment2) %&gt;% tally() %&gt;% group_by(Caso) %&gt;% mutate(prop = n/sum(n)) p1 &lt;- ggplot(dat_plot11, aes(x = Employment2, y = prop, fill = Caso, color=&quot;red&quot;)) + geom_bar(stat=&quot;identity&quot;, position = position_dodge(width = 0.5)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + scale_fill_manual(values = c(&quot;Employment&quot; = &quot;black&quot;)) p1 Con los valores plausibles enocntrados anteriormente, se procede a definir el diseño muestral utilizado en este ejemplo y así poder hacer la estimación de los parámetros. A continuación, se define el diseño muestral: library(srvyr) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) Con el diseño anterior, se estiman los ingresos medios para cada valor plausible junto ocn su varianza, como se muestra a continuación: estimacion_vp &lt;- diseno %&gt;% summarise( vp1 = survey_mean(Income_vp_1, vartype = c(&quot;var&quot;)), vp2 = survey_mean(Income_vp_2, vartype = c(&quot;var&quot;)), vp3 = survey_mean(Income_vp_3, vartype = c(&quot;var&quot;)), vp4 = survey_mean(Income_vp_4, vartype = c(&quot;var&quot;)), vp5 = survey_mean(Income_vp_5, vartype = c(&quot;var&quot;)), vp6 = survey_mean(Income_vp_6, vartype = c(&quot;var&quot;)), vp7 = survey_mean(Income_vp_7, vartype = c(&quot;var&quot;)), vp8 = survey_mean(Income_vp_8, vartype = c(&quot;var&quot;)), vp9 = survey_mean(Income_vp_9, vartype = c(&quot;var&quot;)), vp10 =survey_mean(Income_vp_10, vartype = c(&quot;var&quot;))) estimacion_vp vp1 vp1_var vp2 vp2_var vp3 vp3_var vp4 vp4_var vp5 vp5_var vp6 vp6_var vp7 vp7_var vp8 vp8_var vp9 vp9_var vp10 vp10_var 619.3525 845.706 617.6058 844.5521 617.4322 867.5133 617.748 856.7732 619.9729 857.805 617.1098 852.6605 618.3378 860.7981 619.2779 867.9485 616.9401 850.131 615.7853 870.0434 A continuación se presentan los datos anteriores discriminado por promedio y varianza: require(tidyr) (estimacion_vp %&lt;&gt;% tidyr::gather() %&gt;% separate(key, c(&quot;vp&quot;, &quot;estimacion&quot;)) %&gt;% mutate(estimacion = ifelse(is.na(estimacion), &quot;promedio&quot;,&quot;var&quot; )) %&gt;% spread(estimacion,value) %&gt;% mutate(vp = 1:10)) vp promedio var 1 619.3525 845.7060 2 615.7853 870.0434 3 617.6058 844.5521 4 617.4322 867.5133 5 617.7480 856.7732 6 619.9729 857.8050 7 617.1098 852.6605 8 618.3378 860.7981 9 619.2779 867.9485 10 616.9401 850.1310 Por último, para obtener la estimación de la media y su varianza utilizando la imputación múltiple, se realizan los siguientes cálculos que se derivan de las expresiones matemáticas antes mostradas: Media_vp = mean(estimacion_vp$promedio) (Ubar = mean(estimacion_vp$var)) ## [1] 857.3931 (B = var(estimacion_vp$promedio)) ## [1] 1.645758 var_vp = Ubar + (1 + 1/M) (resultado &lt;- data.frame(Media_vp, Media_vp_se = sqrt(var_vp))) Media_vp Media_vp_se 617.9562 29.30005 estimacion_var_vp &lt;- diseno %&gt;% summarise_at(vars(matches(&quot;Income_vp&quot;)), survey_var, vartype = &quot;var&quot; ) Por otro lado, otro parámetro de interés es la varianza de los ingresos. Este parámetro permite medir la variabilidad de los ingresos de los ciudadanos de la base de datos de ejemplo. La forma de estimarla es la misma que para el promedio de los ingresos y se utilizarán los mismo códigos mostrados anteriormente, cambiando el parámetro a estimar: (estimacion_var_vp %&lt;&gt;% tidyr::gather() %&gt;% separate(key, c(&quot;A&quot;, &quot;B&quot;,&quot;vp&quot;, &quot;estimacion&quot;)) %&gt;% mutate(estimacion = ifelse(is.na(estimacion), &quot;promedio&quot;,&quot;var&quot; ), A = NULL, B = NULL, vp = as.numeric(vp)) %&gt;% spread(estimacion,value)) vp promedio var 1 262689.8 3074674460 2 263092.0 3079895581 3 274370.1 3237991671 4 269127.4 3164842634 5 270450.0 3165285304 6 264992.6 3106582542 7 270916.0 3175865575 8 276069.5 3251836316 9 265060.8 3111426636 10 275462.0 3258214990 Por último, se utilizan las ecuaciones mostradas anteriormente: Media_var_vp = mean(estimacion_var_vp$promedio) Ubar = mean(estimacion_var_vp$var) B = var(estimacion_var_vp$promedio) var_var_vp = Ubar + (1 + 1/M)*B resultado$var_vp &lt;- Media_var_vp resultado$var_vp_se &lt;- sqrt(var_var_vp) cbind(Media_var_vp, var_var_vp) Media_var_vp var_var_vp 269223 3191037330 Otro parámetro de interés a estimar es la proporción. A continuación, se realizará la estimación de la proporción utilizando valores plausibles. Para ello se estimará la variable empleado, como se muestra a continuación: estimacion_prop_vp &lt;- lapply(paste0(&quot;Employment_vp_&quot;,1:10), function(vp){diseno %&gt;% group_by_at(vars(Employment = vp)) %&gt;% summarise(prop = survey_mean(vartype = &quot;var&quot;),.groups = &quot;drop&quot;) %&gt;% mutate(vp = vp)}) %&gt;% bind_rows() Se presenta la estimación de la proporción para cada uno de los 10 valores plausibles en cada categoría de la variable: (estimacion_prop_vp %&lt;&gt;% separate(vp, c(&quot;A&quot;, &quot;B&quot;,&quot;vp&quot;)) %&gt;% mutate(A = NULL, B = NULL, vp = as.numeric(vp)) %&gt;% dplyr::select(vp,Employment:prop_var)) %&gt;% slice(1:12L) vp Employment prop prop_var 1 Unemployed 0.0355675 0.0000375 1 Inactive 0.3753767 0.0002041 1 Employed 0.5890558 0.0001669 2 Unemployed 0.0355675 0.0000375 2 Inactive 0.3771091 0.0002035 2 Employed 0.5873234 0.0001661 3 Unemployed 0.0355675 0.0000375 3 Inactive 0.3867965 0.0002056 3 Employed 0.5776360 0.0001790 4 Unemployed 0.0355675 0.0000375 4 Inactive 0.3549487 0.0001915 4 Employed 0.6094838 0.0001524 Por último, utilizando las ecuaciones de Rubin se obtiene la varianza estimada: resultado = estimacion_prop_vp %&gt;% group_by(Employment) %&gt;% summarise(prop_pv = mean(prop), Ubar = mean(prop_var), B = var(prop)) %&gt;% mutate(prop_pv_var = Ubar + (1 + 1/M)*B) |&gt; dplyr::select(Employment, prop_pv, prop_pv_var) resultado Employment prop_pv prop_pv_var Unemployed 0.0355675 0.0000375 Inactive 0.3868270 0.0004665 Employed 0.5776055 0.0004328 "],["referencias.html", "Capítulo 12 Referencias", " Capítulo 12 Referencias Asparouhov, T., &amp; Muthen, B. (2006, August). Multilevel modeling of complex survey data. In Proceedings of the joint statistical meeting in Seattle (pp. 2718-2726). Asparouhov, T. (2006). General multi-level modeling with sampling weights. Communications in Statistics—Theory and Methods, 35(3), 439-460. Pfeffermann, D., Skinner, C. J., Holmes, D. J., Goldstein, H., &amp; Rasbash, J. (1998). Weighting for unequal selection probabilities in multilevel models. Journal of the Royal Statistical Society: series B (statistical methodology), 60(1), 23-40. Cai, T. (2013). Investigation of ways to handle sampling weights for multilevel model analyses. Sociological Methodology, 43(1), 178-219. Finch, W. H., Bolin, J. E., &amp; Kelley, K. (2019). Multilevel modeling using R. Crc Press. Merlo, J., Chaix, B., Ohlsson, H., Beckman, A., Johnell, K., Hjerpe, P., … &amp; Larsen, K. (2006). A brief conceptual tutorial of multilevel analysis in social epidemiology: using measures of clustering in multilevel logistic regression to investigate contextual phenomena. Journal of Epidemiology &amp; Community Health, 60(4), 290-297. Sarndal, C., Swensson, B. &amp;Wretman, J. (1992), Model Assisted Survey Sampling, Springer, New York. Rojas, H. A. G. (2016). Estrategias de muestreo: diseño de encuestas y estimación de parámetros. Ediciones de la U. Santana Sepúlveda, S., &amp; Mateos Farfán, E. (2014). El arte de programar en R: un lenguaje para la estadística. Lumley, T. (2011). Complex surveys: a guide to analysis using R. John Wiley &amp; Sons. Bache, S. M., Wickham, H., Henry, L., &amp; Henry, M. L. (2022). Package ‘magrittr’. Tellez Piñerez, C. F., &amp; Lemus Polanía, D. F. (2015). Estadística Descriptiva y Probabilidad con aplicaciones en R. Fundación Universitaria Los Libertadore. Groves, R. M., Fowler Jr, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., &amp; Tourangeau, R. (2011). Survey methodology. John Wiley &amp; Sons. Tille, Y. &amp; Ardilly, P. (2006), Sampling Methods: Exercises and Solutions, Springer. Gambino, J. G., &amp; do Nascimento Silva, P. L. (2009). Sampling and estimation in household surveys. In Handbook of Statistics (Vol. 29, pp. 407-439). Elsevier. Cochran, W. G. (1977) Sampling Techniques. John Wiley and Sons. Gutiérrez, H. A. (2017) TeachingSampling. R package. Wickham, H., Chang, W., &amp; Wickham, M. H. (2016). Package ‘ggplot2’. Create elegant data visualisations using the grammar of graphics. Version, 2(1), 1-189. Lumley, T. (2020). Package ‘survey’. Available at the following link: https://cran. r-project. org. Hansen, M. H., &amp; Steinberg, J. (1956). Control of errors in surveys. Biometrics, 12(4), 462-474. Heeringa, S. G., West, B. T., &amp; Berglund, P. A. (2017). Applied survey data analysis. chapman and hall/CRC. Valliant, R., Dever, J.A., and Kreuter, F., Practical Tools for Designing and Weighting Survey Samples, Springer, New York, 2013. Valliant, R., Dorfman, A.H., and Royall, R.M., Finite Population Sampling and Inference: A Prediction Approach, John Wiley &amp; Sons, New York, 2000. Loomis, D., Richardson, D.B., and Elliott, L., Poisson regression analysis of ungrouped data, Occupational and Environmental Medicine, 62, 325–329, 2005. Kovar, J.G., Rao, J.N.K., and Wu, C.F.J., Bootstrap and other methods to measure errors in survey estimates, Canadian Journal of Statistics, 16(Suppl.), 25–45, 1988. Binder, D.A. and Kovacevic, M.S., Estimating some measures of income inequality from survey data: An application of the estimating equations approach, Survey Methodology, 21(2), 137–145, 1995. Kovacevic, M. S., &amp; Binder, D. A. (1997). Variance estimation for measures of income inequality and polarization-the estimating equations approach. Journal of Official Statistics, 13(1), 41. Bautista, J. (1998), Diseños de muestreo estadístico, Universidad Nacional de Colombia. Monroy, L. G. D., Rivera, M. A. M., &amp; Dávila, L. R. L. (2018). Análisis estadístico de datos categóricos. Universidad Nacional de Colombia. Kish, L. and Frankel, M.R., Inference from complex samples, Journal of the Royal Statistical Society, Series B, 36, 1–37, 1974. Fuller, W.A., Regression analysis for sample survey, Sankyha, Series C, 37, 117–132, 1975. Shah, B.V., Holt, M.M., and Folsom, R.F., Inference about regression models from sample survey data, Bulletin of the International Statistical Institute, 41(3), 43–57, 1977. Skinner, C.J., Holt, D., and Smith, T.M.F., Analysis of Complex Surveys, John Wiley &amp; Sons, New York, 1989. Binder, D.A., On the variances of asymptotically normal estimators from complex surveys, International Statistical Review, 51, 279–292, 1983. Fuller, W.A., Regression estimation for survey samples (with discussion), Survey Methodology, 28(1), 5–23, 2002. Pfeffermann, D., Modelling of complex survey data: Why model? Why is it a problem? How can we approach it? Survey Methodology, 37(2), 115–136, 2011. Wolter, K.M., Introduction to Variance Estimation (2nd ed.), Springer-Verlag, New York, 2007. Tellez, C. F., &amp; Morales, M. A. (2016). Modelos Estadísticos lineales con aplicaciones en R. Ediciones de la U. Fay, R.E., On adjusting the Pearson Chi-square statistic for cluster sampling, In Proceedings of the Social Statistics Section, American Statistical Association, Washington, DC, 402–405, 1979. Fay, R.E., A jack-knifed chi-squared test for complex samples, Journal of the American Statistical Association, 80, 148–157, 1985. Fellegi, I.P., Approximate tests of independence and goodness of fit based on stratified multistage samples, Journal of the American Statistical Association, 75, 261–268, 1980. Thomas, D.R. and Rao, J.N.K., Small-sample comparisons of level and power for simple goodness-of-fit statistics under cluster sampling, Journal of the American Statistical Association, 82, 630–636, 1987. Rao, J.N.K. and Scott, A.J., On chi-squared test for multiway contingency tables with cell proportions estimated from survey data, The Annals of Statistics, 12, 46–60, 1984. Van Buuren, S., Flexible Imputation of Missing Data, Chapman &amp; Hall, Boca Raton, FL, 2012. Carpenter, J.R. and Kenward, M.G., Multiple Imputation and Its Application, John Wiley &amp; Sons, Chichester, West Sussex, UK, 2013. Berglund, P.A. and Heeringa, S.G., Multiple Imputation of Missing Data Using SAS®, SAS Institute Inc., Cary, NC, 2014. Chambers, R.L., Steel, D.G., Wang, S., and Welsh, A.H., Maximum Likelihood Estimation for Sample Surveys, Chapman &amp; Hall, Boca Raton, FL, 2012. Zhou, H., Elliott, M.R., and Raghunathan, T.E., Multiple imputation in two-stage cluster samples using the weighted finite population Bayesian Bootstrap, Journal of Survey Statistics and Methodology, 4, 139–170, 2016a. Zhou, H., Elliott, M.R., and Raghunathan, T.E., Synthetic multiple-imputation procedure for multistage complex samples, Journal of Official Statistics, 32(1), 231–256, 2016b. Kim, J.K. and Shao, J., Statistical Methods for Handling Incomplete Data, Chapman &amp; Hall, Boca Raton, FL, 2014. Kim, J.K. and Fuller, W.A., Fractional Hotdeck imputation, Biometrika, 89, 470–477, 2004. StataCorp., Release 14, P Manual, STATA Survey Data Manual, Stata Press, College Station, TX, 2015. Raghunathan, T.E., Missing Data Analysis in Practice, Chapman &amp; Hall/CRC Interdisciplinary Statistics, Boca Raton, FL, 2016. Rubin, D. B. (1987). Multiple imputation for survey nonresponse. Goldstein, H. (2011). Multilevel statistical models (Vol. 922). John Wiley &amp; Sons. Data analysis using regression and multilevel/hierarchical models” de Andrew Gelman y Jennifer Hill (2006) Sophia, R. H., &amp; Skrondal, A. (2012). Multilevel and longitudinal modeling using Stata. STATA press. Browne, W. J., &amp; Draper, D. (2006). A comparison of Bayesian and likelihood-based methods for fitting multilevel models. "]]
