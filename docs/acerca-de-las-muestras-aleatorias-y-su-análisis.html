<!DOCTYPE html>
<html lang="es" xml:lang="es">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.1 Acerca de las muestras aleatorias y su análisis | Análisis de encuestas con R</title>
  <meta name="description" content="Este es el repositorio del libro Análisis de encuestas con R." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="9.1 Acerca de las muestras aleatorias y su análisis | Análisis de encuestas con R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Este es el repositorio del libro Análisis de encuestas con R." />
  <meta name="github-repo" content="psirusteam/HHS-Handbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.1 Acerca de las muestras aleatorias y su análisis | Análisis de encuestas con R" />
  
  <meta name="twitter:description" content="Este es el repositorio del libro Análisis de encuestas con R." />
  

<meta name="author" content="Andrés Gutiérrez, Cristian Téllez, Stalyn Guerrero" />


<meta name="date" content="2023-03-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modelos-lineales-generalizados-introducción-a-la-pseudo-máxima-verosimilitud.html"/>
<link rel="next" href="método-de-máxima-verosimilitud.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Análisis de encuestas con R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a></li>
<li class="chapter" data-level="1" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="conceptos-básicos-en-encuestas-de-hogares.html"><a href="conceptos-básicos-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>2</b> Conceptos básicos en encuestas de hogares</a>
<ul>
<li class="chapter" data-level="2.1" data-path="universo-de-estudio-y-población-objetivo.html"><a href="universo-de-estudio-y-población-objetivo.html"><i class="fa fa-check"></i><b>2.1</b> Universo de estudio y población objetivo</a></li>
<li class="chapter" data-level="2.2" data-path="unidades-de-análisis.html"><a href="unidades-de-análisis.html"><i class="fa fa-check"></i><b>2.2</b> Unidades de análisis</a></li>
<li class="chapter" data-level="2.3" data-path="unidades-de-muestreo.html"><a href="unidades-de-muestreo.html"><i class="fa fa-check"></i><b>2.3</b> Unidades de muestreo</a></li>
<li class="chapter" data-level="2.4" data-path="marcos-de-muestreo.html"><a href="marcos-de-muestreo.html"><i class="fa fa-check"></i><b>2.4</b> Marcos de muestreo</a></li>
<li class="chapter" data-level="2.5" data-path="selección-de-una-muestra.html"><a href="selección-de-una-muestra.html"><i class="fa fa-check"></i><b>2.5</b> Selección de una muestra</a></li>
<li class="chapter" data-level="2.6" data-path="motivación.html"><a href="motivación.html"><i class="fa fa-check"></i><b>2.6</b> Motivación</a></li>
<li class="chapter" data-level="2.7" data-path="muestreo-aleatorio-simple-en-dos-etapas-estratificado.html"><a href="muestreo-aleatorio-simple-en-dos-etapas-estratificado.html"><i class="fa fa-check"></i><b>2.7</b> Muestreo aleatorio simple en dos etapas estratificado</a></li>
<li class="chapter" data-level="2.8" data-path="práctica-en-r.html"><a href="práctica-en-r.html"><i class="fa fa-check"></i><b>2.8</b> Práctica en <code>R</code></a></li>
<li class="chapter" data-level="2.9" data-path="calibrando-con-r.html"><a href="calibrando-con-r.html"><i class="fa fa-check"></i><b>2.9</b> Calibrando con <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="manejando-una-base-de-encuestas-de-hogares-con-r.html"><a href="manejando-una-base-de-encuestas-de-hogares-con-r.html"><i class="fa fa-check"></i><b>3</b> Manejando una base de encuestas de hogares con R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="fundamentos-básicos-de-r-y-rstudio.html"><a href="fundamentos-básicos-de-r-y-rstudio.html"><i class="fa fa-check"></i><b>3.1</b> Fundamentos básicos de R y Rstudio</a></li>
<li class="chapter" data-level="3.2" data-path="algunas-librerías-de-interés.html"><a href="algunas-librerías-de-interés.html"><i class="fa fa-check"></i><b>3.2</b> Algunas librerías de interés</a></li>
<li class="chapter" data-level="3.3" data-path="cración-de-proyectos-en-r.html"><a href="cración-de-proyectos-en-r.html"><i class="fa fa-check"></i><b>3.3</b> Cración de proyectos en <code>R</code></a></li>
<li class="chapter" data-level="3.4" data-path="lectura-de-las-bases-de-datos-y-manipulación.html"><a href="lectura-de-las-bases-de-datos-y-manipulación.html"><i class="fa fa-check"></i><b>3.4</b> Lectura de las bases de datos y manipulación</a></li>
<li class="chapter" data-level="3.5" data-path="el-operador-pipe.html"><a href="el-operador-pipe.html"><i class="fa fa-check"></i><b>3.5</b> El operador <code>pipe</code></a></li>
<li class="chapter" data-level="3.6" data-path="funciones-mutate-summarise-y-group_by-en-encuestas-de-hogares.html"><a href="funciones-mutate-summarise-y-group_by-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>3.6</b> Funciones <strong>mutate, summarise y group_by</strong> en encuestas de hogares</a></li>
<li class="chapter" data-level="3.7" data-path="medidas-descriptivos-y-reflexiones.html"><a href="medidas-descriptivos-y-reflexiones.html"><i class="fa fa-check"></i><b>3.7</b> Medidas descriptivos y reflexiones</a></li>
<li class="chapter" data-level="3.8" data-path="algunas-reflexiones-generales.html"><a href="algunas-reflexiones-generales.html"><i class="fa fa-check"></i><b>3.8</b> Algunas reflexiones generales</a></li>
<li class="chapter" data-level="3.9" data-path="observación-importante.html"><a href="observación-importante.html"><i class="fa fa-check"></i><b>3.9</b> <strong>¡Observación importante!</strong></a></li>
<li class="chapter" data-level="3.10" data-path="medias-y-totales.html"><a href="medias-y-totales.html"><i class="fa fa-check"></i><b>3.10</b> Medias y totales</a></li>
<li class="chapter" data-level="3.11" data-path="medianas-y-percentiles.html"><a href="medianas-y-percentiles.html"><i class="fa fa-check"></i><b>3.11</b> Medianas y percentiles</a></li>
<li class="chapter" data-level="3.12" data-path="varianza-desviación-estándar-y-rangos.html"><a href="varianza-desviación-estándar-y-rangos.html"><i class="fa fa-check"></i><b>3.12</b> Varianza, desviación estándar y rangos</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="análisis-de-las-variables-continuas-en-encuestas-de-hogares.html"><a href="análisis-de-las-variables-continuas-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>4</b> Análisis de las variables continuas en encuestas de hogares</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lectura-de-bases-de-datos-y-definición-del-diseño-muestral.html"><a href="lectura-de-bases-de-datos-y-definición-del-diseño-muestral.html"><i class="fa fa-check"></i><b>4.1</b> Lectura de bases de datos y definición del diseño muestral</a></li>
<li class="chapter" data-level="4.2" data-path="análisis-gráfico-histogramas-y-boxplot.html"><a href="análisis-gráfico-histogramas-y-boxplot.html"><i class="fa fa-check"></i><b>4.2</b> Análisis gráfico: Histogramas y Boxplot</a></li>
<li class="chapter" data-level="4.3" data-path="estimación-puntual.html"><a href="estimación-puntual.html"><i class="fa fa-check"></i><b>4.3</b> Estimación puntual</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#estimación-de-totales-e-intervalos-de-confianza"><i class="fa fa-check"></i><b>4.3.1</b> Estimación de totales e intervalos de confianza</a></li>
<li class="chapter" data-level="4.3.2" data-path="estimación-puntual.html"><a href="estimación-puntual.html#estimación-de-la-media-e-intervalo-de-confianza"><i class="fa fa-check"></i><b>4.3.2</b> Estimación de la media e intervalo de confianza</a></li>
<li class="chapter" data-level="4.3.3" data-path="estimación-puntual.html"><a href="estimación-puntual.html#estimación-de-medidas-de-dispersión-y-localización"><i class="fa fa-check"></i><b>4.3.3</b> Estimación de medidas de dispersión y localización</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="estimación-del-coeficiente-de-ginni-en-encuestas-de-hogares.html"><a href="estimación-del-coeficiente-de-ginni-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>4.4</b> Estimación del coeficiente de Ginni en encuestas de hogares</a></li>
<li class="chapter" data-level="4.5" data-path="análisis-de-la-relación-entre-dos-variable-continuas.html"><a href="análisis-de-la-relación-entre-dos-variable-continuas.html"><i class="fa fa-check"></i><b>4.5</b> Análisis de la relación entre dos variable continuas</a></li>
<li class="chapter" data-level="4.6" data-path="prueba-de-hipótesis-para-la-diferencia-de-medias-en-encuestas-de-hogares.html"><a href="prueba-de-hipótesis-para-la-diferencia-de-medias-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>4.6</b> Prueba de hipótesis para la diferencia de medias en encuestas de hogares</a></li>
<li class="chapter" data-level="4.7" data-path="estimando-razones-en-encuestas-de-hogares.html"><a href="estimando-razones-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>4.7</b> Estimando razones en encuestas de hogares</a></li>
<li class="chapter" data-level="4.8" data-path="estimando-contrastes-en-encuestas-de-hogares.html"><a href="estimando-contrastes-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>4.8</b> Estimando contrastes en encuestas de hogares</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="análisis-de-variables-categóricas-en-encuestas-de-hogares.html"><a href="análisis-de-variables-categóricas-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>5</b> Análisis de variables categóricas en encuestas de hogares</a>
<ul>
<li class="chapter" data-level="5.1" data-path="estimaciones-de-totales.html"><a href="estimaciones-de-totales.html"><i class="fa fa-check"></i><b>5.1</b> Estimaciones de totales</a></li>
<li class="chapter" data-level="5.2" data-path="estimación-de-proporciones.html"><a href="estimación-de-proporciones.html"><i class="fa fa-check"></i><b>5.2</b> Estimación de proporciones</a></li>
<li class="chapter" data-level="5.3" data-path="tablas-cruzadas..html"><a href="tablas-cruzadas..html"><i class="fa fa-check"></i><b>5.3</b> Tablas cruzadas.</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-de-regresión-bajo-diseños-de-muestreo-complejos.html"><a href="modelos-de-regresión-bajo-diseños-de-muestreo-complejos.html"><i class="fa fa-check"></i><b>6</b> Modelos de regresión bajo diseños de muestreo complejos</a>
<ul>
<li class="chapter" data-level="6.1" data-path="estimación-de-los-parámetros-en-un-modelo-de-regresión-con-muestras-complejas..html"><a href="estimación-de-los-parámetros-en-un-modelo-de-regresión-con-muestras-complejas..html"><i class="fa fa-check"></i><b>6.1</b> Estimación de los parámetros en un modelo de regresión con muestras complejas.</a></li>
<li class="chapter" data-level="6.2" data-path="diagnóstico-del-modelo.html"><a href="diagnóstico-del-modelo.html"><i class="fa fa-check"></i><b>6.2</b> Diagnóstico del modelo</a></li>
<li class="chapter" data-level="6.3" data-path="inferencia-sobre-los-parámetros-del-modelo.html"><a href="inferencia-sobre-los-parámetros-del-modelo.html"><i class="fa fa-check"></i><b>6.3</b> Inferencia sobre los parámetros del Modelo</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="gráficas-en-r.html"><a href="gráficas-en-r.html"><i class="fa fa-check"></i><b>7</b> Gráficas en R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="histogramas-para-graficar-variables-continuas..html"><a href="histogramas-para-graficar-variables-continuas..html"><i class="fa fa-check"></i><b>7.1</b> Histogramas para graficar variables continuas.</a></li>
<li class="chapter" data-level="7.2" data-path="agregando-densidades-y-graficando-boxplot.html"><a href="agregando-densidades-y-graficando-boxplot.html"><i class="fa fa-check"></i><b>7.2</b> Agregando densidades y graficando Boxplot</a></li>
<li class="chapter" data-level="7.3" data-path="scaterplot.html"><a href="scaterplot.html"><i class="fa fa-check"></i><b>7.3</b> Scaterplot</a></li>
<li class="chapter" data-level="7.4" data-path="diagrama-de-barras-para-variables-categoricas.html"><a href="diagrama-de-barras-para-variables-categoricas.html"><i class="fa fa-check"></i><b>7.4</b> Diagrama de barras para variables categoricas</a></li>
<li class="chapter" data-level="7.5" data-path="creando-mapas.html"><a href="creando-mapas.html"><i class="fa fa-check"></i><b>7.5</b> Creando mapas</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modelos-lineales-generalizados-en-encuestas-de-hogares.html"><a href="modelos-lineales-generalizados-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>8</b> Modelos lineales generalizados en encuestas de hogares</a>
<ul>
<li class="chapter" data-level="8.1" data-path="prueba-de-independencia-f.html"><a href="prueba-de-independencia-f.html"><i class="fa fa-check"></i><b>8.1</b> Prueba de independencia F</a></li>
<li class="chapter" data-level="8.2" data-path="estadístico-de-wald.html"><a href="estadístico-de-wald.html"><i class="fa fa-check"></i><b>8.2</b> Estadístico de Wald</a></li>
<li class="chapter" data-level="8.3" data-path="modelo-log-lineal-para-tablas-de-contingencia.html"><a href="modelo-log-lineal-para-tablas-de-contingencia.html"><i class="fa fa-check"></i><b>8.3</b> Modelo log lineal para tablas de contingencia</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="modelos-lineales-generalizados-introducción-a-la-pseudo-máxima-verosimilitud.html"><a href="modelos-lineales-generalizados-introducción-a-la-pseudo-máxima-verosimilitud.html"><i class="fa fa-check"></i><b>9</b> Modelos lineales generalizados: Introducción a la pseudo máxima verosimilitud</a>
<ul>
<li class="chapter" data-level="9.1" data-path="acerca-de-las-muestras-aleatorias-y-su-análisis.html"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html"><i class="fa fa-check"></i><b>9.1</b> Acerca de las muestras aleatorias y su análisis</a></li>
<li class="chapter" data-level="9.2" data-path="método-de-máxima-verosimilitud.html"><a href="método-de-máxima-verosimilitud.html"><i class="fa fa-check"></i><b>9.2</b> Método de Máxima Verosimilitud</a></li>
<li class="chapter" data-level="9.3" data-path="método-de-máxima-pseudo-verosimilitud.html"><a href="método-de-máxima-pseudo-verosimilitud.html"><i class="fa fa-check"></i><b>9.3</b> Método de Máxima Pseudo-Verosimilitud</a></li>
<li class="chapter" data-level="9.4" data-path="modelo-multinomial.html"><a href="modelo-multinomial.html"><i class="fa fa-check"></i><b>9.4</b> Modelo multinomial</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="imputación-múltiple-en-encuestas-de-hogares.html"><a href="imputación-múltiple-en-encuestas-de-hogares.html"><i class="fa fa-check"></i><b>10</b> Imputación múltiple en encuestas de hogares</a>
<ul>
<li class="chapter" data-level="10.1" data-path="imputación-por-la-media-no-condicional..html"><a href="imputación-por-la-media-no-condicional..html"><i class="fa fa-check"></i><b>10.1</b> Imputación por la media no condicional.</a></li>
<li class="chapter" data-level="10.2" data-path="imputación-por-la-media-condicional.html"><a href="imputación-por-la-media-condicional.html"><i class="fa fa-check"></i><b>10.2</b> Imputación por la media condicional</a></li>
<li class="chapter" data-level="10.3" data-path="imputación-por-hot-deck-y-cold-deck.html"><a href="imputación-por-hot-deck-y-cold-deck.html"><i class="fa fa-check"></i><b>10.3</b> Imputación por Hot-deck y Cold-deck</a></li>
<li class="chapter" data-level="10.4" data-path="imputación-por-regresión.html"><a href="imputación-por-regresión.html"><i class="fa fa-check"></i><b>10.4</b> Imputación por regresión</a></li>
<li class="chapter" data-level="10.5" data-path="imputación-por-el-vecino-más-cercano.html"><a href="imputación-por-el-vecino-más-cercano.html"><i class="fa fa-check"></i><b>10.5</b> Imputación por el vecino más cercano</a></li>
<li class="chapter" data-level="10.6" data-path="imputación-por-el-vecino-más-cercano-con-regresión.html"><a href="imputación-por-el-vecino-más-cercano-con-regresión.html"><i class="fa fa-check"></i><b>10.6</b> Imputación por el vecino más cercano con regresión</a></li>
<li class="chapter" data-level="10.7" data-path="introducción-a-la-imputación-múltiple..html"><a href="introducción-a-la-imputación-múltiple..html"><i class="fa fa-check"></i><b>10.7</b> Introducción a la imputación múltiple.</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i><b>11</b> Referencias</a></li>
<li class="divider"></li>
<li><a Análisis de encuestas con R </a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Análisis de encuestas con R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="acerca-de-las-muestras-aleatorias-y-su-análisis" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Acerca de las muestras aleatorias y su análisis<a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#acerca-de-las-muestras-aleatorias-y-su-análisis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Hablemos primero de la selección sin reemplazo, en donde una muestra seleccionada está conformada por algunos elementos de la población que no se repiten. Para seleccionar una muestra sin reemplazo de tamaño <span class="math inline">\(n=3\)</span>, de una población de tamaño <span class="math inline">\(N=5\)</span>, el proceso de selección puede ser de la siguiente manera. Se escoge una unidad de las cinco posibles, luego se selecciona una unidad de las cuatro restantes, y por último, una unidad de las tres restantes. Esto hace que el proceso de selección de la muestra no se lleve a cabo de forma independiente. Por ejemplo, si el muestreo es aleatorio simple, la probabilidad de selección de la primera unidad es 1/5, la probabilidad de selección de la segunda unidad es 1/4 y así sucesivamente. Por otro lado, cuando el muestreo es con reemplazo, la selección se realiza de forma independiente puesto que se trata de realizar el mismo ensayo (seleccionar una unidad de cinco posibles) tres veces, sin importar que las unidades tengan diferentes probabilidades de selección.</p>
<p>Por otra parte, es bien sabido que la teoría de muestreo establece que el valor de la característica de interés, <span class="math inline">\(y_k\)</span>, es eso, un valor; por tanto, no es aleatorio. Luego, es incorrecto decir que <span class="math inline">\(y_k\)</span> es una variable aleatoria asociada con alguna distribución de probabilidad. Recuerde que en el muestreo lo único aleatorio en la inferencia es la muestra. Ahora, no significa que no podamos construir variables aleatorias en muestreo. Por ejemplo, construyamos la siguiente variable aleatoria <span class="math inline">\(X_i\)</span> (<span class="math inline">\(i=1,2,3\)</span>) definida como el valor de la característica de interés en el individuo <span class="math inline">\(k\)</span>-ésimo, seleccionado en la <span class="math inline">\(i\)</span>-ésima extracción. En este caso, existen tres variables aleatorias, puesto que la muestra es de tamaño tres.</p>
<p>Si consideramos un muestreo aleatorio sin reemplazo, la primera variable aleatoria <span class="math inline">\(X_1\)</span>, podrá tomar cualquiera de los siguiente cinco valores: <span class="math inline">\(y_1, y_2, y_3, y_4, y_5\)</span>. La segunda variable aleatoria <span class="math inline">\(X_2\)</span>, solo podrá tomar cuatro valores, puesto que <span class="math inline">\(X_1\)</span> ya fue realizada, y la tercera variable aleatoria <span class="math inline">\(X_3\)</span> solo podrá tomar tres valores, puesto que <span class="math inline">\(X_1\)</span> y <span class="math inline">\(X_2\)</span> ya fueron realizadas. Esto hace que <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> y <span class="math inline">\(X_3\)</span> no constituya una sucesión de variables aleatorias independientes (puesto que la selección sin reemplazo no es un proceso independiente) ni idénticamente distribuidas (puesto que ni siquiera su espacio muestral es el mismo: <span class="math inline">\(X_1\)</span> puede tomar cinco valores, <span class="math inline">\(X_2\)</span> solo cuatro y <span class="math inline">\(X_3\)</span> solo tres). Lo cual quiere decir que a partir de un muestreo sin reemplazo (ni siquiera el tan mencionado muestreo aleatorio simple) no es posible construir una muestra aleatoria, como las que aparecen en los libros de teoría estadística.</p>
<p>Sin embargo, algo muy distinto sucede con el muestreo con reemplazo. Cuando construimos las variables aleatorias <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> y <span class="math inline">\(X_3\)</span>, resulta ser que ellas sí conforman una sucesión de variables aleatorias independientes (puesto que el muestreo con reemplazo sí define un proceso de extracciones independientes) e idénticamente distribuidas (puesto que conservan el mismo espacio muestral y mantienen la probabilidad de selección). Es decir, <span class="math inline">\(X_1\)</span> puede tomar los valores <span class="math inline">\(y_1, \ldots, y_5\)</span>. La probabilidad de que <span class="math inline">\(X_1=y_1\)</span> es <span class="math inline">\(p_1\)</span>, la probabilidad de selección del primer elemento; la probabilidad de que <span class="math inline">\(X_1=y_2\)</span> es <span class="math inline">\(p_2\)</span>, la probabilidad de selección del segundo elemento y así sucesivamente hasta obtener que la probabilidad de que <span class="math inline">\(X_1=y_5\)</span> es <span class="math inline">\(p_5\)</span>, la probabilidad de selección del primer elemento primer elemento. La misma distribución la tienen <span class="math inline">\(X_2\)</span> y <span class="math inline">\(X_3\)</span>. Por lo tanto, <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> y <span class="math inline">\(X_3\)</span> conforman una muestra aleatoria, como las que aparecen en los libros clásicos de inferencia estadística.</p>
<p>Entonces, hemos llegado a un punto sin retorno, en donde la conclusión es que, si la muestra fue seleccionada con reemplazo, entonces podemos inducir una muestra aleatoria. Sin embargo, existen muchas variantes en el muestreo con reemplazo. A continuación, vamos a dilucidar cuál de ellas es la indicada para analizar la muestra de acuerdo con la teoría de los libros de inferencia.</p>
<p>En primera instancia, veamos que para que la esperanza (bajo el diseño de muestreo <span class="math inline">\(p\)</span>) de cualquier variable aleatoria <span class="math inline">\(X_i\)</span> sea igual a la media poblacional, es necesario que, para todos los individuos en la población, la probabilidad de selección sea idéntica e igual a <span class="math inline">\(1/N\)</span>, como se muestra a continuación:</p>
<p><span class="math display">\[
E_p(X_i)=\sum_{k \in U} y_k Pr(X_i = Y_k) = \sum_{k \in U} y_k p_k
= \frac{t_y}{N} = \bar{y}_U=\mu_N
\]</span></p>
<p>De la misma manera, para que la varianza de cualquier variable aleatoria <span class="math inline">\(X_i\)</span> sea igual a la varianza poblacional, se requiere la misma condición, puesto que:</p>
<p><span class="math display">\[
Var_p(X_i)
= \sum_{k \in U} (y_k - \bar{y}_U)^2 p_k
= \frac{1}{N}\sum_{k \in U} (y_k - \bar{y}_U)^2  = S^2_{y_U} = \sigma^2_N
\]</span></p>
<p>Por lo tanto, la esperanza y la varianza de un estimador clásico como <span class="math inline">\(\bar{X}\)</span> solo coincidierón con los bien conocidos resultados de la inferencia clásica cuando el muestreo haya sido aleatorio simple con reemplazo. De otra forma, no se tienen las, bien conocidas, propiedades de esta estadística que implican que su esperanza es <span class="math inline">\(E(\bar{X}) = \mu_N\)</span> y su varianza es <span class="math inline">\(Var(\bar{X}) = \frac{\sigma^2_N}{n}\)</span>.</p>
<p>Este razonamiento de aplicarse de la misma forma para pruebas de hipótesis, construcción de intervalos de confianza, modelos de regresión, y hasta diseño de experimentos. Ahora, para una encuesta cuyos datos no fueron extraídos de manera aleatoria simple con reemplazo, la manera correcta de analizarla confiadamente es incluir los pesos de muestreo en todas las técnicas y metodologías estadísticas, ya sean regresiones simples y logísticas o simples varianzas del promedio.</p>
<p><em>Modelos de superpoblación</em></p>
<p>Suponga que la estimación de máxima verosimilitud es apropiada para muestras aleatorias simples. Por ejemplo, modelos de regresión simple, múltiple, regresión logística, entre otros. Bajo este esquema, se asume que la función de densidad poblacional es <span class="math inline">\(f(y | \theta)\)</span> donde <span class="math inline">\(\theta\)</span> es el parámetro de interés. Con una réplica del ejemplo que David Binder utiliza en un artículo del año 2011 (una excelente lectura para quienes ha seguido el trabajo de Ken Brewer), se introducen algunos conceptos que son de utilidad. Finalmente, todos los resultados se van a plasmar en simulaciones de Monte Carlo, algunas veces anidadas.</p>
<p>Suponga que se generaron <span class="math inline">\(N=100\)</span> realizaciones de variables aleatorias independientes distribuidas Bernoulli con parámetro de interés <span class="math inline">\(\theta=0.3\)</span>. Los datos que se obtienen se muestran a continuación:</p>
<p>1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0
0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0
0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0</p>
<p>En esta población finita, que fue generada a partir de un modelo probabilístico (llamado modelo de superpoblación), hay 28 éxitos.</p>
<p><em>Primer proceso inferencial: el modelo</em></p>
<p>En este apartado, es notable que la medida de probabilidad que rige la inferencia hasta el momento sea la inducida por la distribución binomial con parámetro 0.3. De esta manera, el estimador insesgado de mínima varianza (todas estas propiedades obtenidas con base en la distribución binomial) está dado por el promedio poblacional. Nótese que la inferencia utiliza todos los datos de la población. Ahora, para reproducirlo computacionalmente, basta con simular muchas poblaciones de 100 variables aleatorias independientes distribuidas Bernoulli con parámetro desconocido <span class="math inline">\(\theta\)</span>=0.3.</p>
<p>Como es bien sabido, bajo la perspectiva de los modelos poblacionales y la inferencia estadística clásica, el estimador <span class="math inline">\(\bar{y}_U = \frac{\sum_U y_k}{N}\)</span> es insesgado. Para corroborarlo, es posible introducir la siguiente simulación de Monte Carlo.</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb454-2"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-2" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fl">0.3</span></span>
<span id="cb454-3"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-3" aria-hidden="true" tabindex="-1"></a>nsim1 <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb454-4"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-4" aria-hidden="true" tabindex="-1"></a>Est0<span class="ot">=</span><span class="fu">rep</span>(<span class="cn">NA</span>,nsim1)</span>
<span id="cb454-5"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb454-6"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim1){</span>
<span id="cb454-7"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-7" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="fu">rbinom</span>(N, <span class="dv">1</span>, theta)</span>
<span id="cb454-8"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-8" aria-hidden="true" tabindex="-1"></a>Est0[i]<span class="ot">=</span><span class="fu">mean</span>(y)</span>
<span id="cb454-9"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb454-10"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb454-11"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-11" aria-hidden="true" tabindex="-1"></a>Esp0 <span class="ot">=</span> <span class="fu">mean</span>(Est0)</span>
<span id="cb454-12"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb454-13"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb454-13" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(theta, Esp0)  </span></code></pre></div>
<pre><code>##      theta    Esp0
## [1,]   0.3 0.30052</code></pre>
<p><em>Segundo proceso inferencial: el muestreo</em></p>
<p>En el primer proceso inferencial, se asume que las variables de estudio son realizaciones de variables aleatorias gobernadas por un modelo probabilístico. Sin embargo, un razonamiento muy válido es que en cualquier población finita en particular, los valores de la medición son fijos aunque desconocidos y no siguen ningún modelo probabilístico;
es decir, no corresponden a realizaciones de variables aleatorias. Por ejemplo, suponga que para esa misma población del ejemplo anterior el dato uno corresponde a un individuo desempleado y el dato cero corresponde a un individuo empleado.</p>
<p>Por otra parte, asuma que la población está subdividida en conglomerados, que pueden ser llamados hogares. De esta forma, nuestra población finita toma la siguiente caracterización, mediante una partición de <span class="math inline">\(N_{I}=27\)</span> hogares:</p>
<p>(1 1 0) (1 0) (0 0 0 0 0 0 1) (1 0) (0 0 0 0 0 0 1) (0 0
1) (0 0 0 0 0 0 0 1) (0 0 1) (0 0 0 1) (0 0 0 0 1) (0 0
0 0 0 0 0 1) (1 0) (1 0) (0 0 1) (1 0) (0 0 1) (1 0) (0 1)
(0 0 0 1) (0 0 1) (1 1 0) (0 0 0 0 1) (0 1) (0 1) (0 0 0 0
0 0 0 0 0 1) (0 1) (0)</p>
<p>El proceso de aglomeración en hogares es obviamente artificioso en este ejemplo, pero ilustra que en la vida real las poblaciones finitas siempre están aglomeradas. Suponga por otra parte que tomamos una muestra <span class="math inline">\(S_{I}\)</span> de <span class="math inline">\(n_{I}\)</span> hogares y en cada hogar seleccionado realizamos un censo; además la selección de los hogares se hará aleatoriamente, sin reemplazo y con probabilidades de inclusión <span class="math inline">\(\pi_{Ii}\)</span> proporcionales
al tamaño del hogar <span class="math inline">\(N_{i}\)</span>. Siendo la característica de interés <span class="math inline">\(y_{k}\)</span>, el estado del individuo en la fuerza laboral (1, si está desempleado y 0, en otro caso); entonces es bien sabido que bajo este esquema de muestreo un estimador insesgado para la proporción de desempleados <span class="math inline">\(\bar{y}_{U}\)</span> es el siguiente:</p>
<p><span class="math display">\[
\bar{y}_{\pi S}=\sum_{i\in S_{I}}\frac{t_{y_{i}}}{\pi_{Ii}}=\frac{\sum_{i\in S_{I}}\bar{y}_{i}}{n_{I}}
\]</span>
En donde <span class="math inline">\(\bar{y}_{i}=\frac{t_{y_{i}}}{N_{i}}\)</span> es la proporción de desempleados en el hogar <span class="math inline">\(i\)</span>-ésimo, <span class="math inline">\(t_{y_{i}}\)</span> es el total de desempleados en el hogar <span class="math inline">\(i\)</span>-ésimo, <span class="math inline">\(N_{i}\)</span> es el número de individuos en el hogar y <span class="math inline">\(n_{I}\)</span> es el número de hogares seleccionados. Por otro lado, un estimador ingenuo, correspondiente a la proporción de desempleados
en la muestra, que asume que el agrupamiento de los valores no interfiere
en el proceso de inferencia e ignora el diseño de muestreo es el siguiente:</p>
<p><span class="math display">\[
\bar{y}_{S}=\frac{\sum_{i\in S_{I}}t_{y_{i}}}{\sum_{i\in S_{I}}N_{i}}
\]</span></p>
<p>En términos generales el siguiente esquema trata de reproducir gráficamente este proceso de inferencia, en donde un gran número de muestras podrían haber sido extraídas siguiendo el diseño de muestreo. Con la siguiente simulación de Monte Carlo se comprueba fácilmente que es insesgado, mientras que es sesgado:</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(TeachingSampling)</span>
<span id="cb456-2"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">=</span><span class="dv">100</span></span>
<span id="cb456-3"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-3" aria-hidden="true" tabindex="-1"></a>theta<span class="ot">=</span><span class="fl">0.3</span></span>
<span id="cb456-4"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-4" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="fu">rbinom</span>(N, <span class="dv">1</span>, theta)</span>
<span id="cb456-5"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-5" aria-hidden="true" tabindex="-1"></a>theta_N<span class="ot">=</span><span class="fu">mean</span>(y)</span>
<span id="cb456-6"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-6" aria-hidden="true" tabindex="-1"></a>nsim2<span class="ot">=</span><span class="dv">1000</span></span>
<span id="cb456-7"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-7" aria-hidden="true" tabindex="-1"></a>Est1<span class="ot">=</span>Est2<span class="ot">=</span><span class="fu">rep</span>(<span class="cn">NA</span>,nsim2)</span>
<span id="cb456-8"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-9"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-9" aria-hidden="true" tabindex="-1"></a><span class="co">#-----Creación de los clusters---------</span></span>
<span id="cb456-10"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-11"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-11" aria-hidden="true" tabindex="-1"></a>clus<span class="ot">=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">which</span>((y[<span class="sc">-</span>N]<span class="sc">-</span>y[<span class="sc">-</span><span class="dv">1</span>])<span class="sc">!=</span><span class="dv">0</span>)<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb456-12"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-12" aria-hidden="true" tabindex="-1"></a>NI<span class="ot">=</span>(<span class="fu">length</span>(clus)<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb456-13"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-13" aria-hidden="true" tabindex="-1"></a>Ind<span class="ot">=</span><span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span>N, <span class="at">ncol=</span>NI)</span>
<span id="cb456-14"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-14" aria-hidden="true" tabindex="-1"></a>Tamaños<span class="ot">=</span>clus[<span class="sc">-</span><span class="dv">1</span>]<span class="sc">-</span>clus[<span class="sc">-</span>(<span class="fu">length</span>(clus))]</span>
<span id="cb456-15"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-16"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(l <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(clus)<span class="sc">-</span><span class="dv">1</span>)){</span>
<span id="cb456-17"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-17" aria-hidden="true" tabindex="-1"></a>a<span class="ot">=</span>(clus[l]<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>clus[l<span class="sc">+</span><span class="dv">1</span>]</span>
<span id="cb456-18"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-18" aria-hidden="true" tabindex="-1"></a>Ind[a,l]<span class="ot">=</span>a</span>
<span id="cb456-19"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb456-20"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-21"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-21" aria-hidden="true" tabindex="-1"></a><span class="co">#Tamaños</span></span>
<span id="cb456-22"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-23"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-23" aria-hidden="true" tabindex="-1"></a>nsim2<span class="ot">=</span><span class="dv">1000</span></span>
<span id="cb456-24"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-24" aria-hidden="true" tabindex="-1"></a>nI<span class="ot">=</span><span class="fu">floor</span>(NI<span class="sc">*</span><span class="fl">0.3</span>)</span>
<span id="cb456-25"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-26"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim2){</span>
<span id="cb456-27"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-27" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">S.piPS</span>(nI,Tamaños)</span>
<span id="cb456-28"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-28" aria-hidden="true" tabindex="-1"></a>sam <span class="ot">&lt;-</span> res[,<span class="dv">1</span>] </span>
<span id="cb456-29"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-29" aria-hidden="true" tabindex="-1"></a>Ind.sam<span class="ot">=</span>Ind[,sam]</span>
<span id="cb456-30"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-30" aria-hidden="true" tabindex="-1"></a>Tamaños.sam<span class="ot">=</span>Tamaños[sam]</span>
<span id="cb456-31"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-31" aria-hidden="true" tabindex="-1"></a><span class="co">#-------Espacio para las medias</span></span>
<span id="cb456-32"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-32" aria-hidden="true" tabindex="-1"></a>medias<span class="ot">=</span><span class="fu">matrix</span>(<span class="cn">NA</span>)</span>
<span id="cb456-33"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(Ind.sam)){</span>
<span id="cb456-34"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-34" aria-hidden="true" tabindex="-1"></a>medias[k]<span class="ot">=</span><span class="fu">mean</span>(y[Ind.sam[,k]])</span>
<span id="cb456-35"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb456-36"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-36" aria-hidden="true" tabindex="-1"></a><span class="co">#-------</span></span>
<span id="cb456-37"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-37" aria-hidden="true" tabindex="-1"></a>Est1[j]<span class="ot">=</span><span class="fu">mean</span>(medias)</span>
<span id="cb456-38"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-38" aria-hidden="true" tabindex="-1"></a>Est2[j]<span class="ot">=</span><span class="fu">sum</span>(Tamaños.sam<span class="sc">*</span>medias)<span class="sc">/</span><span class="fu">sum</span>(Tamaños)</span>
<span id="cb456-39"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-39" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb456-40"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-41"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-41" aria-hidden="true" tabindex="-1"></a>Esp1<span class="ot">=</span><span class="fu">mean</span>(Est1)</span>
<span id="cb456-42"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-42" aria-hidden="true" tabindex="-1"></a>Esp2<span class="ot">=</span><span class="fu">mean</span>(Est2)</span>
<span id="cb456-43"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-44"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb456-44" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(theta_N, Esp1, Esp2)</span></code></pre></div>
<pre><code>##      theta_N      Esp1    Esp2
## [1,]    0.27 0.2691843 0.10901</code></pre>
<p>Nótese que el primer estimador es insesgado (su esperanza equivale al parámetro de la población finita) porque es función del inverso de la probabilidad de inclusión de los elementos que son inducidas por la medida de probabilidad definida por el plan de muestreo. El segundo estimador es sesgado porque no tiene en cuenta el diseño de muestreo.</p>
<p><em>Inferencia doble: los modelos y el muestreo</em></p>
<p>En último lugar, suponga que los valores de las variables de interés sí constituyen realizaciones de variables aleatorias que siguen un modelo probabilístico. Como una población finita está constituida por la realización particular de las variables aleatorias, condicionado a la realización de una población finita, se extrae una muestra aleatoria de elementos, mediante un diseño de muestreo complejo. Nótese que, en este tercer proceso inferencial, tanto el modelo como el diseño de muestreo como la medida de probabilidad que da origen a las superpoblaciones, constituyen dos medidas de probabilidad distintas que deben regir la inferencia del parámetro de interés.</p>
<p>Al respecto, nótese que, dado que el diseño de muestreo es complejo, no es viable utilizar técnicas clásicas, como el método de máxima verosimilitud, puesto que los datos finales no constituyen una muestra aleatoria de variables independientes ni idénticamente distribuidas. Por lo anterior, la forma final de la función de verosimilitud, definida como la densidad conjunta de las variables en la muestra, será muy compleja, intratable e insoluble. Una solución a este problema de estimación es la técnica de máxima pseudo-verosimilitud, la cual induce estimadores que tienen en cuenta las ponderaciones del diseño de muestreo complejo. Para el ejemplo de las proporciones, el estimador <span class="math inline">\(\bar{y}_{\pi S}\)</span> cumple la siguiente relación:</p>
<p><span class="math display">\[
E_{\xi p}(\bar{y}_{\pi S})=E_{\xi}E_{p}(\bar{y}_{\pi S}|Y)=E_{\xi}(\bar{y}_{U})=\theta=0.3
\]</span>
Con la siguiente simulación de Monte Carlo se comprueba fácilmente que <span class="math inline">\(\bar{y}_{\pi S}\)</span> es insesgado, mientras que es <span class="math inline">\(\bar{y}_{S}\)</span> sesgado:</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(TeachingSampling)</span>
<span id="cb458-2"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-3"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-3" aria-hidden="true" tabindex="-1"></a>N<span class="ot">=</span><span class="dv">100</span></span>
<span id="cb458-4"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-4" aria-hidden="true" tabindex="-1"></a>theta<span class="ot">=</span><span class="fl">0.3</span></span>
<span id="cb458-5"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-5" aria-hidden="true" tabindex="-1"></a>nsim1<span class="ot">=</span><span class="dv">100</span></span>
<span id="cb458-6"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-6" aria-hidden="true" tabindex="-1"></a>Esp1<span class="ot">=</span>Esp2<span class="ot">=</span><span class="fu">rep</span>(<span class="cn">NA</span>,nsim1)</span>
<span id="cb458-7"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-8"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim1){</span>
<span id="cb458-9"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-9" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="fu">rbinom</span>(N, <span class="dv">1</span>, theta)</span>
<span id="cb458-10"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-10" aria-hidden="true" tabindex="-1"></a><span class="co">#-----Creación de los clusters---------</span></span>
<span id="cb458-11"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-11" aria-hidden="true" tabindex="-1"></a>clus<span class="ot">=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">which</span>((y[<span class="sc">-</span>N]<span class="sc">-</span>y[<span class="sc">-</span><span class="dv">1</span>])<span class="sc">!=</span><span class="dv">0</span>)<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb458-12"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-12" aria-hidden="true" tabindex="-1"></a>NI<span class="ot">=</span>(<span class="fu">length</span>(clus)<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb458-13"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-13" aria-hidden="true" tabindex="-1"></a>Ind<span class="ot">=</span><span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span>N, <span class="at">ncol=</span>NI)</span>
<span id="cb458-14"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-14" aria-hidden="true" tabindex="-1"></a>Tamaños<span class="ot">=</span>clus[<span class="sc">-</span><span class="dv">1</span>]<span class="sc">-</span>clus[<span class="sc">-</span>(<span class="fu">length</span>(clus))]</span>
<span id="cb458-15"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-16"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(l <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(clus)<span class="sc">-</span><span class="dv">1</span>)){</span>
<span id="cb458-17"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-17" aria-hidden="true" tabindex="-1"></a>a<span class="ot">=</span>(clus[l]<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>clus[l<span class="sc">+</span><span class="dv">1</span>]</span>
<span id="cb458-18"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-18" aria-hidden="true" tabindex="-1"></a>Ind[a,l]<span class="ot">=</span>a</span>
<span id="cb458-19"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb458-20"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-21"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-21" aria-hidden="true" tabindex="-1"></a>Ind</span>
<span id="cb458-22"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-22" aria-hidden="true" tabindex="-1"></a>Tamaños</span>
<span id="cb458-23"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-24"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-24" aria-hidden="true" tabindex="-1"></a>nsim2<span class="ot">=</span><span class="dv">100</span></span>
<span id="cb458-25"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-25" aria-hidden="true" tabindex="-1"></a>nI<span class="ot">=</span><span class="fu">floor</span>(NI<span class="sc">*</span><span class="fl">0.3</span>)</span>
<span id="cb458-26"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-26" aria-hidden="true" tabindex="-1"></a>Est1<span class="ot">=</span>Est2<span class="ot">=</span><span class="fu">rep</span>(<span class="cn">NA</span>,nsim2)</span>
<span id="cb458-27"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-28"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim2){</span>
<span id="cb458-29"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-29" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">S.piPS</span>(nI,Tamaños)</span>
<span id="cb458-30"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-30" aria-hidden="true" tabindex="-1"></a>sam <span class="ot">&lt;-</span> res[,<span class="dv">1</span>] </span>
<span id="cb458-31"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-31" aria-hidden="true" tabindex="-1"></a>sam</span>
<span id="cb458-32"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-32" aria-hidden="true" tabindex="-1"></a>Ind.sam<span class="ot">=</span>Ind[,sam]</span>
<span id="cb458-33"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-33" aria-hidden="true" tabindex="-1"></a>Tamaños.sam<span class="ot">=</span>Tamaños[sam]</span>
<span id="cb458-34"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-34" aria-hidden="true" tabindex="-1"></a><span class="co">#-------Espacio para las medias</span></span>
<span id="cb458-35"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-35" aria-hidden="true" tabindex="-1"></a>medias<span class="ot">=</span><span class="fu">matrix</span>(<span class="dv">0</span>)</span>
<span id="cb458-36"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(Ind.sam)){</span>
<span id="cb458-37"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-37" aria-hidden="true" tabindex="-1"></a>medias[k]<span class="ot">=</span><span class="fu">mean</span>(y[Ind.sam[,k]])</span>
<span id="cb458-38"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-38" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb458-39"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-40"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-40" aria-hidden="true" tabindex="-1"></a>Est1[j]<span class="ot">=</span><span class="fu">mean</span>(medias)</span>
<span id="cb458-41"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-41" aria-hidden="true" tabindex="-1"></a>Est2[j]<span class="ot">=</span><span class="fu">sum</span>(Tamaños.sam<span class="sc">*</span>medias)<span class="sc">/</span><span class="fu">sum</span>(Tamaños)</span>
<span id="cb458-42"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-42" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb458-43"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-44"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-44" aria-hidden="true" tabindex="-1"></a>Esp1[i]<span class="ot">=</span><span class="fu">mean</span>(Est1)</span>
<span id="cb458-45"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-45" aria-hidden="true" tabindex="-1"></a>Esp2[i]<span class="ot">=</span><span class="fu">mean</span>(Est2)</span>
<span id="cb458-46"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-47"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-47" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb458-48"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-49"><a href="acerca-de-las-muestras-aleatorias-y-su-análisis.html#cb458-49" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(theta, <span class="fu">mean</span>(Esp1), <span class="fu">mean</span>(Esp2))</span></code></pre></div>
<pre><code>##      theta                    
## [1,]   0.3 0.3082382 0.1155773</code></pre>
<p>Por supuesto que, dado que el proceso de inferencia es doble, entonces este ejercicio de Monte Carlo debe ser anidado. Es decir, muchas simulaciones dentro de una simulación. Nótese que en primer lugar se debe generar todas las poblaciones finitas y para cada una de ellas se debe generar las posibles muestras.</p>
<p>Los métodos que se explicarán en este capítulo serán la estimación por Máxima Verosimilitud (MV) y Máxima Pseudo Verosimilitud (MPV) para modelos de regresión. El primer método se basa en estimar un parámetro desconocido suponiendo que las variables de interés constituyen una muestra aleatoria de variables independiente e idénticamente distribuidas (IID) para poder hacer inferencia sobre la población de interés. Por otra parte el método de Máxima Pseudo Verosimilitud sigue un razonamiento parecido, pero con la gran diferencia de que la variable de interés se rige por un diseño muestral específico, lo cual induce una probabilidad de inclusión del individuo que debe ser tenida en cuenta al momento de realizar cualquier tipo de inferencia.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modelos-lineales-generalizados-introducción-a-la-pseudo-máxima-verosimilitud.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="método-de-máxima-verosimilitud.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/psirusteam/2021ASDA/09-Generalizados2.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Análisis de encuestas con R.pdf", "Análisis de encuestas con R.epub", "Análisis de encuestas con R.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"tconfig": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
