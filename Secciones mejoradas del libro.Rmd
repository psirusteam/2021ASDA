---
title: "Untitled"
author: "Alser"
date: "2025-09-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 3.2 Algunas librerías de interés

R es un software libre y de código abierto que ha ganado gran popularidad en el procesamiento de encuestas y la investigación social, convirtiéndose en una herramienta de elección para aplicar los desarrollos científicos y metodológicos más recientes en el análisis de datos de encuestas (R Core Team, 2024). Su carácter abierto permite que investigadores de todo el mundo aporten funciones y paquetes propios al Comprehensive R Archive Network (CRAN), poniéndolos a disposición de la comunidad académica y profesional. Entre sus recursos más destacados se encuentra el paquete samplesize4surveys (Gutiérrez, 2020), que facilita el cálculo de tamaños de muestra para individuos y hogares en encuestas repetidas, de panel y rotacionales. Asimismo, los paquetes sampling (Tillé y Matei, 2016) y TeachingSampling (Gutiérrez, 2015) ofrecen soporte para seleccionar muestras probabilísticas a partir de marcos de muestreo bajo diferentes diseños y algoritmos. Para el análisis de datos de encuestas de hogares, el paquete survey (Lumley, 2024) permite especificar el diseño muestral mediante la función svydesign() y obtener estimaciones correctas de errores estándar. El paquete convey (Pessoa et al., 2024) complementa este proceso al facilitar el cálculo de medidas de desigualdad. En el ámbito del modelado de regresiones, svydiags (Valliant, 2024) incluye herramientas de diagnóstico como análisis de residuos, valores de apalancamiento, factores de inflación de varianza y pruebas de colinealidad, mientras que PracTools (Valliant et al., 2025) proporciona utilidades para el cálculo del tamaño de muestra, el diseño de muestreo, la estimación de efectos de diseño y el análisis de componentes de varianza en esquemas multietápicos.

Puesto que R es un lenguaje colaborativo el cual permite que la comunidad vaya haciendo aportes al desarrollo de funciones dentro de paquetes o librerías. Alguna de las librerías más usadas para el análisis de bases de datos son las siguientes:

- `dplyr`, dplyr es la evolución del paquete plyr, enfocada en herramientas para trabajar con marcos de datos (de ahí la d en el nombre). Según Hadley Wickham, las siguientes son las tres propiedades principales de la librería:
    1) Identificar las herramientas de manipulación de datos más importantes necesarias para el análisis de datos y hacerlas fáciles de usar desde R.
    2) Proporcionar un rendimiento ultrarrápido para los datos en memoria escribiendo piezas clave en C++.
    3) Utilizar la misma interfaz para trabajar con datos sin importar dónde estén almacenados, ya sea en un marco de datos, una tabla de datos o una base de datos. Esta librería permite manejar eficientemente las bases de datos.
    
- `tidyverse`, es una colección de paquetes disponibles en R y orientados a la manipulación, importación, exploración y visualización de datos y que se utiliza exhaustivamente en ciencia de datos. El uso de `tidyverse` permite facilitar el trabajo estadístico y la generación de trabajos reproducibles. Está compuesto de los siguientes paquetes: `readr`, `dplyr`, `ggplot2`, `tibble`, `tidyr`, `purr`, `stringr`, `forcats`
    
- `readstata13`, este paquete permite leer y escribir todos los formatos de archivo de Stata (versión 17 y anteriores) en un marco de datos R. Se admiten las versiones de formato de archivo de datos 102 a 119. para leer las bases de datos de `STATA`. Además, el paquete admite muchas características del formato Stata dta, como conjuntos de etiquetas en diferentes idiomas o calendarios comerciales.

- `survey`, este paquete ha sido elaborado por el Profesor Thomas Lumley (Lumley, T. 2011) y nos proporciona funciones en R útiles para analizar datos provenientes de encuestas complejas. Alguno de los parámetros que se pueden estimar usando este paquete son medias, totales, razones, cuantiles, tablas de contingencias, modelos de regresión, modelos loglineales, entre otros.

- `srvyr`, este paquete permite utilizar el operador *pipe operators* en las consultas que se realizan con el paquete `survey`.

- `ggplot2`, es un paquete de visualización de datos para el lenguaje R que implementa lo que se conoce como la *Gramática de los Gráficos*, que no es más que una representación esquemática y en capas de lo que se dibuja en dichos gráficos, como lo pueden ser los marcos y los ejes, el texto de los mismos, los títulos, así como, por supuesto, los datos o la información que se grafica, el tipo de gráfico que se utiliza, los colores, los símbolos y tamaños, entre otros.

- `TeachingSampling`, este paquete permite al usuario extraer muestras probabilísticas y hacer inferencias a partir de una población finita basada en varios diseños de muestreo. Entre los diseño empleados en esta librería están: Muestreo Aleatorio Simple (MAS), Muestreo Bernoullí, Muestreo Sistemático, PiPT, PPT, estre otros.

- `samplesize4surveys`, este paquete permite calcular el tamaño de muestra requerido para la estimación de totales, medias y proporciones bajo diseños de muestreo complejos.

Antes de poder utilizar las diferentes funciones que cada librería tiene, es necesario descargarlas de antemano de la web. El comando `install.packages` permite realizar esta tarea. Note que algunas librerías pueden depender de otras, así que para poder utilizarlas es necesario instalar también las dependencias.

```{r, eval=F}
install.packages("dplyr")
install.packages("tidyverse")
install.packages("readstata13") 
install.packages("survey")
install.packages("srvyr")
install.packages("ggplot2")
install.packages("TeachingSampling")
install.packages("samplesize4surveys")
install.packages("sampling")
install.packages("convey")
install.packages("svydiags")
install.packages("PracTools")
```

Una vez instaladas las librerías hay que informarle al software que vamos a utilizarlas con el comando `library`. *Recuerde que es necesario haber instalado las librerías para poder utilizarlas*. 

```{r, warning=FALSE, echo=TRUE, message=FALSE}
rm(list = ls())

library("dplyr")
library("tidyverse")
library("readstata13") 
library("survey")
library("srvyr")
library("ggplot2")
library("TeachingSampling")
library("samplesize4surveys")
library("sampling")
library("convey")
library("svydiags")
library("PracTools")
```


## 3.4 Lectura de bases de datos y definición del diseño muestral

### Uso de software para generar inferencias válidas

Según Naciones Unidas (2005, sec. 7.8), es fundamental que la estructura de los diseños de muestreo complejos se tenga en cuenta en el proceso de inferencia al estimar estadísticas oficiales basadas en encuestas de hogares. El organismo advierte, a través de un ejemplo empírico, que ignorar este aspecto puede generar estimaciones sesgadas y errores de muestreo subestimados. En este contexto, el análisis riguroso de encuestas de hogares requiere herramientas computacionales especializadas que implementen métodos apropiados para el tratamiento de la no respuesta, el cálculo de errores estándar ajustados al diseño muestral y el análisis multivariado de las variables de interés.

De manera general, herramientas estadísticas como R, Stata, SAS y SPSS cuentan con módulos y bibliotecas que optimizan la estimación de varianzas en muestras complejas, incorporando incluso métodos de replicación para varianzas basadas en el diseño. Mientras que R es un software de acceso libre, los otros programas requieren licencias de pago. Además de permitir el cálculo de estadísticas descriptivas, como medias, totales, proporciones, percentiles y razones, estas plataformas posibilitan el ajuste de modelos de regresión considerando la estructura del diseño de la encuesta. Los programas especializados en análisis de encuestas generan de forma automática el efecto del diseño, lo que apoya a los investigadores en la interpretación de la variabilidad de sus estimaciones.

El uso de estos paquetes y herramientas computacionales implica que el usuario suministre información clave del diseño muestral, como los factores de expansión, la estratificación y los identificadores de conglomerados. A continuación, se presenta un análisis detallado de los procedimientos para la lectura de bases de datos y la correcta especificación del diseño muestral en R.


### Gestión de formatos de archivos y recomendaciones técnicas

Las bases de datos pueden estar disponibles en una variedad de formatos (`.xlsx`, `.dat`, `.csv`, `.sav`, `.txt`, etc.). Sin embargo, por experiencia práctica es recomendable realizar la lectura de cualquiera de estos formatos y proceder inmediatamente a guardarlo en un archivo de extensión **.rds**, la cual es nativa de R. Las extensiones **.rds** permiten almacenar cualquier objeto o información en R como pueden ser marcos de datos, vectores, matrices, listas, entre otros.

Los archivos **.rds** se caracterizan por su flexibilidad a la hora de almacenarlos, sin limitarse a bases de datos únicamente, y por su perfecta compatibilidad con R. Por otro lado, existe otro tipo de archivos propios de R como lo es **.RData**. Sin embargo, existen diferencias importantes entre ellos: mientras que los archivos **.RData** pueden contener múltiples objetos en un solo archivo, los **.rds** se limitan a un solo objeto pero ofrecen mayor eficiencia y control sobre la carga de datos. Es por lo anterior que se recomienda trabajar con archivos **.rds** para el manejo individual de bases de datos.

### Implementación práctica en R

Para ejemplificar las sintaxis que se utilizarán en R, se tomará una base de datos que contiene una muestra de 2,427 registros proveniente de un muestreo complejo. A continuación, se muestra la sintaxis en R para cargar un archivo con extensión **.rds**:

```{r, eval=T}
library(tidyverse)
encuesta <- readRDS("Correciones_libro/Data/encuesta.rds")
head(encuesta)
```

### Definición del diseño muestral

Una vez cargada la muestra de hogares en R, el siguiente paso crítico es definir el diseño muestral del cual proviene dicha muestra. Para esto se utiliza el paquete `srvyr`, el cual surge como un complemento moderno para `survey`. Estas librerías permiten definir objetos tipo **survey.design** a los que se aplican las funciones de estimación y análisis de encuestas cargadas en el paquete `srvyr`, complementados con la programación de tubería (`%>%`) del paquete `tidyverse`.

La correcta especificación del diseño muestral es fundamental para obtener estimaciones insesgadas y errores estándar válidos. A continuación, se ejemplifica la definición en R del diseño de muestreo:

```{r}
options(survey.lonely.psu = "adjust") 
library(srvyr)
diseno <- encuesta %>% 
  as_survey_design(
    strata = Stratum,  
    ids = PSU,        
    weights = wk,      
    nest = T)
```

### Especificación de parámetros del diseño

En el código anterior se observa una secuencia lógica de especificación. Primero, se debe definir la base de datos que contiene la muestra seleccionada. Seguidamente, se especifica el tipo de objeto en R con el cual se trabajará mediante la función `as_survey_design`, que crea un objeto *survey_design*.

Los argumentos dentro de la función `as_survey_design` se definen según las características del diseño muestral:

- **strata**: Define la columna que contiene los identificadores de estratos en la base de datos
- **ids**: Especifica la columna donde se encuentran los identificadores de las Unidades Primarias de Muestreo (UPM) seleccionadas en la primera etapa
- **weights**: Establece la columna que contiene los pesos de muestreo o factores de expansión
- **nest = TRUE**: Indica que las UPM están anidadas dentro de los estratos, lo cual es típico en diseños estratificados multietápicos

La opción `survey.lonely.psu = "adjust"` al inicio del código especifica cómo manejar estratos que contienen una sola UPM, aplicando un ajuste automático para evitar problemas en la estimación de varianzas.

Esta especificación correcta del diseño muestral constituye la base fundamental para todos los análisis posteriores, garantizando que las estimaciones y sus medidas de precisión reflejen adecuadamente la complejidad del diseño de la encuesta.



## 3.7 Medidas descriptivos y reflexiones

Al analizar datos de encuestas de hogares, uno de los productos más habituales son los parámetros descriptivos, cuyo propósito es sintetizar las principales características de la población. Estas estimaciones permiten ofrecer una representación clara y comprensible de la realidad poblacional a partir de la información obtenida en una muestra representativa.

Entre los resultados más comunes de este tipo de análisis se encuentran las frecuencias, proporciones, medias y totales. Las medias proporcionan información sobre el valor promedio de una variable, mientras que los totales reflejan su acumulado en toda la población. Las frecuencias cuentan cuántos hogares o individuos pertenecen a una categoría determinada —por ejemplo, el número de personas en situación de pobreza—, y las proporciones expresan la participación relativa de quienes presentan una característica específica, como el porcentaje de población pobre.

Actualmente, el análisis descriptivo va más allá de los parámetros básicos, incorporando métricas más complejas. Se estiman cuantiles de variables numéricas, como la mediana del ingreso de los hogares, para describir la distribución de los datos con mayor detalle. Además, se aplican indicadores especializados para evaluar fenómenos concretos, como los índices FGT para la medición de la pobreza, los indicadores de desigualdad (Gini, Theil, Atkinson) y los de polarización (Wolfson, DER), entre otros (Jacob, Damico y Pessoa, 2024).

En estadística, según *Tellez Piñerez, C. F., & Lemus Polanía, D. F. (2015)* las medidas descriptivas permiten la presentación y caracterización de un conjunto de datos con el fin de poder describir apropiadamente las diversas características presentes en la información de la muestra. Involucra cualquier labor o actividad para resumir y describir los datos univariados o multivariados sin tratar de hacer inferencia más allá de los mismos. Este tipo de análisis son primordiales en cualquier encuesta de hogares dado que, permiten tener una idea inicial del comportamiento de la población en ciertas variables de estudio. A continuación, se presentan las funciones básicas en `R` para realizar análisis descriptivo.

-   Media: `mean()`
-   Mediana: `median()`
-   Varianza: `var()`
-   Desviación estándar: `sd()`
-   Percentiles: `quantile()`
-   Algunas medidas descriptivas: `summary()`
-   Covarianza: `cov( , )`
-   Correlación: `cor( , )`

Ahora bien, para continuar con lo análisis de las encuestas de hogares es necesario que el lector tenga claro algunos conceptos básicos en el muestreo probabilístico. A continuación, se dan unas definiciones básicas:

-   *¿Qué es una encuesta?*

Según  Groves, R. M., et al (2011) una encuesta es un método sistemático para recopilar información de una muestra de elementos con el propósito de construir descriptores cuantitativos de los parámetros de la población.

-   *¿Qué es una muestra?*

La definición más básica de una muestra es un subconjunto de la población. Esta definición es muy general dado que, no es específico de si la muestra es representativa de una población o no.

-   *¿Qué es una muestra representativa?*

Según *Gutiérrez (2016)* una muestra representativa es un modelo reducido de la población y de aquí se desprende un argumento de validez sobre la muestra. En pocas palabras, se desea que la muestra representativa tenga la cantidad de información suficiente para poder hacer una inferencia adecuada a la población.

-   *¿Está bien sacar conclusiones sobre una muestra?*

Si la muestra es representativa, las conclusiones que se obtienen de la población utilizando las técnicas de muestreo adecuadas, son correctas. Sin embargo, si se toma una muestra no representativa, no es correcto realizar inferencias dado que estas no representan la realidad de la población.


## (3.10) Medias y totales

Al trabajar con encuestas de hogares, el análisis de datos numéricos implica con frecuencia calcular estadísticas descriptivas como medias, totales y razones, ya que estas permiten sintetizar las principales características de la población y sirven de base para la toma de decisiones. Dichas estimaciones pueden calcularse para la población en su conjunto o para subgrupos específicos, de acuerdo con los propósitos de la investigación. Tal como destacan Heeringa, West y Berglund (2017), el cálculo de totales y medias poblacionales, junto con sus varianzas, ha sido esencial para el desarrollo de la teoría del muestreo probabilístico.

La determinación de los totales poblacionales constituye uno de los pilares del análisis de encuestas. Tanto las medias como las proporciones y las razones derivan de los totales. Un total se define como la suma de una variable específica, por ejemplo, el ingreso o el gasto, a nivel de toda la población. Para estimar el ingreso total de todos los hogares de un país, se combinan los datos de la muestra aplicando los pesos muestrales que reflejan el diseño y aseguran representatividad. En el caso de variables numéricas simples, las estimaciones básicas son los totales y las medias, mientras que las razones permiten establecer comparaciones entre dos variables numéricas. Estos cálculos pueden hacerse para toda la población o de manera desagregada por dominios de estudio, dependiendo de las preguntas de investigación.

Una vez establecido el diseño muestral, se realiza el cálculo de los parámetros de interés. En el caso de encuestas con diseños complejos que incorporan estratificación ($h=1,2,...,H$) y submuestreo en las UPM —ubicadas dentro de cada estrato $h$ e identificadas por $i$—, el total poblacional se estima mediante la expresión:

$$
\hat{Y} = \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} \, y_{hik}
$$

Cuando se cuenta con respuesta completa, la estimación de la varianza de $\hat{Y}$ puede obtenerse aplicando el estimador de Ultimate Cluster descrito previamente en la Sección 9.2.

El intervalo de confianza de nivel $1-\alpha$ para el total poblacional $Y$ se calcula mediante la expresión

$$
\hat{Y} \pm t_{1-\alpha/2, df} \times \sqrt{\hat{V}_{UC}(\hat{Y})},
$$

como se muestra en la ecuación (9-15). A medida que los grados de libertad aumentan, la distribución t de Student tiende a la normal, lo que explica que muchas Oficinas Nacionales de Estadística (ONE) utilicen esta aproximación para reportar intervalos de confianza. No obstante, es importante considerar que esta aproximación puede ser menos fiable cuando el tamaño de muestra es reducido, aunque suele ofrecer buenos resultados en encuestas de hogares extensas.

Las medias o promedios poblacionales son igualmente esenciales para describir la tendencia central de una variable. Por ejemplo, el promedio de gasto de los hogares es un indicador representativo del comportamiento económico de la población. Su cálculo consiste en dividir el total de la variable entre el tamaño de la población, de modo que su exactitud depende de que ambos componentes se estimen correctamente. El estimador de la media poblacional se obtiene como la razón de dos totales:

$$
\widehat{\bar{Y}} = \frac{\displaystyle \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} y_{hik}} {\displaystyle \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik}} = \frac{\hat{Y}}{\hat{N}}.
$$

Dado que $\widehat{\bar{Y}}$ es un estimador de naturaleza no lineal, su varianza exacta no puede expresarse en forma cerrada. Por esta razón, es necesario recurrir a técnicas de remuestreo, como el método bootstrap, o a aproximaciones basadas en series de Taylor. En este último caso, se utiliza su ecuación de estimación muestral

$$
\sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} (y_{hik} - \widehat{\bar{Y}}) = 0,
$$

lo que permite aplicar el estimador de varianza descrito en la Sección 9.2, considerando $z_{hik} = y_{hik} - \widehat{\bar{Y}}$. Estas estimaciones se calculan automáticamente en la mayoría de los paquetes estadísticos diseñados para manejar encuestas complejas.

En la práctica, utilizando R y el paquete `survey`, la función `summarise` permite conocer el total de los ingresos en la base de datos y la media de los ingresos sobre los respondientes:

```{r}
data2 %>% summarise(total.ing = sum(ingcorte),
                    media.ing = mean(ingcorte))
```

También se puede calcular medias de manera agrupada. Particularmente, si se desea calcular la media de los ingresos por área se hace de la siguiente manera:

```{r}
data2 %>% group_by(area_ee) %>%
  summarise(n = n(),
            media = mean(ingcorte))
```

Si ahora el análisis de los ingresos se desea hacer por sexo se realiza de la siguiente manera:

```{r}
data2 %>% group_by(sexo) %>%
  summarise(n = n(),
            media = mean(ingcorte))
```

Estos ejemplos ilustran cómo aplicar los conceptos teóricos de estimación de medias y totales en el análisis práctico de encuestas de hogares, considerando la estructura de diseño complejo y los pesos muestrales apropiados para obtener estimaciones representativas de la población.








## (3.11) Medianas y percentiles

Para estudiar la distribución de los datos más allá de la media, los percentiles y cuantiles son herramientas de gran utilidad, pues permiten segmentar la información en partes que muestran cómo se distribuyen los valores en la población. El percentil 10, por ejemplo, indica el punto por debajo del cual se ubica el 10 % de las observaciones, mientras que la mediana (percentil 50) divide la muestra en dos grupos de igual tamaño. Estas métricas no solo describen la tendencia central, sino también la dispersión y variabilidad de los datos, lo que resulta valioso para orientar políticas públicas, como gravar al 10 % de mayores ingresos o diseñar programas de subsidios para el 15 % más vulnerable.

Su cálculo se basa en la función de distribución acumulada (CDF), que representa la proporción de individuos con valores menores o iguales a un umbral dado. Una vez estimada la CDF empleando los datos de la encuesta y sus pesos, se pueden derivar los cuantiles. El estimador de la CDF en un diseño complejo se expresa como:

$$
\hat{F}(t) = \frac{\displaystyle\sum_{h=1}^{H}\sum_{i\in s_{1h}}\sum_{k\in s_{hi}}w_{hik}I(y_{hik}\leq t)} {\displaystyle\sum_{h=1}^{H}\sum_{i\in s_{1h}}\sum_{k\in s_{hi}}w_{hik}} \quad (9-19)
$$

donde $I(y_{hik}\leq t)$ toma valor 1 si $y_{hik}$ es menor o igual a $t$, y 0 en caso contrario. De acuerdo con Heeringa, West y Berglund (2017), para estimar cuantiles se ordenan los datos $y_{(1)},…,y_{(n)}$ y se identifica $j$ tal que

$$
\hat{F}(y_{(j)})\leq q\leq \hat{F}(y_{(j+1)}),
$$

obteniéndose el estimador del cuantil $q$-ésimo como

$$
\hat{y}_{(q)} = y_{(j)} + \frac{q-\hat{F}(y_{(j)})}{\hat{F}(y_{(j+1)})-\hat{F}(y_{(j)})}(y_{(j+1)}-y_{(j)}).
$$

Al ser medidas no lineales, los cuantiles requieren métodos más sofisticados para estimar su varianza. Kovar, Rao y Wu (1988) recomiendan el uso de métodos de replicación para calcular errores estándar en este contexto.

En la práctica, utilizando R y el paquete `survey`, la función `summarise` permite conocer algunas medidas de localización de los ingresos en la base de datos:

```{r}
data2 %>% summarise(mediana = median(ingcorte),
                    decil1 = quantile(ingcorte, 0.1),
                    decil9 = quantile(ingcorte, 0.9),
                    rangodecil = decil9 - decil1)
```

Este código calcula la mediana (percentil 50), el primer decil (percentil 10), el noveno decil (percentil 90) y el rango entre deciles, que proporciona información sobre la dispersión de la distribución de ingresos. Estas medidas son particularmente útiles para analizar desigualdades económicas y diseñar políticas focalizadas en segmentos específicos de la población.











## (4.4 nueva) Enfoques para la estimación de la varianza

### Ecuaciones de estimación y linealización de Taylor

Muchos parámetros poblacionales pueden expresarse como soluciones de ecuaciones de estimación que involucran totales poblacionales. Aunque los detalles técnicos pueden ser complejos, la idea fundamental es que los mismos principios utilizados para estimar totales pueden aplicarse también para la estimación de varianzas. Este marco general hace que el método sea sencillo y versátil, permitiendo una implementación eficiente en software especializado.

Una ecuación de estimación poblacional genérica se expresa como $\sum_{k\in U} z_k(\theta)=0$, donde $z_k(\cdot)$ es una función de estimación evaluada para la unidad $k$ y $\theta$ representa el parámetro poblacional de interés. Estas ecuaciones proporcionan un marco general para definir y calcular diversos parámetros de la población, como totales, medias y razones. La idea central es que los parámetros poblacionales pueden definirse como las soluciones de ecuaciones que involucran a todas las unidades de la población.

**• Para el caso del total poblacional:**
se toma $z_k(\theta)=y_k-\theta/N$. La ecuación de estimación correspondiente es $\sum_{k\in U}(y_k-\theta/N)=0$. Al resolver para $\theta$, se obtiene el total poblacional: $\theta=\sum_{k\in U} y_k = Y$.

**• De forma análoga, para la media poblacional:**
se define $z_k(\theta)=y_k-\theta$. La ecuación resultante es $\sum_{k\in U}(y_k-\theta)=0$, cuya solución es $\theta=\left(\sum_{k\in U} y_k\right)/N = \overline{Y}$.

**• Para las razones de totales poblacionales:**
se utiliza $z_k(\theta)=y_k-\theta x_k$, dando lugar a $\sum_{k\in U}(y_k-\theta x_k)=0$. Al resolver para $\theta$, se obtiene la razón poblacional: $\theta=\dfrac{\sum_{k\in U} y_k}{\sum_{k\in U} x_k} = R$.

La idea de definir los parámetros poblacionales como soluciones de ecuaciones de estimación a nivel de población conduce a un método general para obtener los estimadores muestrales correspondientes. Este enfoque consiste en utilizar ecuaciones de estimación muestrales de la forma
$$\sum_{k\in s} d_k\, z_k(\theta)=0,$$
donde $d_k$ son los pesos de diseño y $z_k(\theta)$ la función de estimación evaluada para cada unidad de la muestra. Bajo un muestreo probabilístico y asumiendo respuesta completa, la suma muestral $\sum_{k\in s} d_k\, z_k(\theta)$ es insesgada respecto a la suma poblacional $\sum_{k\in U} z_k(\theta)$ presente en la ecuación de estimación poblacional correspondiente. Al resolver la ecuación de estimación basada en la muestra se obtienen estimadores consistentes de los parámetros poblacionales de interés.

La linealización de Taylor es un método ampliamente utilizado para aproximar la varianza de estimadores no lineales. Consiste en aplicar una expansión de Taylor de primer orden alrededor del parámetro estimado, con el fin de aproximar el estimador no lineal por una expresión lineal. Esta transformación permite tratar el problema mediante técnicas lineales, lo que facilita el cálculo de varianzas en casos donde no existen fórmulas directas o su obtención resulta complicada.

Un estimador consistente de la varianza de estimadores no lineales, obtenidos como soluciones de ecuaciones de estimación muestrales y derivado mediante linealización de Taylor, se expresa como
$$\hat{V}_{TL}(\hat{\theta}) = [\hat{J}(\hat{\theta})]^{-1} \, \hat{V}_p \Big[\sum_{k\in s} d_k\, z_k(\hat{\theta})\Big] \, [\hat{J}(\hat{\theta})]^{-1} \tag{9-6}$$
donde $\hat{J}(\hat{\theta}) = \sum_{k\in s} d_k \left[ \frac{\partial z_k(\theta)}{\partial \theta} \right]_{\theta=\hat{\theta}}$. Este enfoque permite estimar numerosos parámetros poblacionales y sus varianzas utilizando métodos bien conocidos para la estimación de totales, y su simplicidad y generalidad han facilitado su integración en software especializado.

### Método del Ultimate Cluster

El método del Ultimate Cluster es un enfoque directo y robusto para estimar la varianza de totales en encuestas que utilizan diseños de muestreo por conglomerados estratificados en múltiples etapas. Propuesto por Hansen, Hurwitz y Madow (1953), este método simplifica la complejidad de los diseños multinivel al centrarse únicamente en la variación entre las Unidades Primarias de Muestreo (PSU). Se asume que, dentro de cada estrato de muestreo, las PSU fueron seleccionadas de manera independiente con reemplazo (posiblemente con probabilidades desiguales), incluso si en la práctica se seleccionaron sin reemplazo.

El método considera la variación entre las estadísticas calculadas a nivel de PSU. Cuando se aplica correctamente, refleja implícitamente cualquier submuestreo realizado dentro de las PSU, permitiendo estimaciones de varianza más simples pero confiables. Este enfoque es especialmente útil, ya que puede adaptarse a una amplia gama de diseños complejos, incluyendo estratificación y selección con probabilidades desiguales (con o sin reemplazo) tanto de las PSU como de las unidades de muestreo de niveles inferiores (hogares e individuos). 

**Requisitos para aplicar el método Ultimate Cluster:**

- **Disponibilidad de estimaciones insesgadas de totales** para la(s) variable(s) de interés en cada PSU muestreada.
- **Datos disponibles para al menos dos PSU muestreadas** dentro de cada estrato (si la muestra se estratifica en la primera etapa).
- **El conjunto de datos de la encuesta debe contener información completa** sobre PSU, estratos y pesos.

Considere un diseño de muestreo en múltiples etapas, en el cual se seleccionan $n_h$ PSU en el estrato $h$, para $h=1,\dots,H$. Sea $\hat{Y}_{hi}=\sum_{k\in s_{hi}} d_{hik} y_{hik}$ una estimación del total poblacional $Y_{hi}$ de la PSU $i$ en el estrato $h$. Un estimador insesgado del total poblacional $Y=\sum_{h=1}^H \sum_{i \in U_{1h}} Y_{hi}$ se expresa como $\hat{Y}_{UC} = \sum_{h=1}^H \hat{Y}_h$, donde $\hat{Y}_h = \frac{1}{n_h} \sum_{i\in s_{1h}} \hat{Y}_{hi}$. Nótese que $U_{1h}$ y $s_{1h}$ representan los conjuntos de población y muestra de PSU en el estrato $h$, respectivamente. El estimador Ultimate Cluster de la varianza correspondiente se calcula como:
$$\hat{V}_{UC}(\hat{Y}) = \sum_{h=1}^H \frac{n_h}{n_h-1} \sum_{i \in s_{1h}} (\hat{Y}_{hi} - \hat{Y}_h)^2 \tag{9-7}$$
Para más detalles, véase Hansen, Hurwitz y Madow (1953, vol. I, p. 257) o Wolter (2007).

Si bien el método se diseñó en un inicio para calcular las varianzas de los estimadores de totales, también puede emplearse junto con técnicas como la linealización de Taylor y el enfoque de ecuaciones de estimación para derivar varianzas de estimadores de otras cantidades poblacionales que puedan formularse como la solución de una ecuación de estimación. Gracias a esta característica, el método resulta flexible y aplicable en múltiples contextos de análisis de encuestas de hogares.

Por otra parte, un supuesto fundamental de este método establece que, dentro de cada estrato, las unidades primarias de muestreo (PSU) se eligen de forma independiente y con reemplazo. No obstante, en la práctica, la mayoría de las encuestas de hogares realizan la selección de PSU sin reemplazo, lo que suele generar diseños más eficientes. Por ello, las estimaciones de varianza que parten de la premisa de independencia constituyen aproximaciones de las verdaderas varianzas de muestreo. Cuando la fracción de muestreo es reducida (por ejemplo, menor al 5 %), dichas aproximaciones suelen ser apropiadas y con un nivel de precisión suficiente para su uso por parte de las oficinas nacionales de estadística o de analistas secundarios.

El método Ultimate Cluster destaca por su simplicidad, lo que lo hace muy atractivo. En la práctica, quienes realizan encuestas suelen preferirlo frente a métodos más sofisticados que consideran cada etapa del diseño de muestreo. Si bien estos procedimientos más detallados pueden ofrecer estimaciones de varianza algo más precisas, su aplicación es más compleja y demanda información más completa sobre el diseño muestral. En cambio, el método proporciona una aproximación razonable que suele ser suficiente en la mayoría de los casos prácticos, especialmente al estimar totales o medias. Un análisis sobre la precisión de esta aproximación y las posibles alternativas se presenta en Särndal, Swensson y Wretman (1992, p. 153).

### Métodos de replicación: Bootstrap

Para proteger la confidencialidad, los microdatos de encuestas disponibles públicamente suelen excluir información esencial del diseño (por ejemplo, identificadores de estratos o de unidades primarias de muestreo), lo que limita la capacidad de los usuarios para obtener estimaciones de varianza válidas. En estos casos, es recomendable que las oficinas nacionales de estadística (NSO) proporcionen pesos de replicación, lo que permite a los analistas calcular errores estándar apropiados. Sin variables de diseño ni pesos de replicación, los usuarios secundarios no pueden reproducir los errores estándar publicados ni considerar de manera adecuada el diseño complejo de la encuesta en la estimación de varianzas.

Los métodos de replicación para estimar la varianza se fundamentan en generar subconjuntos de la muestra original, obtener estimaciones para cada uno de ellos y emplear la variabilidad observada entre dichas estimaciones para aproximar la varianza del estimador principal. Estos métodos resultan particularmente valiosos cuando la base de datos carece de información sobre estratos o identificadores de UPM, situación en la que no es posible aplicar el método de Última Conglomeración.

El método bootstrap constituye una herramienta de replicación robusta y versátil para calcular varianzas tanto en encuestas como en otros ámbitos de análisis. Aunque fue introducido por Efron (1979) para datos que no provenían de encuestas, la adaptación más utilizada en encuestas de hogares es el Bootstrap de Reescalamiento de Rao-Wu-Yue (Rao, Wu y Yue, 1992), el cual se ajusta de manera óptima a diseños de muestreo estratificados y multietápicos, siendo una de las técnicas más empleadas para la estimación de varianzas en encuestas complejas.

**Procedimiento del método Bootstrap:**

1. **Generación de réplicas:** Para cada estrato, se seleccionan aleatoriamente las UPM de la muestra original con reemplazo, de modo que algunas pueden repetirse y otras no aparecer. Cada UPM elegida se incorpora junto con todas sus observaciones. Cuando el tamaño de la muestra de primera etapa en el estrato $h$ es mayor que dos ($n_h > 2$), el número de UPM seleccionadas en cada réplica es $n_h-1$, una menos que en la muestra inicial.

2. **Repetición del proceso:** Este procedimiento se repite muchas veces, habitualmente cientos, para generar un gran número de réplicas. La cantidad de veces que una UPM $i$ del estrato $h$ aparece en la réplica $r$ se representa como $n_{hi}^{(r)}$, y puede variar entre 0 y $n_h-1$.

3. **Cálculo de pesos bootstrap:** A partir de cada réplica se calculan pesos bootstrap nuevos para todas las unidades, que reflejan cuántas veces fue seleccionada su UPM en esa réplica. El peso de la unidad $k$ en la réplica $r$ se obtiene mediante:
$$w_{hik}^{(r)} = w_{hik} \times \frac{n_h}{n_h-1} \times n_{hi}^{(r)}$$

4. **Aplicación de ajustes:** Si los pesos originales ya incluyen ajustes por no respuesta o calibración, esos mismos ajustes deben aplicarse a cada conjunto de pesos bootstrap, para capturar la incertidumbre adicional que dichos ajustes generan.

Cuando la Oficina Nacional de Estadística no incluye en la base de datos los identificadores de estratos o UPM, pero sí entrega los pesos de replicación Bootstrap, los investigadores pueden seguir calculando errores estándar de manera adecuada. En cada réplica $r$, el parámetro de interés $\theta$ se estima como $\hat{\theta}^{(r)}$, empleando los pesos bootstrap $w_{hik}^{(r)}$ en lugar de los pesos muestrales originales $w_{hik}$. Posteriormente, la variabilidad entre todas las estimaciones obtenidas a partir de las réplicas se utiliza para aproximar la varianza del estimador original.

El estimador de la varianza mediante bootstrap se expresa como:
$$\hat{V}_B(\hat{\theta}) = \frac{1}{R} \sum_{r=1}^{R} \left(\hat{\theta}^{(r)} - \tilde{\theta}\right)^2$$
donde
$$\tilde{\theta} = \frac{1}{R} \sum_{r=1}^{R} \hat{\theta}^{(r)}$$
corresponde al promedio de todas las estimaciones replicadas, lo que asegura que la medida de variabilidad capture fielmente la incertidumbre del parámetro.

El método Bootstrap ofrece múltiples beneficios. Aunque exige un procesamiento computacional considerable, ya que implica generar y analizar un gran número de réplicas, es muy eficaz para el análisis de diseños de encuesta complejos y permite estimar una amplia gama de parámetros, incluso aquellos de difícil cálculo con métodos tradicionales, como las medianas u otras estadísticas no lineales. Además, representa una solución práctica para la estimación de varianzas cuando otros métodos no están disponibles o resultan poco viables. Este enfoque resulta particularmente útil para los analistas que trabajan con bases de datos de encuestas que no incluyen identificadores de estratos y de UPM, pero que sí disponen de un conjunto de pesos de replicación.

La sencillez del método Bootstrap facilita que los investigadores puedan estimar varianzas aun sin utilizar programas estadísticos especializados. Sin embargo, en la actualidad la mayoría de los paquetes estadísticos incluyen procedimientos para aplicar la replicación Bootstrap y calcular varianzas, lo que amplía su disponibilidad y permite obtener estimaciones robustas incluso para parámetros de alta complejidad. Esto lo convierte en una de las herramientas más versátiles para el análisis de encuestas. Pese a ello, su aplicación no es la más recomendable en encuestas repetidas con muestras que se superponen ni en situaciones donde las fracciones de muestreo son grandes y los tamaños de muestra son reducidos (Bruch, 2011).





### Efecto del diseño (DEFF)


De acuerdo con Kish (1965, p. 258), el efecto del diseño (DEFF) se define como la relación entre la varianza de un estimador calculado bajo un diseño de muestreo complejo y la varianza del mismo estimador cuando se emplea un muestreo aleatorio simple (MAS) con igual tamaño de muestra. Su estimación se formula como:
$$\widehat{\text{DEFF}} = \frac{\widehat{V}_{p}(\hat{\theta})}{\widehat{V}_{\text{SRS}}(\hat{\theta})}$$
donde $\widehat{V}_{p}(\hat{\theta})$ corresponde a la varianza estimada de $\hat{\theta}$ bajo el diseño complejo $p(s)$, mientras que $\widehat{V}_{\text{SRS}}(\hat{\theta})$ representa la varianza estimada del mismo estimador bajo un MAS de igual tamaño. Tal como se discutió en el Capítulo 5, este indicador permite cuantificar cuánto aumenta la varianza como consecuencia de la conglomeración y otras características propias de los diseños complejos en comparación con un muestreo simple. Según Naciones Unidas (2008, p. 49), el DEFF puede entenderse de tres maneras: como el factor de incremento de la varianza frente al MAS, como una medida de la pérdida relativa de precisión o como una indicación del aumento en el tamaño de la muestra que sería necesario en un diseño complejo para alcanzar el mismo nivel de varianza que en un MAS.


Según Park et al. (2003), el efecto del diseño de una encuesta puede desglosarse en tres factores multiplicativos.

• El primero está relacionado con la ponderación desigual, ya que la presencia de pesos muestrales no uniformes suele incrementar ligeramente la varianza; por ello, el uso de pesos iguales resulta ventajoso y explica por qué los diseños auto-ponderados son preferidos en encuestas de hogares. 

• El segundo factor corresponde a la estratificación, la cual, cuando se aplica correctamente, puede disminuir la varianza, aunque en la práctica su efecto reductor suele ser moderado. 

• Finalmente, el tercer factor se asocia con el muestreo en varias etapas, que generalmente incrementa la varianza, dado que las unidades que pertenecen a un mismo conglomerado tienden a ser más homogéneas entre sí que en comparación con las de otros conglomerados.


Al analizar encuestas, el DEFF se convierte en un indicador fundamental para medir la calidad de las estimaciones y orientar el diseño de estudios futuros. Cuando su valor es elevado, evidencia que el diseño complejo introduce ineficiencias que incrementan la varianza y reducen la precisión de los resultados. Por el contrario, un valor cercano a uno revela que el diseño tiene un efecto mínimo sobre la varianza. Esta información permite a los investigadores identificar si es necesario ajustar la ponderación, optimizar la estratificación o modificar el tamaño del submuestreo para aumentar la eficiencia en levantamientos posteriores.


La interpretación de un DEFF alto debe hacerse con precaución, pues no siempre implica que el diseño muestral sea inadecuado. Es fundamental considerar el contexto de la encuesta; por ejemplo, un valor superior a tres podría parecer alarmante, pero en muchos casos se debe a limitaciones prácticas, como restricciones presupuestales, dificultades logísticas o la necesidad de garantizar la participación de los entrevistados. En ciertos levantamientos de hogares, puede ser indispensable seleccionar solo una fracción de los individuos elegibles en cada hogar en lugar de incluirlos a todos. Asimismo, situaciones propias del trabajo de campo, como problemas de cobertura o tasas de no respuesta, pueden aumentar la variabilidad de los pesos muestrales (véase el Capítulo 8) y, en consecuencia, elevar los valores del DEFF.




## (4.4) Estimación del coeficiente de Gini en encuestas de hogares

El estudio de la desigualdad económica es prioritario para gobiernos y organismos internacionales. Entre las métricas más utilizadas destaca el coeficiente de Gini, que compara la distribución de ingresos observada con una distribución perfectamente equitativa. En encuestas de hogares, su cálculo incorpora pesos muestrales ajustados al diseño de la encuesta, y con frecuencia se normalizan para simplificar el procesamiento. Sus valores oscilan entre 0 (igualdad perfecta) y 1 (máxima desigualdad). Este indicador es clave para evaluar cambios en la distribución del ingreso a lo largo del tiempo y para comparar niveles de desigualdad entre regiones o países.

Para iniciar esta sección tengamos en cuenta la siguiente reflexión: *Definir lo justo siempre será difícil y es algo a lo que quizá sea poco realista aspirar a conseguir. Sin embargo si estamos un poco más conscientes de cómo la desigualdad afecta nuestra libertad y cómo se refleja en el bienestar y calidad de vida de las personas, podremos poner en contexto una discusión que tendremos cada vez más presente en el mundo y en el país.*

La desigualdad en todos los aspectos es un problema más común en todos los países del mundo. Particularmente, la desigualdad económica es un problema que atañe a muchas instituciones internacionales como, por ejemplo, Naciones Unidas quien tiene este problema detectado en los Objetivos de Desarrollo Sostenibles (ODS, por sus siglas). Dado lo anterior, es clave poder medir la desigualdad económica de los hogares en los países y para esto, el indicador más utilizado es el coeficiente de Gini (CG). El valor del índice de Gini se encuentra entre 0 y 1. Un valor del coeficiente de Gini de $G = 0$ indica perfecta igualdad en la distribución de la riqueza, con valores más grandes significa una desigualdad cada vez mayor en la distribución de la riqueza.

De acuerdo con Binder y Kovacevic (1995), el estimador del coeficiente de Gini se define como:

$$
\hat{G} = \frac{2\sum_{h=1}^{H}\sum_{i\in s_{1h}}\sum_{k\in s_{hi}}w_{hik}^{*}\hat{F}_{hik}y_{hik}-1}{\widehat{\bar{Y}}} \quad (9-22)
$$

donde $w_{hik}^{*}$ es el peso normalizado:

$$
w_{hik}^{*}=\frac{w_{hik}} {\displaystyle\sum_{h=1}^{H}\sum_{i\in s_{1h}}\sum_{k\in s_{hi}}w_{hik}} \quad (9-23)
$$

y $\hat{F}_{hik}$ representa la CDF estimada para el individuo $k$ en la UPM $i$ del estrato $h$. Osier (2009) y Langel y Tillé (2013) ofrecen detalles técnicos relevantes para estimar la varianza de este indicador complejo.

Siguiendo la ecuación de estimación de *Binder y Kovacevic (1995)*, un estimador del coeficiente de Gini es:

\begin{eqnarray*}
\hat{G}\left(y\right) & = & \frac{2\times\sum_{h=1}^{H}\sum_{\alpha=1}^{a_{h}}\sum_{i=1}^{n_{h\alpha}}\omega_{h\alpha i}^{*}\hat{F}_{h\alpha i}y_{h\alpha i}-1}{\bar{y}_{\omega}}
\end{eqnarray*}

Donde:

- $\omega_{h\alpha i}^{*}=\frac{\omega_{h\alpha i}}{\sum_{h=1}^{H}\sum_{\alpha=1}^{a_{h}}\sum_{i=1}^{n_{h\alpha}}\omega_{h\alpha i}}$
- $\hat{F}_{h\alpha i}=$ La estimación de la CDF en el conglomerado $\alpha$ en el estrato $h$
- $\bar{y}_{\omega}=$ La estimación del promedio

Para calcular el índice de Gini y su varianza estimada en una encuesta de hogares, `R` tiene cargados los procedimientos en la librería `convey`. A continuación, se muestra la sintaxis de cómo se realiza la estimación del índice de Gini para los hogares en la base de ejemplo de este capítulo.

```{r, eval=T}
library(convey)
diseno_gini <- convey_prep(diseno)
svygini( ~Income, design = diseno_gini) %>%
  data.frame()
```

En primer lugar, se carga el diseño de muestreo con la función `convey_prep`. Luego, se estima el índice Gini con la función `svygini`. En los argumentos de esta última función se introducen la variable ingresos y el diseño muestral complejo.

Por otro lado, si el interés ahora es estimar la **curva de Lorenz**. La cual, según *Kovacevic, M. S. et. al (1997)* para una distribución dada de ingresos, traza el porcentaje acumulado de la población (desplegado desde el más pobre hasta el más rico) frente a su participación en el ingreso total. El área entre la curva de Lorenz y la línea de 45 grados se conoce como el área de Lorenz. El índice de Gini es igual al doble del área de Lorenz. Una población con la curva de Lorenz más cerca de la línea de 45 grados tiene una distribución de ingresos más equitativa. Si todos los ingresos son iguales, la curva de Lorenz degenera a la línea de 45 grados.

Para realizar la curva de Lorenz en `R` se utiliza la función `svylorenz`. A continuación, se muestran los códigos computacionales para realizar la curva de Lorenz para los ingresos:

```{r, out.width="70%", fig.align="center"}
library(convey)
svylorenz(formula = ~Income, 
          design = diseno_gini, 
          quantiles = seq(0,1,.05), 
          alpha = .01 )
```

Los argumentos que requiere la función son, inicialmente, los ingresos de los hogares y el diseño muestral complejo. Adicionalmente, se definen una secuencia de probabilidades que define la suma de los cuantiles a calcular (quantiles) y por último, un número que especifica el nivel de confianza para el gráfico (alpha).











## (4.5) Análisis de la relación entre dos variables continuas

Para explorar cómo se relacionan dos variables numéricas en datos de encuestas, el análisis de correlación es una de las técnicas más empleadas. Así, por ejemplo, puede resultar de interés examinar si el ingreso de los hogares está asociado con su nivel de gasto y en qué medida. El coeficiente de correlación de Pearson, que varía entre –1 y 1, permite cuantificar tanto la fuerza como la dirección de esta relación lineal. Un valor positivo indica que ambas variables tienden a aumentar al mismo tiempo, un valor negativo señala que cuando una se incrementa la otra tiende a disminuir y valores próximos a cero sugieren una relación lineal débil o inexistente.

Para que la correlación obtenida sea representativa de la población y no solo de la muestra, en el análisis de encuestas se incorporan los pesos muestrales. Este procedimiento ajusta la estimación a los efectos del diseño complejo, contemplando la estratificación, la conglomeración y las probabilidades de selección desiguales. El cálculo del coeficiente implica evaluar la covarianza entre las dos variables y normalizarla por sus varianzas individuales, lo que elimina la influencia de las unidades de medida y facilita la interpretación.

Formalmente, el coeficiente de correlación de Pearson para dos variables numéricas de encuesta $x$ y $y$ se calcula mediante:

$$
\hat{\rho}_{xy} = \frac{\displaystyle \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} (y_{hik} - \widehat{\bar{Y}})(x_{hik} - \widehat{\bar{X}})} {\sqrt{\displaystyle \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} (y_{hik} - \widehat{\bar{Y}})^2} \sqrt{\displaystyle \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} (x_{hik} - \widehat{\bar{X}})^2}}
$$

Cuando las variables son categóricas u ordinales, se utilizan otras medidas de asociación. De igual manera, es posible ajustar modelos estadísticos para estudiar la relación entre una variable de respuesta y sus covariables.

En muchos análisis de variables relacionadas con encuestas de hogares no solo basta con analizar el comportamiento de variables de manera individual, por ejemplo, ingresos medios de hombres y mujeres en un país, sino también analizar la diferencia entre los ingresos de los hombres y las mujeres. Esto último con el fin de ir cerrando la brecha salarial que existe.

En este capítulo se estudiará la prueba de hipótesis para diferencia de medias, se darán las herramientas computacionales para estimar razones y contrastes. El análisis de la relación entre variables continuas en el contexto de encuestas complejas requiere considerar la estructura de diseño muestral para obtener estimaciones válidas y representativas de la población bajo estudio.







## (4.7) Estimando razones en encuestas de hogares

Las razones permiten expresar la relación entre dos variables. Un ejemplo común es la razón entre el gasto y el ingreso de los hogares, que ayuda a identificar patrones de consumo. La exactitud de este indicador depende de la correcta estimación de los totales que conforman el numerador y el denominador. Este tipo de medida resulta particularmente valiosa para construir indicadores que faciliten la comparación entre distintos grupos poblacionales o el monitoreo de cambios a lo largo del tiempo. Así, por ejemplo, el indicador 2.1.1 de los Objetivos de Desarrollo Sostenible (prevalencia de subalimentación) puede calcularse a partir de la razón entre el consumo de alimentos, medido en calorías o energía ingerida, y los requerimientos energéticos de la dieta, determinados según edad, sexo y nivel de actividad física.

Un caso particular de una función no lineal de totales es la razón poblacional. Esta se define como el cociente de dos totales poblacionales de características de interés. En las encuestas de hogares, en ocasiones se requiere estimar este parámetro, por ejemplo, cantidad de hombres por cada mujer o la cantidad de mascotas por cada hogar en un país determinado. Puesto que la razón es un cociente de totales, tanto el numerador como el denominador son cantidades desconocidas y por tanto requieren estimarse *(Bautista, 1998)*. Por definición la razón poblacional se define de la siguiente manera:

\begin{eqnarray*}
R & = & \frac{Y}{X}
\end{eqnarray*}

El estimador puntual de la razón se obtiene dividiendo el estimador del total de la variable de interés entre el estimador del total de la variable de referencia:

$$
\hat{R} = \frac{\hat{Y}}{\hat{X}} = \frac{\displaystyle \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} y_{hik}} {\displaystyle \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} x_{hik}}
$$

El estimador puntual de una razón en muestreos complejos no es más que estimar los totales por separado como se define a continuación:

\begin{eqnarray*}
\hat{R} & = & \frac{\hat{Y}}{\hat{X}}\\
 & = & \frac{{ \sum_{h=1}^{H}\sum_{\alpha=1}^{\alpha_{h}}\sum_{i=1}^{nh\alpha}}\omega_{h\alpha i}y_{h\alpha i}}{{ \sum_{h=1}^{H}\sum_{\alpha=1}^{\alpha_{h}}\sum_{i=1}^{nh\alpha}}\omega_{h\alpha i}x_{h\alpha i}}
\end{eqnarray*}

Sin embargo, dado que el estimador de la razón es un cociente entre dos estimadores, es decir, dos variables aleatorias, el cálculo de la estimación de la varianza no es sencillo de obtener. Para calcular la varianza de esta razón, se debe definir la función de estimación como $z_{hik} = y_{hik} - \hat{R}x_{hik}$ —donde $y$ y $x$ son las variables del numerador y denominador— y emplear el estimador de varianza. Para ello, se debe aplicar linealización de Taylor como lo muestra *Gutiérrez (2016)*.

De manera computacional, la función `survey_ratio` tiene implementado los procedimientos para estimar las razones y sus varianzas. Para un correcto cálculo de la estimación de la razón y su varianza estimada se le debe introducir a la función el numerador de la razón (numerator) y el denominador (denominator). Adicional a esto, se le debe indicar el nivel de confianza de los intervalos y qué estadística de resúmenes debe calcular (vartype). A continuación, se muestran los códigos computacionales para estimar la razón entre el gasto y el ingreso.

```{r, eval=TRUE}
diseno %>% summarise(
    Razon =  survey_ratio(
      numerator = Expenditure,
      denominator = Income,
      level = 0.95,
    vartype =  c("se", "ci")
    ))
```

Como se puede observar, la razón entre el gasto y el ingreso es, aproximando, 0.71. Lo que implica que por cada 100 unidades monetarias que le ingrese al hogar, se gastan 71 unidades, consiguiendo un intervalo de confianza al 95% de 0.65 y 0.76.

Si ahora el objetivo es estimar la razón entre mujeres y hombres en la base de ejemplo, se realiza de la siguiente manera:

```{r}
diseno %>% summarise(
    Razon =  survey_ratio(
      numerator = (Sex == "Female"),
      denominator = (Sex == "Male"),
      level = 0.95,
    vartype =  c("se", "ci")
    ))
```

Como la variable sexo en la base de datos es una variable categórica, se tuvo la necesidad de generar las variables dummys para su cálculo realizando, Sex == "Female" para el caso de las mujeres y Sex == "Male" para el caso de los hombres. Los resultados del ejercicio anterior muestran que en la base de datos hay más mujeres que hombres, generando una razón de 1.13. Esto significa que, por cada 100 hombres hay aproximadamente 113 mujeres con un intervalo que varía entre 1.04 y 1.21.

Si se desea hacer la razón de mujeres y hombres pero en la zona rural, se haría de la siguiente manera:

```{r}
sub_Rural %>% summarise(
    Razon =  survey_ratio(
      numerator = (Sex == "Female"),
      denominator = (Sex == "Male"),
      level = 0.95,
    vartype =  c("se", "ci")
    ))
```

Obteniendo nuevamente que hay más mujeres que hombres. Ahora bien, otro análisis de interés es estimar la razón de gastos pero solo en la población femenina. A continuación, se presentan los códigos computacionales.

```{r}
sub_Mujer %>% summarise(
    Razon =  survey_ratio(
      numerator = Expenditure,
      denominator = Income,
      level = 0.95,
    vartype =  c("se", "ci")
    ))
```

Dando como resultado que por cada 100 unidades monetarias que le ingresan a las mujeres se gastan 70 con un intervalo de confianza entre 0.65 y 0.76. Por último, análogamente para los hombres, la razón de gastos resulta muy similar que para las mujeres.

```{r}
sub_Hombre %>% summarise(
    Razon =  survey_ratio(
      numerator = Expenditure,
      denominator = Income,
      level = 0.95,
    vartype =  c("se", "ci")
    ))
```



















## (5.1) Estimaciones de totales

En el análisis de encuestas de hogares, resulta esencial determinar el tamaño de las subpoblaciones, es decir, identificar cuántas personas u hogares pertenecen a categorías específicas y qué proporción representan dentro del total poblacional. Este tipo de estimaciones permite caracterizar el perfil demográfico y socioeconómico de la población, información que es clave para orientar la asignación de recursos, el diseño de políticas públicas y la formulación de programas sociales.

Así, es de gran utilidad conocer cuántas personas se encuentran por debajo de la línea de pobreza, cuántas no tienen empleo o cuántas han alcanzado determinado nivel educativo. Estos datos permiten atender las desigualdades, diseñar intervenciones más focalizadas y promover un desarrollo más equitativo. Analizar cómo se distribuyen los individuos entre las distintas categorías ofrece información indispensable para reducir brechas y avanzar en la construcción de un desarrollo inclusivo.

La estimación del tamaño de una población o subpoblación se realiza a partir de variables categóricas, las cuales segmentan a la población en grupos mutuamente excluyentes. Estas categorías pueden corresponder, por ejemplo, a quintiles de ingreso, estados de ocupación o niveles educativos alcanzados. El tamaño poblacional hace referencia al número total de individuos u hogares que, en la base de datos de la encuesta, pertenecen a una categoría determinada. Para obtener estas estimaciones, se combinan las respuestas de los encuestados con los pesos muestrales, que indican cuántas personas u hogares representa cada unidad de la muestra dentro de la población total.

El estimador del tamaño de la población se define como:
$$
\hat{N} = \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik}
$$
donde $s_{hi}$ corresponde a la muestra de hogares o individuos en la UPM $i$ del estrato $h$; $s_{1h}$ representa la muestra de UPM seleccionadas en el estrato $h$; y $w_{hik}$ es el peso o factor de expansión de la unidad $k$ en la UPM $i$ del estrato $h$.

La estimación del tamaño de una subpoblación sigue el mismo principio que el cálculo del tamaño poblacional total, pero se enfoca en un subconjunto definido por una característica específica. Así, para determinar cuántas personas pertenecen a una categoría en particular, se identifica dicho grupo en la base de datos de la encuesta y se suman sus pesos muestrales. Este procedimiento permite no solo conocer el tamaño total de la población, sino también cuantificar grupos de interés específicos.

Para llevarlo a cabo, se construye una variable binaria $I(y_{hik}=d)$, que toma el valor de uno si la unidad $k$ de la UPM $i$ en el estrato $h$ pertenece a la categoría $d$ —la cual no fue considerada en el ajuste de los pesos— de la variable discreta $y$, y cero en caso contrario. El estimador muestral de este parámetro se expresa como:
$$
\hat{N}_d = \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} I(y_{hik}=d)
$$
En el caso de que $d$ sea una categoría incluida en el proceso de calibración de los pesos, el valor de $\hat{N}_d$ coincidirá con el control externo utilizado y no debe interpretarse como una estimación.

En esta sección se realizarán los procesos de estimación de variables categóricas. En primera instancia se presenta cómo se estima los tamaños de la población y subpoblaciones.

```{r, eval=TRUE}
tamano_zona <- diseno %>% group_by(Zone) %>% 
               summarise( n = unweighted(n()), 
                          Nd = survey_total(vartype = c("se","ci")))

tamano_zona
```

En la tabla anterior, *n* denota el número de observaciones en la muestra por Zona y *Nd* denota la estimación del total de observaciones en la población. Adicionalmente, en el código anterior se introdujo la función `unweighted` la cual, calcula resúmenes no ponderados a partir de un conjunto de datos de encuestas.

Para el ejemplo, el tamaño de muestra en la zona rural fue de 1297 personas y para la urbana fue de 1308. Con esta información se logró estimar una población de 72102 con una desviación estándar de 3062.204 en la zona rural y una población de 78164 con desviación estándar de 2847.221 en la zona urbana. Así mismo, con una confianza del 95% se construyeron unos intervalos de confianza para el tamaño poblacional en la zona rural de  (66038.5,	78165.4) y para la urbana de (72526.2,	83801.7).

Ahora bien, empleando una sintaxis similar a la anterior es posible estimar el número de personas en condición de pobreza extrema, pobreza y no pobres como sigue:

```{r, eval=TRUE}
tamano_pobreza <- diseno %>% group_by(Poverty) %>% 
                  summarise( Nd = survey_total(vartype = c("se","ci")) )
tamano_pobreza
```

De la tabla anterior podemos concluir que, la cantidad estimada de personas en estado de no pobreza son 91398.3, en pobreza 37348.9 y pobreza extrema de 21518.7. Los demás parámetros estimados se interpretan de la misma manera que para la estimación desagregada por zona. 

En forma similar es posible estimar el número de personas debajo de la línea de pobreza. 

```{r}
tamano_pobreza <- diseno %>% 
                  group_by(Poverty) %>% 
                  summarise(
                  Nd = survey_total(vartype = c("se","ci")))
tamano_pobreza

names(diseno$variables)
```

Concluyendo para este ejemplo que, 58867.6 personas están por debajo de la línea de pobreza con una desviación estándar de 5731.3 y un intervalo de confianza (47518.9	70216.3).

Otra variable de interés en encuestas de hogares es conocer el estado de ocupación de las personas. A continuación, se muestra el código computacional:

```{r, eval=TRUE}
tamano_ocupacion <- diseno %>% 
                    group_by(Employment) %>% 
                    summarise( Nd = survey_total(vartype = c("se","ci")))
tamano_ocupacion
```

De los resultados de la estimación se puede concluir que, 4634.8 personas están desempleadas con un intervalo de confianza de (3128.6, 6140.9). 41465.2 personas están inactivas con un intervalo de confianza de (37182.6,	45747.8) y por último, 61877.0 personas empleadas con intervalos de confianza (36784.2, 47793.5).

Utilizando la función `group_by` es posible obtener resultados por más de un nivel de agregación. A continuación, se muestra la estimación ocupación desagregada por niveles de pobreza:

```{r tabs0, echo=TRUE, eval=TRUE}
tamano_ocupacion_pobreza <- diseno %>% 
                            group_by(Employment, Poverty) %>% 
                            cascade( Nd = survey_total(vartype = c("se","ci")), .fill = "Total") %>%
                            data.frame()
tamano_ocupacion_pobreza
```

De lo cual se puede concluir, entre otros que, 44600.3 personas que trabajan no son pobres con un intervalo de confianza (39459.6, 49741.0) y 6421.8 inactivas están en pobreza extrema con un intervalo de confianza de (3806.6,	9037.0).





## (5.2) Estimación de proporciones

Otro parámetro de interés en las encuestas de hogares, particularmente con variables categóricas, es la estimación de las proporciones poblacionales. Las proporciones permiten expresar el peso relativo que tienen determinados grupos dentro de la población. Por ejemplo, conocer el porcentaje de hogares que se encuentran por debajo de la línea de pobreza es esencial para evaluar las desigualdades socioeconómicas. Para obtener este indicador, se calcula el promedio ponderado de una variable dicotómica, lo que asegura que la estimación represente adecuadamente la distribución poblacional.

De acuerdo con Heeringa, West y Berglund (2017), al transformar las categorías de respuesta originales en variables indicadoras $y$ con valores de 1 y 0 (por ejemplo, 1 = Sí y 0 = No), la proporción estimada se obtiene mediante:

$$
\hat{p}_d = \frac{\hat{N}_d}{\hat{N}} = \frac{\displaystyle\sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} I(y_{hik}=d)} {\displaystyle\sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik}}
$$

Dado que se trata de un estimador no lineal, su varianza puede aproximarse mediante la técnica de linealización de Taylor, utilizando como función de estimación $z_{hik}=I(y_{hik}=d)-\hat{p}_d$. Actualmente, la mayoría de los programas estadísticos generan este tipo de proporciones junto con sus errores estándar, generalmente presentados en forma de porcentajes.

En términos de notación se define la estimación de proporciones de población como $p$ y proporciones de población como $\pi$. Es normal observar que en muchos paquetes estadísticos optan por generar estimaciones de proporciones y errores estándar en la escala de porcentaje. *R* genera las estimaciones de proporciones en escala [0,1].

En situaciones donde las proporciones estimadas se aproximan a 0 o 1, se vuelve necesario aumentar el tamaño de la muestra para garantizar resultados estadísticamente sólidos. Además, es recomendable aplicar métodos que aseguren que los intervalos de confianza permanezcan en el rango [0,1], ya que los intervalos convencionales basados en la normal pueden desbordar estos límites y perder su utilidad interpretativa. Para resolver este inconveniente, se han desarrollado alternativas para la construcción de intervalos de confianza, como las propuestas por Rust y Hsu (2007) y Dean y Pagano (2015).

Una de estas alternativas recurre a la transformación logit de la proporción estimada y se expresa de la siguiente forma:

$$
CI(\hat{p}_d; 1-\alpha) = \frac{\exp \left[\ln\left(\frac{\hat{p}_d}{1-\hat{p}_d}\right) \pm \frac{me(\hat{p}_d)}{\hat{p}_d(1-\hat{p}_d)}\right]}{1 + \exp \left[\ln\left(\frac{\hat{p}_d}{1-\hat{p}_d}\right) \pm \frac{me(\hat{p}_d)}{\hat{p}_d(1-\hat{p}_d)}\right]}
$$

donde $me(\hat{p}_d)$ representa el margen de error del estimador y se obtiene mediante
$$
me(\hat{p}_d) = t_{1-\alpha/2, df} \times se(\hat{p}_d),
$$
siendo $t_{1-\alpha/2, df}$ el cuantil de la distribución t de Student con $df$ grados de libertad, dejando un área $\alpha/2$ a su derecha. Los grados de libertad se calculan restando el número de estratos al número total de UPM ($df = n - H$).

Existen además otros métodos que permiten calcular intervalos de confianza incluso cuando la proporción es exactamente cero o uno, asegurando resultados interpretables en casos extremos.

A continuación se presenta el código computacional para estimar la proporción de personas por zona:

```{r, eval=TRUE}
prop_zona <- diseno %>% group_by(Zone) %>% 
             summarise(
             prop = survey_mean(vartype = c("se","ci"), 
                    proportion = TRUE ))
prop_zona
```

Como se pudo observar, se usó la función `survey_mean` para la estimación. Sin embargo, con el parámetro "proportion = TRUE", se le indica a `R` que lo que se desea estimar es una proporción. Para este ejemplo se puede observar que, el 47.9% de las personas viven en zona rural obteniendo un intervalo de confianza comprendido entre (45.2%,	50.7%) y el 52% de las personas viven en la zona urbana con un intervalo de confianza de (49.2%, 54.7%).

La librería `survey` tiene implementado una función específica para estimar proporciones la cual es `survey_prop` que genera los mismos resultados mostrados anteriormente. Le queda al lector la decisión de usar la función con la que más cómodo se sienta. A continuación, se muestra un ejemplo del uso de la función `survey_prop`:

```{r, eval=TRUE}
prop_zona2 <- diseno %>% group_by(Zone) %>% 
              summarise( prop = survey_prop(vartype = c("se","ci") ))
prop_zona2
```

Si el interés ahora se centra en estimar subpoblaciones por ejemplo, proporción de hombres y mujeres que viven en la zona urbana, el código computacional es:

```{r, eval=TRUE}
prop_sexoU <- sub_Urbano %>% group_by(Sex) %>% 
              summarise(prop = survey_prop(vartype = c("se","ci")))
prop_sexoU
```

Arrojando como resultado que, el 53.6% de las mujeres y 46.4% de los hombres viven en la zona urbana y con intervalos de confianza (51%, 56.2%) y (43.7%, 48.9%) respectivamente. Los intervalos anteriores nos reflejan que, con una confianza del 95% la cantidad estimada de mujeres que viven en la zona urbana es de 56% y de hombres es de 48%.

Realizando el mismo ejercicio anterior, pero ahora en la zona rural se tiene:

```{r, eval=TRUE}
prop_sexoR <- sub_Rural %>% group_by(Sex) %>% 
              summarise( n = unweighted(n()),
                         prop = survey_prop(vartype = c("se","ci")))
prop_sexoR
```

El 51.6% de las mujeres y el 48.4% de los hombres viven en la zona rural con intervalos de confianza de (49.9%, 53.2%) y (46,7%, 50%) respectivamente. Los intervalos de confianza anteriores nos reflejan que, inclusive, con una confianza del 95%, la cantidad estimada de mujeres en la zona rural es de 53% y de hombres es de 50%.

Ahora bien, si nos centramos solo en la población de hombres en la base de datos y se desea estimar la proporción de hombres por zona, el código computacional es el siguiente:

```{r, eval=TRUE}
prop_ZonaH <- sub_Hombre %>% group_by(Zone) %>% 
              summarise(prop = survey_prop(vartype = c("se","ci")))
prop_ZonaH
```

En la anterior tabla se puede observar que el 49% de los hombres están en la zona rural y el 51% en la zona urbana. Si se observa el intervalo de confianza se puede concluir que, con una confianza del 95%, la población estimada de hombres que viven en la zona rural puede llegar a ser el 52% y en urbana un 54%.

Si se realiza ahora el mismo ejercicio para las mujeres el código computacional es:

```{r, eval=TRUE}
prop_ZonaM <- sub_Mujer %>% group_by(Zone) %>% 
              summarise(prop = survey_prop(vartype = c("se","ci")))
prop_ZonaM
```

De la tabla anterior se puede inferir que, el 47% de las mujeres están en la zona rural y el 52% en la zona urbana. Observando también intervalos de confianza al 95% de (44%, 49%) y (50%, 55%) para las zonas rural y urbana respectivamente.

Si se desea estimar por varios niveles de desagregación, con el uso de la función `group_by` es posible estimar un mayor número de niveles de agregación al combinar dos o más variables. Por ejemplo, si se desea estimar la proporción de hombres por zona y en estado de pobreza, se realiza de la siguiente manera:

```{r, prop_ZonaH_Pobreza, eval=TRUE}
prop_ZonaH_Pobreza <- sub_Hombre %>%
                      group_by(Zone, Poverty) %>% 
                      summarise(
                      prop = survey_prop(vartype = c("se","ci")))%>%
                      data.frame()
prop_ZonaH_Pobreza
```

De la salida anterior se puede observar que, en la ruralidad, el 19% de los hombres están en pobreza extrema mientras que en la zona urbana el 11% también están en pobreza extrema. Por otro lado, el 54% de los hombres que viven en la zona rural no están en pobreza mientras que, en la zona urbana el 65% no está en esta condición.

El mismo ejercicio anterior para la población de mujeres sería:

```{r, eval=TRUE}
prop_ZonaM_Pobreza <- sub_Mujer %>% 
                      group_by(Zone, Poverty) %>% 
                      summarise( prop = survey_prop(vartype = c("se","ci"))) %>%
                      data.frame()
prop_ZonaM_Pobreza
```

De la salida anterior se puede observar que, en la ruralidad, el 16% de las mujeres están en pobreza extrema mientras que en la zona urbana el 10% también están en pobreza extrema. Por otro lado, el 55% de las mujeres que viven en la zona rural no están en pobreza mientras que, en la zona urbana el 66% no está en esta condición.

Si lo que se desea ahora es estimar la proporción de hombres empleados o no por zona, se realiza de la siguiente manera:

```{r, eval=TRUE}
prop_ZonaH_Ocupacion <- sub_Hombre %>%
                        group_by(Zone, Employment) %>% 
                        summarise(prop = survey_prop(vartype = c("se","ci")))%>%
                        data.frame()
prop_ZonaH_Ocupacion
```

De la salida anterior se puede observar que, el 5% de los hombres que viven en la ruralidad están desempleados mientras que el 4% de los que viven en la zona urbana están en esta misma condición. Ahora bien, el 52% de los hombres que viven en la ruralidad trabajan mientras que el 51% de los que viven en la zona rural también están empleados.

Si se hace este mismo ejercicio para las mujeres se obtiene:

```{r,prop_ZonaM_Ocupacio, eval=TRUE}
prop_ZonaM_Ocupacion <- sub_Mujer %>% 
                        group_by(Zone, Employment) %>% 
                        summarise(prop = survey_prop(vartype = c("se","ci"))) %>%
                        data.frame()
prop_ZonaM_Ocupacion
```

Para las mujeres se puede observar que, el 1% de las mujeres que viven en la ruralidad están desempleadas mientras que el 2% de las que viven en la zona urbana están en esta misma condición. Ahora bien, el 24% de las mujeres que viven en la ruralidad trabajan mientras que el 38% de las que viven en la zona rural también están empleadas.

Otro parámetro que es de interés es estimar en encuestas de hogares la cantidad de personas menores y mayores de edad en los hogares. A continuación, ejemplificamos la estimación de menores y mayores a 18 años cruzado por pobreza:

```{r, tabs01, echo=TRUE, eval=TRUE}
diseno %>% group_by(edad_18, pobreza) %>% 
           summarise(Prop = survey_prop(vartype =  c("se", "ci"))) %>%
           data.frame()
```

De la anterior salida se puede observar que, el 50% de los menores de edad y el 33% de los mayores de edad están en estado de pobreza. Al observar los intervalos de confianza para los menores de edad en estado de pobreza se puede observar que, dicha estimación puede llegar, con una confianza del 95% a 57% mientras que a los mayores de edad puede llegar a 39%.

Ahora, si se hace este mismo ejercicio, pero esta vez cruzando con la variable que indica empleo se obtiene:

```{r, tabs2, echo=TRUE, eval = TRUE}
diseno %>% group_by(edad_18, desempleo) %>% 
           summarise(Prop = survey_prop(vartype =  c("se", "ci"))) %>%
           data.frame()
```

De la tabla anterior se puede observar que, el 0.3% de los menores de edad y el 4% de los mayores de edad están desempleados. Adicionalmente, con una confianza del 95% y basados en la muestra se puede observar que el desempleo en menores de edad puede llegar a 0.7% y para los mayores llega a un 5%.

Por otro lado, si el objetivo ahora es estimar la cantidad de menores de edad en la zona rural se realiza de la siguiente manera:

```{r}
sub_Rural %>% group_by(edad_18) %>% 
              summarise(Prop = survey_prop(vartype =  c("se", "ci"))) %>%
              data.frame()
```

De la anterior tabla se puede observar que, el 37% de las personas que viven en la zona rural de la base de ejemplo son menores de edad con un intervalo de confianza al 95% comprendido entre 31% y 43%.

Como se mencionó al inicio del capítulo, es posible categorizar una variable de tipo cuantitativo como por ejemplo la edad y cruzarla con la variable que categoriza la empleabilidad. A continuación, se estima la edad de las mujeres por rango.

```{r,tabtemp1,eval= TRUE}
sub_Mujer %>% mutate(edad_rango = case_when(
                     Age>= 18 & Age <=35  ~ "18 - 35", TRUE ~ "Otro")) %>%
                     group_by(edad_rango, Employment) %>% 
                     summarise(Prop = survey_prop(vartype =  c("se", "ci"))) %>% 
                     data.frame()
```

De la anterior tabla se puede observar, entre otros que, las mujeres con edades entre 18 y 35 años el 2% están desempleadas y el 45% están empleadas. Análisis similares se pueden hacer para los demás rangos de edades.

Este mismo ejercicio se puede realizar para los hombres y hacer los mismos análisis. A continuación, se muestra el código computacional:

```{r, tab_01, echo=TRUE,eval= TRUE}
sub_Hombre %>% mutate(edad_rango = case_when(
                      Age>= 18 & Age <=35  ~ "18 - 35",TRUE ~ "Otro")) %>%
                      group_by(edad_rango, Employment) %>% 
                      summarise(Prop = survey_prop(vartype =  c("se", "ci"))) %>% 
                      data.frame()
```









## (5.3) Tablas cruzadas

En los levantamientos de encuestas de hogares, es habitual recopilar información sobre variables categóricas, que permiten clasificar a la población en grupos mutuamente excluyentes. Ejemplos comunes son el estado laboral ("ocupado", "desempleado", "inactivo"), el nivel educativo alcanzado ("primaria", "secundaria", "terciaria") o el acceso a determinados servicios ("Sí", "No"). Explorar si dos de estas variables están asociadas constituye un elemento central del análisis de encuestas, pues ofrece información valiosa para distintas aplicaciones: en política pública (relacionando educación y empleo para diseñar estrategias laborales), en evaluación de programas (detectando variaciones en el acceso a salud según el nivel de ingresos) o en investigación social (estudiando vínculos entre factores demográficos y servicios) para comprender dinámicas y tendencias sociales.

El estudio de la asociación entre dos variables categóricas implica verificar si la distribución de una depende de la otra. Para ello, se comparan las frecuencias de todas las combinaciones posibles de categorías. Por ejemplo, es posible contabilizar cuántos individuos corresponden simultáneamente a cada nivel educativo y estado laboral. Estas frecuencias pueden transformarse en proporciones que muestran la participación relativa de cada combinación en la población. Como punto de partida, suele emplearse una tabla de contingencia, que organiza en forma de cuadrícula los conteos o proporciones para cada cruce de categorías; en ella, un eje puede representar el estado laboral y el otro los niveles de educación.

Para formalizar este análisis, se introduce una notación específica. Sean $x$ y $y$ dos variables categóricas con $R$ y $C$ categorías, respectivamente. Para plantear pruebas de hipótesis sobre su independencia, se asume un modelo de superpoblación, en el que los pares $(x_{hik}, y_{hik})$ corresponden a observaciones de vectores aleatorios $(X,Y)$ con distribución conjunta definida como:

$$
P_{rc} = Pr(X=r ; Y=c), \quad r=1,\dots,R;\, c=1,\dots,C \quad (9-24)
$$

y que cumple $\sum_{r}\sum_{c} P_{rc}=1$.

Si en lugar de una muestra se dispusiera de un censo que recogiera la información de $x$ y $y$ para toda la población, sería posible calcular el número de unidades en cada celda $(r,c)$ mediante:

$$
N_{rc} = \sum_{h=1}^{H} \sum_{i \in U_{1h}} \sum_{k \in U_{hi}} I(x_{hik}=r ; y_{hik}=c) \quad (9-25)
$$

y obtener las proporciones poblacionales $p_{rc} = N_{rc}/N_{(++)}$, donde $N_{(++)}$ es el total de unidades en la población. Estas proporciones permiten aproximar las probabilidades $P_{rc}$ bajo el modelo de superpoblación. En la práctica, como se trabaja con una muestra, estas proporciones se estiman a través de los estimadores ponderados explicados en secciones previas.

Las tablas cruzadas, o tablas de contingencia, constituyen una de las herramientas esenciales para el análisis de encuestas, ya que permiten organizar los datos de manera estructurada y mostrar cómo se distribuyen las frecuencias de dos o más variables categóricas. Gracias a este tipo de resumen, es posible identificar patrones y asociaciones que no serían evidentes a simple vista. Este procedimiento es ampliamente utilizado en la investigación y en el diseño de políticas públicas, pues facilita la exploración de relaciones entre variables. Por ejemplo, puede emplearse para analizar cómo varía la condición laboral en función del nivel educativo o cómo se distribuye el acceso a internet en zonas urbanas y rurales. Además, la literatura especializada suele referirse a estas como tablas de contingencia, y su interpretación puede reforzarse mediante gráficos de barras apiladas, los cuales permiten visualizar tendencias y diferencias de forma más intuitiva.

Una tabla de contingencia puede definirse como una matriz de doble entrada en la que las filas, indexadas por $r=1,\dots,R$, y las columnas, indexadas por $c=1,\dots,C$, corresponden a las categorías de dos variables. Cada celda de la tabla contiene la frecuencia o proporción de casos que presentan simultáneamente la combinación $(r,c)$. De manera opcional, la tabla puede incluir totales marginales, que resumen los datos por fila o columna, y un total general que representa el tamaño poblacional total. Por ejemplo, las filas pueden representar niveles educativos y las columnas los estados de participación en el mercado laboral, permitiendo observar la distribución conjunta de ambas variables.

En el análisis de encuestas de hogares, las frecuencias incluidas en las tablas de contingencia no se obtienen mediante simples conteos de casos, sino a partir de frecuencias ponderadas, calculadas con los pesos muestrales de la encuesta. Cada celda de la tabla representa, de esta forma, el número estimado de personas en la población que pertenecen a la combinación específica de categorías.

En la mayoría de las encuestas de hogares, el resultado estándar es una tabla de doble entrada con las frecuencias ponderadas, que funcionan como estimaciones de las frecuencias poblacionales. La frecuencia estimada en cada celda $(r,c)$ se calcula mediante:

$$
\hat{N}_{rc} = \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} I(x_{hik}=r; y_{hik}=c) \quad (9-26)
$$

y los totales marginales se obtienen como:

$$
\hat{N}_{(r+)} = \sum_{c} \hat{N}_{rc}, \qquad \hat{N}_{(+c)} = \sum_{r} \hat{N}_{rc}, \qquad \hat{N}_{(++)} = \sum_{r} \sum_{c} \hat{N}_{rc}.
$$

Una tabla de contingencia se asume como un arreglo bidimensional de $r=1,\ldots,R$ filas y $c=1,\ldots,C$ columnas. Cabe resaltar que las tablas cruzadas o de contingencia no se limitan a dos dimensiones, también se pueden incluir una tercera variable o más, es decir, $l=1,\ldots,L$ subtablas basadas en las categorías de una tercera variable.

Para efectos de ilustración y facilitación de los ejemplos y conceptos teóricos, en esta sección se trabajarán, en su mayoría con tablas $2\times2$. Gráficamente, estas tablas se construyen con frecuencias no estimadas como se muestra a continuación:

| Variable 2        | Variable 1 |           | Marginal fila |
|-------------------|------------|-----------|---------------|
|                   | 0          | 1         |               |
| 0                 | $n_{00}$   | $n_{01}$  | $n_{0+}$      |
| 1                 | $n_{10}$   | $n_{11}$  | $n_{1+}$      |
| Marginal columna  | $n_{+0}$   | $n_{+1}$  | $n_{++}$      |

A continuación, se muestra la tabla de doble entrada con las frecuencias estimadas o ponderadas:

| Variable 2        | Variable 1 |               | Marginal fila |
|-------------------|------------|---------------|---------------|
|                   | 0          | 1             |               |
| 0                 | $\hat{N}_{00}$ | $\hat{N}_{01}$ | $\hat{N}_{0+}$ |
| 1                 | $\hat{N}_{10}$ | $\hat{N}_{11}$ | $\hat{N}_{1+}$ |
| Marginal columna  | $\hat{N}_{+0}$ | $\hat{N}_{+1}$ | $\hat{N}_{++}$ |

donde, por ejemplo, la frecuencia ponderada o estimada en la celda (0, 1) está dada por $\hat{N}_{01}={\sum_{h=1}^{H}\sum_{\alpha=1}^{\alpha_{h}}\sum_{i\in\left(0,1\right)}^{n_{h\alpha}}}\omega_{h\alpha i}$. Las proporciones estimadas a partir de estas frecuencias muestrales ponderadas se obtienen de la siguiente manera $p_{rc}=\frac{\hat{N}_{rc}}{\hat{N}_{++}}$.


### Proporciones relativas en tablas de contingencia

Las frecuencias ponderadas pueden transformarse en proporciones, las cuales expresan el peso relativo de cada grupo en relación con la población total o respecto a los totales de las filas y columnas de una tabla. Estas proporciones resultan especialmente valiosas para comparar grupos de distinto tamaño o para analizar la distribución relativa de las categorías.

El cálculo de las proporciones a partir de las frecuencias muestrales ponderadas se realiza mediante las siguientes expresiones:

$$
\hat{p}_{rc} = \frac{\hat{N}_{rc}}{\hat{N}_{++}} \quad (9-27)
$$

así como

$$
\hat{p}_{r+} = \frac{\hat{N}_{r+}}{\hat{N}_{++}}, \quad \hat{p}_{+c} = \frac{\hat{N}_{+c}}{\hat{N}_{++}}
$$

Estas proporciones permiten presentar las estimaciones de frecuencias relativas en la población, facilitando la comparación entre categorías de diferentes tamaños. Por ejemplo, la siguiente tabla ilustra cómo se organizarían estas proporciones:

| Variable 2 / Variable 1 | 1       | ...     | C       | Total fila |
|-------------------------|---------|---------|---------|------------|
| 1                       | $\hat{p}_{11}$ | ...     | $\hat{p}_{1C}$ | $\hat{p}_{(1+)}$ |
| ...                     | ...     | $\hat{p}_{rc}$ | ...     | ...        |
| R                       | $\hat{p}_{R1}$ | ...     | $\hat{p}_{RC}$ | $\hat{p}_{(R+)}$ |
| Total columna           | $\hat{p}_{(+1)}$ | ...     | $\hat{p}_{(+C)}$ | 1          |


### Estimación de proporciones para variables binarias

La estimación de una sola proporción, $\pi$, para una variable de respuesta binaria requiere solo una extensión directa del estimador de razón mostrado en secciones anteriores. Como lo menciona *Heeringa, S. G. (2017)*, al recodificar las categorías de respuesta originales en una sola variable indicadora $y_{i}$ con valores posibles de 1 y 0 (por ejemplo, sí = 1, no = 0), el estimador de la media de la razón estima la proporción o prevalencia, $\pi$, de "1" en la población está dada por:

$$
p = \frac{{\sum_{h=1}^{H}\sum_{\alpha=1}^{\alpha_{h}}\sum_{i\in\left(0,1\right)}^{n_{h\alpha}}}\omega_{h\alpha i}I\left(y_{i}=1\right)}{{\sum_{h=1}^{H}\sum_{\alpha=1}^{\alpha_{h}}\sum_{i\in\left(0,1\right)}^{n_{h\alpha}}}\omega_{h\alpha i}} = \frac{\hat{N}_{1}}{\hat{N}}
$$

Aplicando Linealización de Taylor (TSL) al estimador de razón de $\pi$ genera el siguiente estimador para la varianza:

$$
v\left(p\right) \dot{=} \frac{V\left(\hat{N}_{1}\right)+p^{2}V\left(\hat{N}\right)-2\,p\,cov\left(\hat{N}_{1},\hat{N}\right)}{\hat{N}^{2}}
$$

Como es bien sabido en la literatura especializada, cuando la proporción de interés estimada está cerca de 0 o 1, los límites del intervalo de confianza estándar basados en el diseño de muestreo pueden ser menores que 0 o superiores a 1. Lo cual no tendría interpretación por la naturaleza del parámetro. Es por lo anterior que, para solventar este problema se puede realizar cálculos alternativos de IC basados en el diseño de muestreo para las proporciones como lo proponen *Wilson modificado (Rust y Hsu, 2007; Dean y Pagano, 2015)*. El intervalo de confianza utilizando la transformación $Logit\left(p\right)$ está dado por:

$$
IC\left[logit\left(p\right)\right] = \left\{ ln\left(\frac{p}{1-p}\right)\pm\frac{t_{1-\alpha/2,\,gl}se\left(p\right)}{p\left(1-p\right)}\right\} 
$$

Por tanto, el intervalo de confianza para $p$ sería:

$$
IC\left(p\right) = \left\{ \frac{exp\left[ln\left(\frac{p}{1-p}\right)\pm\frac{t_{1-\alpha/2,\,gl}se\left(p\right)}{p\left(1-p\right)}\right]}{1+exp\left[ln\left(\frac{p}{1-p}\right)\pm\frac{t_{1-\alpha/2,\,gl}se\left(p\right)}{p\left(1-p\right)}\right]}\right\} 
$$

Ahora bien, si el interés es estimar proporciones para variables multinomiales. El estimador es el siguiente:

$$
p_{k} = \frac{{\sum_{h=1}^{H}\sum_{\alpha=1}^{\alpha_{h}}\sum_{i=1}^{n_{h\alpha}}}\omega_{h\alpha i}I\left(y_{i}=k\right)}{{\sum_{h=1}^{H}\sum_{\alpha=1}^{\alpha_{h}}\sum_{i=1}^{n_{h\alpha}}}\omega_{h\alpha i}} = \frac{\hat{N}_{k}}{\hat{N}}
$$

A continuación, siguiendo con la base de ejemplo, se estima la proporción de hombres y mujeres en pobreza y no pobreza junto con su error estándar e intervalos de confianza.

```{r, tab2, eval=T}
prop_sexo_zona <- diseno %>% 
                  group_by(pobreza,Sex) %>%
                  summarise(prop = survey_prop(vartype = c("se", "ci"))) %>% 
                  data.frame()

prop_sexo_zona
```

Como se puede observar, el 52.3% de las mujeres y el 47.6% son pobres. Generando intervalos de confianza al 95% de (49.2%, 55.5%) para las mujeres y (44.5%, 50.7%) para los hombres.

En la librería survey existe una alternativa para estimar tablas de contingencias y es utilizando la función `svyby` como se muestra a continuación:

```{r, eval=TRUE}
tab_Sex_Pobr <- svyby(formula = ~Sex, by =  ~pobreza, design = diseno, FUN = svymean)
tab_Sex_Pobr
```

Como se pudo observar, los argumentos que requiere la función son definir la variable a la cual se desea estimar (formula), las categorías por la cual se desea estimar (by), el diseño muestral (design) y el parámetro que se desea estimar (FUN). Para la estimación de los intervalos de confianza se utiliza la función `confint` como sigue:

```{r}
confint(tab_Sex_Pobr) %>% as.data.frame()
```

Los cuales coinciden con los generados anteriormente usando la función `group_by`.

Otro análisis de interés relacionado con tablas de doble entrada en encuestas de hogares es estimar el porcentaje de desempleados por sexo.

```{r, tab_02, echo=TRUE,eval=T}
tab_Sex_Ocupa <- svyby(formula = ~Sex,  by = ~Employment,
                       design = diseno, FUN = svymean)
tab_Sex_Ocupa
```

De la anterior salida se puede observar que, el 27.2% de las mujeres y el 72.7% de los hombres están desempleados con errores estándares para estas estimaciones de 5.3% para mujeres y hombres. Cuyos intervalos de confianza se calculan a continuación:

```{r}
confint(tab_Sex_Ocupa) %>% as.data.frame()
```

Si ahora el objetivo es estimar la pobreza, pero por las distintas regiones que se tienen en la base de datos. Primero, dado que la variable *pobreza* es de tipo numérica, es necesario convertirla en factor y luego realizar la estimación con la función `svyby`.

```{r, eval=TRUE}
tab_region_pobreza <- svyby(formula = ~as.factor(pobreza),  by = ~Region, 
                            design =  diseno, FUN = svymean)
tab_region_pobreza
```

De lo anterior se puede concluir que, en la región Norte, el 35% de las personas están en estado de pobreza mientras que en el sur es el 34%. La pobreza más alta se tiene en la región oriente con un 45% de pobres. Los errores estándares de las estimaciones.




### Prueba de independencia χ²

Una prueba de hipótesis es un método estadístico que permite analizar la validez de una afirmación o supuesto sobre una población. Para ello, se plantea una hipótesis nula (H₀), que corresponde a la proposición que se desea verificar, y una hipótesis alternativa (H₁), que plantea lo contrario. Estas hipótesis suelen originarse en la experiencia o en ciertas creencias y se evalúan a partir de la información obtenida en una muestra. La aceptación de una u otra dependerá de la evidencia estadística derivada de los datos. Este procedimiento recibe el nombre de contraste o prueba de hipótesis.

En los estudios de hogares resulta relevante establecer si dos variables categóricas guardan relación entre sí o si son independientes, es decir, si la distribución de una no depende de las categorías de la otra. Un ejemplo de ello sería indagar: "¿Existe vínculo entre el nivel educativo y la situación laboral?". Para resolver este tipo de interrogantes se aplican pruebas de independencia, las cuales contrastan los datos observados con los valores que se esperarían en caso de que no hubiera relación entre las variables.

Para realizar estas pruebas, se parte del supuesto de que los datos provienen de una superpoblación, entendida como una población hipotética de mayor tamaño. La información recolectada en la encuesta se interpreta como una muestra de esa superpoblación y el análisis busca generalizar los resultados hacia ella. La hipótesis nula constituye el punto inicial del contraste y establece que las dos variables no guardan relación entre sí. En este caso, la probabilidad de pertenecer a una combinación específica de categorías equivale al producto de sus probabilidades marginales.

Según lo señalado por Heeringa, West y Berglund (2017), esta hipótesis nula se formula de la siguiente manera:
H₀: P_rc = P_r+ × P_+c, para todo r=1,…,R y c=1,…,C

En consecuencia, la prueba de independencia consiste en contrastar las proporciones estimadas p̂_rc con las proporciones poblacionales que se esperarían bajo la hipótesis nula, P_rc⁰. Cuando la discrepancia entre ambas es amplia, los datos no respaldan la independencia entre las variables. Dicho de otro modo, si los valores observados se apartan significativamente de los esperados bajo H₀, se obtiene evidencia en contra de su validez.

La evaluación de la independencia en encuestas resulta más compleja que en los contrastes estadísticos convencionales, ya que es necesario introducir ajustes que consideren el diseño muestral. En este sentido, el ajuste de Rao-Scott adapta la prueba chi-cuadrado clásica para reflejar dichas particularidades. La estadística de prueba habitual se corrige mediante el efecto de diseño generalizado (GDEFF), que cuantifica la complejidad del muestreo. Una vez ajustada, la estadística conserva una distribución chi-cuadrado bajo la hipótesis nula.

El estadístico de Rao-Scott, denotado como X²_RS (Rao y Scott, 1984), se expresa de la siguiente forma:

$$
X_{RS}^2 = \frac{n_{++}}{GDEFF} \sum_r \sum_c \frac{(\hat{p}_{rc} - \hat{P}_{rc}^0)^2}{\hat{P}_{rc}^0} \quad (9-29)
$$

donde $\hat{P}_{rc}^0 = \hat{p}_{r+} \times \hat{p}_{+c}$ corresponde a las frecuencias esperadas en cada celda bajo la hipótesis nula, y el GDEFF constituye una estimación del efecto de diseño generalizado (Heeringa, West y Berglund, 2017, p. 177). Bajo H₀, la distribución asintótica de X²_RS se aproxima a una χ² con (R-1)(C-1) grados de libertad.

Cuando se trabaja con muestras pequeñas o con pocos grados de libertad, la aplicación de ajustes basados en la distribución F al estadístico X²_RS permite obtener estimaciones más precisas. Según lo indicado por Heeringa, West y Berglund (2017), los primeros en plantear correcciones al estadístico chi-cuadrado de Pearson a partir del efecto de diseño generalizado fueron Fay (1979) y Fellegi (1980). Más adelante, estos aportes fueron ampliados por Rao y Scott (1984) y Thomas y Rao (1987).

El procedimiento de Rao-Scott implica la estimación de efectos de diseño generalizados, cuya complejidad analítica supera la de la propuesta original de Fellegi. Aun así, las pruebas ajustadas de Rao-Scott constituyen actualmente el criterio de referencia en el análisis de datos categóricos de encuestas mediante programas estadísticos especializados.

Esta prueba es una de las más utilizadas para determinar si no existe asociación o independencia entre dos variables de tipo cualitativa. En otras palabras, que dos variables sean independientes significa que una no depende de la otra, ni viceversa. 

A modo de ejemplificar la técnica, para una tabla de $2\times2$, la prueba $\chi^{2}$ de Pearson se define como:

$$
\chi^{2}  =  n_{++}\sum_{r}\sum_{c}\frac{\left(p_{rc}-\hat{\pi}_{rc}\right)^{2}}{\hat{\pi}_{rc}}
$$

donde, $\hat{\pi}_{rc}=\frac{n_{r+}}{n_{++}}\,\frac{n_{+c}}{n_{++}}\,p_{r+}\,p_{+c}$.

Para realizar la prueba de independencia $\chi^{2}$ en `R`, se utilizará la función `svychisq` del paquete `srvyr`. Esta función implementa la corrección de Rao-Scott y requiere que se definan las variables de interés (formula) y el diseño muestral (design). Ahora, para ejemplificar el uso de esta función tomaremos la base de datos de ejemplo y se probará si la pobreza es independiente del sexo. A continuación, se presentan los códigos computacionales: 

```{r}
svychisq(formula = ~Sex + pobreza, design = diseno, statistic="F")
```

Dado que el p-valor es superior al nivel de significancia 5% se puede concluir que, con una confianza del 95% y basado en la muestra, la pobreza no depende del sexo de las personas.

En este mismo sentido, si se desea saber si el desempleo está relacionado con el sexo, se realiza la prueba de hipótesis $\chi^{2}$ como sigue:

```{r, eval=TRUE}
svychisq(formula = ~Sex + Employment, 
         design = diseno,  statistic="F")
```

Concluyendo que, con una confianza del 95% y basado en la muestra se rechaza la hipótesis nula, es decir, no se puede afirmar que las variables sexo y desempleo sean independientes. 

Si en el análisis ahora se quiere verificar que la pobreza de las personas es independiente de las regiones establecidas en la base de datos, se realiza de la siguiente manera:

```{r}
svychisq(formula = ~Region + pobreza, 
         design = diseno,  statistic="F")
```

Concluyendo que, con una confianza del 95% y basado en la muestra hay independencia entre la pobreza y la región. Lo anterior implica que, no existe relación lineal entre las personas en estado de pobreza por región.





### Razón de odds

Como lo menciona *Monroy, L. G. D. (2018)*, la traducción más aproximada del término odds es "la ventaja", en términos de probabilidades es la posibilidad de que un evento ocurra con relación a que no ocurra, es decir, es un número que expresa cuánto más probable es que se produzca un evento frente a que no se produzca. También se puede utilizar para cuantificar la asociación entre los niveles de una variable y un factor categórico *(Heeringa, S. G. 2017)*.

Suponga que se desea calcular la siguiente razón de odds:

$$
\frac{\frac{P(Sex = Female \mid pobreza = 0 )}{P(Sex = Female \mid pobreza = 1 )}}{\frac{P(Sex = Male \mid pobreza = 1 )}{P(Sex = Male \mid pobreza = 0 )}}
$$

El procedimiento para realizarlo en `R` sería, primero estimar las proporciones de la tabla cruzada entre las variables sexo y pobreza:

```{r, echo = TRUE, eval = TRUE}
tab_Sex_Pobr <- svymean(x = ~interaction(Sex, pobreza), design = diseno, 
                        se=T, na.rm=T, ci=T, keep.vars=T)

tab_Sex_Pobr %>% as.data.frame()
```

Luego, se realiza el contraste dividiendo cada uno de los elementos de la expresión mostrada anteriormente:

```{r, echo = TRUE, eval = TRUE}
svycontrast(stat = tab_Sex_Pobr, 
contrasts = quote((`interaction(Sex, pobreza)Female.0`/`interaction(Sex, pobreza)Female.1`) /(`interaction(Sex, pobreza)Male.0`/ `interaction(Sex, pobreza)Male.1`)))
```

Obteniendo que, se estima que el odds de las mujeres que no están en estado de pobreza es 1.02 comparándolo con el odds de los hombres. En otras palabras, se estima que las probabilidades de que las mujeres no estén en estado de pobreza sin tener en cuenta ninguna otra variable de la encuesta es cerca de 2% mayor que las probabilidades de los hombres.

### Diferencia de proporciones en tablas de contingencias

**Pruebas para la comparación de grupos**

El análisis de medias entre diferentes grupos es un objetivo habitual en las encuestas de hogares. Un ejemplo típico sería la pregunta: "¿Se presentan diferencias estadísticamente significativas en el ingreso medio entre hogares dirigidos por hombres y aquellos dirigidos por mujeres?".

Con el fin de responder a este tipo de cuestiones, se aplican procedimientos estadísticos que incorporan los ajustes necesarios derivados del diseño de muestreo. En este apartado se describen los enfoques para contrastar diferencias de medias teniendo en cuenta la estructura muestral, mediante ejemplos que facilitan su comprensión, e introduciendo tanto las pruebas t como los contrastes diseñados para ajustarse a las características del diseño de la encuesta.

Como lo menciona *Heeringa, S. G. (2017)*, las estimaciones de las proporciones de las filas en las tablas de doble entrada son, de hecho, estimaciones de subpoblaciones en las que la subpoblación se define por los niveles de la variable factorial. Ahora bien, si el interés se centra en estimar diferencias de las proporciones de las categorías entre dos niveles de una variable factorial, se pueden utilizar contrastes. 

A manera de ejemplo, se requiere estimar ahora, el contraste de proporciones de mujeres en estado de pobreza versus los hombres en esta misma condición ($\hat{p}_F - \hat{p}_M$). Para ello, primero, estimemos la proporción de hombres y mujeres en estado de pobreza como se ha mostrado en capítulos anteriores:

```{r}
(tab_sex_pobreza <- svyby(formula = ~pobreza, by = ~Sex, 
                          design = diseno, svymean, na.rm=T,
                          covmat = TRUE, vartype = c("se", "ci")))
```

Ahora bien, para calcular la estimación de la diferencia de proporciones junto con sus errores estándares, se realizarán los siguientes pasos:

- **Paso 1**: Calcular la diferencia de estimaciones 
```{r}
0.3892 - 0.3946
```

Con la función `vcov` se obtiene la matriz de covarianzas:

```{r}
library(kableExtra)
vcov(tab_sex_pobreza) %>% data.frame() %>% 
  kable(digits = 10,
        format.args = list(scientific = FALSE))
```

- **Paso 2**: El cálculo del error estándar es:   
```{r}
sqrt(0.0009983 + 0.0013416 - 2*0.0009183)
```

Ahora bien, aplicando la función `svycontrast` se puede obtener la estimación de la diferencia de proporciones anterior:

```{r}
svycontrast(tab_sex_pobreza,
            list(diff_Sex = c(1, -1))) %>%
  data.frame()
```

De lo que se concluye que, la diferencia entre las proporciones de mujeres y hombres en estado de pobreza es -0.005 (-0.5%) con una desviación estándar de 0.022.

Otro ejercicio de interés en un análisis de encuestas de hogares es verificar la diferencia del desempleo por sexo. Al igual que el ejemplo anterior, se inicia con la estimación del porcentaje de desempleados por sexo:

```{r}
tab_sex_desempleo <- svyby(formula = ~desempleo, by = ~Sex, 
                           design = diseno %>% filter(!is.na(desempleo)), 
                           FUN = svymean, na.rm=T, covmat = TRUE,
                           vartype = c("se", "ci"))
tab_sex_desempleo
```

Para calcular la estimación de la diferencia de proporciones junto con sus errores estándares, se realizarán los siguientes pasos:

- **Paso 1**: Diferencia de las estimaciones 
```{r}
0.02169 - 0.06783
```

Estimación de la matriz de covarianza:

```{r}
vcov(tab_sex_desempleo) %>% data.frame() %>% 
  kable(digits = 10,
        format.args = list(scientific = FALSE))
```

- **Paso 2**: Estimación del error estándar.
```{r}
sqrt(0.00003114 + 0.00014789 - 2*0.00002081)
```

Siguiendo el ejemplo anterior, utilizando la función `svycontrast` se tiene que:

```{r}
svycontrast(tab_sex_desempleo,
            list(diff_Sex = c(-1, 1))) %>%
  data.frame()
```

De lo que se concluye que, la estimación del contraste es 0.04 (4%) con un error estándar de 0.011.

Otro ejercicio que se puede realizar en una encuesta de hogares es ahora estimar la proporción de desempleados por región. Para la realización de este ejercicio, se seguirán los pasos de los dos ejemplos anteriores:

```{r}
tab_region_desempleo <- svyby(formula = ~desempleo, by = ~Region, 
                              design = diseno %>% filter(!is.na(desempleo)), 
                              FUN = svymean, na.rm=T, covmat = TRUE,
                              vartype = c("se", "ci"))
tab_region_desempleo
```

Ahora, el interés es realizar los contrastes siguientes para desempleo: 

$\hat{p}_{Norte} - \hat{p}_{Centro} = 0.01004$, 
$\hat{p}_{Sur} - \hat{p}_{Centro} = 0.02691$ 	
$\hat{p}_{Occidente} - \hat{p}_{Oriente} = 0.01046$	

Escrita de forma matricial sería:

$$
\left[\begin{array}{ccccc}
1 & 0 & -1 & 0 & 0\\
0 & 1 & -1 & 0 & 0\\
0 & 0 & 0 & 1 & -1
\end{array}\right]
$$

La matriz de varianzas y covarianzas es:

```{r, tab_03, echo=TRUE, eval=FALSE}
vcov(tab_region_desempleo) %>%
  data.frame() %>% 
  kable(digits = 10,
        format.args = list(scientific = FALSE))
```

Por tanto, la varianza estimada está dada por:

```{r}
sqrt(0.0002981 + 0.0002884 - 2*0)
sqrt(0.0001968 + 0.0002884 - 2*0)
sqrt(0.0001267 + 0.0004093 - 2*0)
```

Usando la función `svycontrast`, la estimación de los contrastes sería:

```{r}
svycontrast(tab_region_desempleo, list(
                             Norte_sur = c(1, 0, -1, 0, 0),
                             Sur_centro = c(0, 1, -1, 0, 0),
                             Occidente_Oriente = c(0, 0, 0, 1, -1))) %>% data.frame()
```

Por último, repitiendo el contraste anterior y los pasos para resolverlo, pero ahora utilizando la variable pobreza se tiene:

```{r}
tab_region_pobreza <- svyby(formula = ~pobreza, by = ~Region, 
                            design = diseno %>% filter(!is.na(desempleo)), 
                            FUN = svymean, na.rm=T, covmat = TRUE,
                            vartype = c("se", "ci"))
tab_region_pobreza
```

El interés se centra en realizar los contrastes siguientes para pobreza: 

$\hat{p}_{Norte} - \hat{p}_{Centro}$, 
$\hat{p}_{Sur}-\hat{p}_{Centro}$ 	
$\hat{p}_{Occidente}-\hat{p}_{Oriente}$	

Escrita de forma matricial es:

$$
\left[\begin{array}{ccccc}
1 & 0 & -1 & 0 & 0\\
0 & 1 & -1 & 0 & 0\\
0 & 0 & 0 & 1 & -1
\end{array}\right]
$$

Y, utilizando la función `svycontrast` se obtiene:

```{r}
svycontrast(tab_region_pobreza, list(
                Norte_sur = c(1, 0, -1, 0, 0),
                Sur_centro = c(0, 1, -1, 0, 0),
                Occidente_Oriente = c(0, 0, 0, 1, -1))) %>% data.frame()
```






## (5.4 nueva) Prueba de hipótesis para la diferencia de parámetros

Diversos parámetros de interés —como las diferencias de medias o las medias ponderadas— pueden representarse mediante combinaciones lineales de estadísticas descriptivas. Estas expresiones son de gran utilidad, por ejemplo, en la construcción de índices económicos o en la comparación de medias poblacionales. Para evaluar la precisión de dichas estimaciones, resulta esencial calcular la varianza de la combinación.

Entre los casos más frecuentes se encuentran las diferencias de medias y las sumas ponderadas de medias aplicadas en índices económicos. En este sentido, se define una función como combinación lineal de J estadísticas descriptivas:

$$
f(\theta_1,\ldots,\theta_J)=\sum_{j=1}^J a_j \theta_j
$$

donde los $a_j$ corresponden a constantes conocidas. El estimador de esta función se formula como:

$$
\hat{f}(\hat{\theta}_1,\ldots,\hat{\theta}_J)=\sum_{j=1}^J a_j \hat{\theta}_j
$$

La varianza asociada se obtiene mediante:

$$
Var\left(\sum_{j=1}^J a_j \hat{\theta}_j\right)=\sum_{j=1}^J a_j^2 Var(\hat{\theta}_j) + 2 \sum_{j=1}^{J-1}\sum_{k>j}^J a_j a_k Cov(\hat{\theta}_j, \hat{\theta}_k)
$$

Este cálculo requiere no solo disponer de las varianzas de cada estimador individual, sino también de las covarianzas entre ellos, lo que permite una medición más completa de la precisión de la estimación.

En la comparación de dos poblaciones, es posible formular distintas hipótesis de contraste. Una de ellas, la hipótesis nula, sostiene que los parámetros de interés —como totales, medias, proporciones o razones— son equivalentes en ambas poblaciones. En contraposición, la hipótesis alternativa plantea que existe una diferencia entre dichos parámetros o que uno de ellos resulta mayor o menor respecto al otro.

Un aspecto de especial interés es el estudio de la diferencia entre medias poblacionales. Para plantear las pruebas de hipótesis en este contexto, se recurre a un modelo de superpoblación, bajo el cual se asume que $y_{hik}$ corresponde a observaciones de variables aleatorias $Y$, idénticamente distribuidas, cuya media es $\mu_{(y,d)}$ cuando la unidad $k$ pertenece al dominio $d$, siendo $d=1,2$.

Con base en ello, la diferencia entre las medias de los dominios 1 y 2 se expresa como:

$$
\mu_{(y,1)} - \mu_{(y,2)}
$$

A modo ilustrativo, puede considerarse el caso en el que $\mu_{(y,1)}$ representa el ingreso promedio de los hogares con jefatura masculina, mientras que $\mu_{(y,2)}$ corresponde al ingreso promedio de los hogares con jefatura femenina.

La diferencia entre medias puede estimarse de forma consistente a través de la expresión:

$$
\hat{\bar{Y}}_1 - \hat{\bar{Y}}_2 \quad (9-33)
$$

donde $\hat{\bar{Y}}_d$ corresponde al estimador muestral de $\mu_{(y,d)}$ para cada dominio $d=1,2$.

En este contexto, las hipótesis a contrastar suelen formularse de la siguiente manera:
- **Hipótesis nula (H₀)**: Las medias poblacionales no presentan diferencias significativas.
- **Hipótesis alternativa (H₁)**: Se observa una diferencia entre las medias, la cual puede manifestarse en cualquiera de los dos sentidos, ya sea mayor o menor.

Para evaluar una de estas hipótesis se utiliza el siguiente estadístico de contraste:

$$
t = \frac{\hat{\bar{Y}}_1 - \hat{\bar{Y}}_2}{se(\hat{\bar{Y}}_1 - \hat{\bar{Y}}_2)}
$$

Este estadístico se distribuye según una t de Student con $df$ grados de libertad ($t \sim t_{(df)}$), donde $df$ se determina restando del número total de unidades primarias de muestreo (PSUs) $n$ el número de estratos $H$.

El error estándar correspondiente se define como:

$$
\hat{se}(\hat{\bar{Y}}_1 - \hat{\bar{Y}}_2) = \sqrt{\hat{Var}(\hat{\bar{Y}}_1) + \hat{Var}(\hat{\bar{Y}}_2) - 2\hat{Cov}(\hat{\bar{Y}}_1, \hat{\bar{Y}}_2)}
$$

En este sentido, el contraste se fundamenta en la estimación de la varianza y la covarianza de las medias muestrales, lo que permite determinar con precisión si la diferencia observada entre ellas es estadísticamente significativa.

El intervalo de confianza para la diferencia entre medias se obtiene a partir de la estimación puntual de dicha diferencia, el error estándar correspondiente y el valor crítico de la distribución t para un nivel de significancia dado. Este intervalo define un rango dentro del cual es probable que se ubique la verdadera diferencia poblacional, aportando así una interpretación más completa y robusta de los resultados.

La expresión matemática del intervalo es:

$$
\hat{\bar{Y}}_1 - \hat{\bar{Y}}_2 \; \pm \; t_{(1-\alpha/2,\,df)} \; \hat{se}(\hat{\bar{Y}}_1 - \hat{\bar{Y}}_2)
$$

Este procedimiento general también puede aplicarse a otros parámetros, tales como proporciones, totales, razones o, en términos más amplios, cualquier función diferenciable de los totales. Debido a que su distribución muestral se aproxima igualmente a una distribución t (Valliant & Rust, 2010), los intervalos de confianza y las pruebas de hipótesis pueden formularse siguiendo el mismo criterio: empleando la estimación puntual de la diferencia, el error estándar asociado y el valor crítico correspondiente de la distribución t.
# (6) Modelos de regresión bajo diseños de muestreo complejos

La regresión estadística constituye una técnica fundamental para examinar los vínculos entre variables en el marco de los datos muestrales obtenidos mediante encuestas. A través de este procedimiento, es posible determinar la forma en que una o varias variables de respuesta (dependientes) se relacionan con una o varias variables explicativas (independientes). Tal como exponen Nolan y Speed (2000) y Freedman (2005), la validez de los resultados depende de una adecuada formulación del modelo.

Un ejemplo ilustrativo sería la estimación del ingreso de los hogares (variable dependiente) en función del nivel educativo alcanzado y de la situación laboral de sus miembros (variables independientes), empleando datos provenientes de encuestas de hogares.

No obstante, dado que estas encuestas suelen estar sustentadas en diseños muestrales complejos, los enfoques tradicionales de regresión resultan insuficientes, por lo que deben ser modificados y ajustados para garantizar inferencias válidas bajo este tipo de esquemas.

## Enfoques inferenciales para el análisis de datos

En el trabajo con datos de encuestas, un desafío central consiste en identificar y abordar la variabilidad que caracteriza a la información. Dicha variabilidad proviene de dos fuentes principales: por un lado, el diseño muestral, relacionado con la estrategia utilizada para recolectar los datos; y, por otro, el modelo estadístico, empleado para examinar la información y derivar inferencias sobre la población.

Con el fin de integrar estas dos fuentes de variabilidad dentro de un mismo marco analítico, se requieren técnicas inferenciales avanzadas. Estas metodologías buscan, simultáneamente, preservar las particularidades del diseño muestral y contemplar los supuestos e incertidumbres propios del modelo. Entre las aproximaciones más destacadas se encuentran la pseudo-verosimilitud (Molina & Skinner, 1992) y la inferencia combinada (Binder, 2011).

### Pseudo-verosimilitud

El método de seudo-verosimilitud adapta las técnicas tradicionales de verosimilitud con el fin de considerar las particularidades del diseño de una encuesta. Bajo este enfoque, la inferencia se apoya en la distribución de muestreo repetido determinada por el diseño, mientras que la distribución del modelo tiene un rol secundario (aunque, si el modelo está bien especificado, las estimaciones obtenidas mediante seudo-verosimilitud pueden resultar insesgadas o consistentes). En palabras más sencillas, este procedimiento ajusta los métodos de modelado habituales para representar de manera adecuada cómo se seleccionó la muestra. Tales ajustes son esenciales, pues pasar por alto el diseño muestral puede generar estimaciones sesgadas y conclusiones equivocadas acerca de la población.

### Inferencia combinada

En contraste, la inferencia combinada pretende articular en un solo marco la información derivada del diseño de la encuesta y la del modelo. Con ello se asegura que las incertidumbres provenientes de ambas fuentes —el muestreo y el modelo— queden representadas en los resultados. Al integrar estos elementos, este enfoque brinda una visión más amplia de la variabilidad y favorece la obtención de estimaciones más precisas y confiables.

## Modelos lineales: fundamentos teóricos

### Definiciones básicas

Un modelo de regresión permite describir la influencia que una o varias variables independientes (explicativas) ejercen sobre una variable dependiente (de respuesta). En su versión más sencilla, la regresión lineal estudia la relación entre una única variable independiente y una variable dependiente. La variable dependiente refleja el fenómeno de interés, mientras que la independiente representa los posibles factores que lo afectan. Además, el modelo incorpora un término de error, encargado de representar la parte de la variabilidad que no puede ser explicada por las variables incluidas.

Un modelo de regresión lineal simple se expresa como $y = \beta_0 + \beta_1x + \varepsilon$, en el que $y$ corresponde a la variable dependiente, $x$ a la variable independiente y $\beta_0$ y $\beta_1$ a los parámetros del modelo. El término $\varepsilon$ se denomina error aleatorio y representa la parte de la variabilidad que el modelo no logra explicar.

En escenarios más complejos, la regresión lineal múltiple posibilita incorporar varias variables independientes, lo que permite analizar el efecto conjunto de distintos factores sobre la variable de interés. En este tipo de modelos, cada variable independiente está asociada a un coeficiente que refleja tanto la intensidad como la dirección de su relación con la variable dependiente. Un coeficiente positivo indica que, al incrementarse la variable independiente, también tiende a aumentar la dependiente.

Como una extensión del modelo simple, la regresión lineal múltiple puede expresarse así:
$$y = x\beta + \varepsilon = \sum_{j=0}^p \beta_j x_j + \varepsilon = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \varepsilon \quad (9-37)$$

Otra manera de representar este modelo es:
$$y_k = x_k \beta + \varepsilon_k \quad (9-38)$$

donde $x_k = (1, x_{k1}, \ldots, x_{kp})$ y $\beta = (\beta_0, \beta_1, \ldots, \beta_p)^T$. El subíndice $k$ señala el elemento específico de la población.

### Supuestos de los modelos de regresión

Los modelos de regresión se fundamentan en ciertas condiciones básicas. Según Heeringa, West y Berglund (2017), estas pueden resumirse así:

- **$E(\varepsilon_k∣x_k) = 0$**: Para cualquier valor de la variable independiente, el promedio de los errores es nulo, lo que indica que el modelo no tiende a sobreestimar ni subestimar los resultados de manera sistemática.
- **$Var(\varepsilon_k∣x_k) = \sigma^2_{(y|x)}$**: La dispersión de los errores se mantiene constante en todos los niveles de las variables independientes, característica conocida como homocedasticidad o igualdad de varianzas.
- **$\varepsilon_k∣x_k \sim N(0, \sigma^2_{(y|x)})$**: Los errores, al condicionarse por las covariables, siguen una distribución normal. Esta propiedad se extiende igualmente a la variable dependiente $y_k$.
- **$cov(\varepsilon_k, \varepsilon_l∣x_k, x_l) = 0$** para todo $k \neq l$: Los errores de distintas observaciones son independientes entre sí, lo que implica que el resultado de una observación no afecta el de otra. De este modo, los errores en diferentes elementos observados no guardan correlación con los valores estimados a partir de sus variables predictoras.

Cuando se satisfacen estas condiciones, los modelos de regresión permiten obtener estimaciones fiables y sin sesgos acerca de las relaciones entre las variables. Las predicciones generadas reflejan los resultados esperados a partir de las variables independientes, lo que convierte a la regresión en una herramienta valiosa para identificar patrones en los datos y realizar proyecciones con fundamento. Por su parte, Shah, Holt y Folsom (1977) abordan las implicaciones de incumplir estas suposiciones y plantean métodos apropiados para llevar a cabo inferencias sobre los parámetros de la regresión lineal en el contexto de datos de encuestas.

## Estimación de parámetros en diseños complejos

### Desafíos en la estimación

Los enfoques estadísticos clásicos parten del supuesto de que los datos de la muestra son independientes, están distribuidos de manera idéntica y siguen una distribución de probabilidad definida (como Binomial, Poisson, Exponencial, Normal, entre otras). No obstante, en los diseños de encuestas complejas estas condiciones suelen no cumplirse, ya que pueden implicar estratificación, conglomerados o probabilidades de selección distintas entre elementos. En consecuencia, al aplicar modelos de regresión con este tipo de datos, los estimadores tradicionales (como los de máxima verosimilitud o mínimos cuadrados) pueden conducir a resultados sesgados.

### Estimación de parámetros con pesos muestrales

Como ejemplo, puede analizarse la estimación de la pendiente $(\beta_1)$ en un modelo de regresión lineal simple. Utilizando el método de ecuaciones de estimación, el estimador se expresa de la siguiente manera:
$$\hat{\beta}_1 = \frac{\sum_{h=1}^H \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} \, (y_{hik} - \hat{\bar{Y}})(x_{hik} - \hat{\bar{X}})}{\sum_{h=1}^H \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} \, (x_{hik} - \hat{\bar{X}})^2} \quad (9-40)$$

La principal diferencia entre este estimador y el clásico radica en la utilización de pesos de muestreo. La generalización de estos procedimientos a modelos de regresión múltiple implica una mayor complejidad algebraica y excede el alcance de este capítulo; para un análisis más profundo, se recomienda consultar a Heeringa, West y Berglund (2017).

En el caso de la regresión múltiple, el cálculo de la varianza de cada coeficiente se realiza considerando su interdependencia con los demás coeficientes del modelo. Esto conduce a la construcción de una matriz de varianza-covarianza, que refleja tanto la variabilidad individual como las relaciones entre todos los coeficientes estimados. En términos generales, Kish y Frankel (1974) señalan que la estimación de la varianza de los coeficientes en un modelo de regresión lineal múltiple exige emplear totales ponderados de los cuadrados y de los productos cruzados de todas las combinaciones entre $y$ y $x = \{1, x_1, …, x_p\}$.

## Desarrollo histórico y evolución metodológica

Según lo expuesto por Heeringa, West y Berglund (2017), Kish y Frankel (1974) fueron los primeros en examinar de manera empírica cómo los diseños de muestreo complejos afectan las inferencias en modelos de regresión, poniendo de relieve las dificultades que estos implican. Posteriormente, Fuller (1975) propuso un estimador de varianza para los parámetros de regresión mediante la linealización de Taylor, considerando ponderaciones desiguales en observaciones provenientes de diseños estratificados y de dos etapas. 

Más adelante, Binder (1983) obtuvo las distribuciones muestrales de los estimadores de parámetros de regresión en poblaciones finitas, además de estimadores de varianza aplicables a muestras complejas. Por su parte, Skinner, Holt y Smith (1989) analizaron las propiedades de los estimadores de varianza de los coeficientes de regresión bajo este tipo de diseños. 

En años posteriores, Fuller (2002) recopiló diferentes métodos de estimación de modelos de regresión en muestras complejas, mientras que Pfeffermann (2011) revisó enfoques alternativos para ajustar modelos de regresión lineal en datos de encuestas con diseños de muestreo complejos.

## Inferencia y validación de modelos

### Pruebas de significancia

Tras comprobar que el modelo presenta un buen ajuste y cumple con los supuestos planteados, el siguiente paso es determinar si las variables independientes aportan una explicación significativa a la variable dependiente. Para ello, se realizan pruebas de significancia sobre los coeficientes de regresión. Cuando un coeficiente es estadísticamente significativo, se infiere que la variable correspondiente guarda una relación importante con la variable dependiente.

Considerando las propiedades de distribución de los estimadores de los coeficientes de regresión, una forma habitual de comprobar la significancia de estos parámetros es mediante una estadística basada en la distribución t de Student, expresada como:
$$t = \frac{\hat{\beta}_j - \beta_j}{se(\hat{\beta}_j)} \sim t_{(df - p)}$$

En este caso, $df$ hace referencia a los grados de libertad (calculados como el número de unidades primarias de muestreo menos el número de estratos) y $p$ corresponde a la cantidad de variables independientes incluidas en el modelo.

### Intervalos de confianza y predicción

Este estadístico de prueba sirve para contrastar las hipótesis H₀: βⱼ = 0 frente a la alternativa H₁: βⱼ ≠ 0. De igual forma, se puede calcular un intervalo de confianza al $(1-\alpha)\times100\%$ para βⱼ, el cual se define como:
$$\hat{\beta}_j \pm t_{(1-\alpha/2, \, df)} \cdot se(\hat{\beta}_j)$$

De acuerdo con Neter, Wasserman y Kutner (1996), los modelos de regresión lineal cumplen esencialmente dos funciones: por un lado, explicar la variable de interés a partir de ciertos predictores, y por otro, estimar los valores de dicha variable, ya sea dentro del rango observado en la muestra o incluso más allá de este. El primer objetivo ya se ha desarrollado en esta sección, mientras que el segundo se logra mediante la siguiente expresión:
$$\hat{\mu}_k = \hat{E}(y_k \mid x_k) = x_k \hat{\beta} \quad (9-43)$$

Esta fórmula permite obtener el valor estimado de $y_k$ a partir de las variables explicativas observadas. Los resultados pueden aplicarse tanto en análisis inferenciales como en usos prácticos, tales como completar datos faltantes, realizar pronósticos o construir poblaciones sintéticas.

## ¿Aplicar o no aplicar ponderaciones?

### Enfoques metodológicos

Heeringa, West y Berglund (2017) examinan el desafío de determinar cómo utilizar adecuadamente los pesos en modelos de regresión y si conviene emplear factores de expansión al estimar coeficientes de regresión en encuestas con diseños complejos. En este marco, se distinguen dos enfoques principales para incorporar los pesos en los modelos:

**Enfoque orientado al diseño**: busca realizar inferencias válidas sobre la población total. Los pesos de la encuesta resultan indispensables para obtener estimaciones insesgadas de los coeficientes, ya que corrigen las probabilidades desiguales de selección derivadas del diseño muestral. Sin embargo, este método no protege frente a la mala especificación del modelo: si el modelo no refleja de manera adecuada las relaciones existentes en la población, aunque los estimadores resulten insesgados dentro de la formulación planteada, no necesariamente representarán de forma significativa a la población real.

**Enfoque orientado al modelo**: sostiene que los pesos no son necesarios siempre que el modelo esté correctamente formulado y el muestreo sea no informativo, es decir, que el modelo válido para la muestra coincida con el de la población. Este enfoque parte del supuesto de que las relaciones entre variables están bien descritas por el modelo sin importar el diseño de muestreo, y que la utilización de ponderaciones podría incrementar de forma innecesaria la variabilidad de las estimaciones, elevando los errores estándar.

### Criterios prácticos para la decisión

La decisión entre utilizar o no ponderaciones en los modelos de regresión depende tanto del contexto como de la sensibilidad de los resultados a su inclusión. Autores como Skinner, Holt y Smith (1989) y Pfeffermann (2011) han debatido ampliamente sobre la pertinencia de incorporar los pesos muestrales en la estimación de los parámetros de regresión y en sus errores estándar. Una recomendación metodológica es estimar los modelos con y sin ponderaciones y comparar los resultados. Si al incluir los pesos se observan variaciones significativas en los coeficientes o en las conclusiones, esto sugiere que el muestreo fue informativo o que el modelo presenta deficiencias de especificación, por lo que es preferible emplear estimaciones ponderadas. En cambio, si los pesos solo aumentan los errores estándar sin afectar sustancialmente los coeficientes, puede asumirse que el modelo está bien planteado y que no es indispensable ponderar.

En términos prácticos, la decisión puede resumirse en dos escenarios:

**Inferencia descriptiva**: es obligatorio aplicar ponderaciones, ya que el objetivo es reflejar con precisión la estructura de la población.

**Inferencia analítica**: es posible recurrir a modelos no ponderados o ajustados por pesos. En este caso, si la meta es analizar relaciones o verificar hipótesis, la ponderación no siempre es necesaria, especialmente cuando el modelo incluye variables del diseño muestral (estratos o conglomerados). Sin embargo, cuando se utilicen modelos sin ponderar, se debe justificar y documentar de manera explícita, pues implican supuestos más restrictivos que los modelos ponderados.

### Ventajas y limitaciones de las ponderaciones

El uso de ponderaciones en las encuestas permite asegurar que los modelos de regresión sean representativos de la población, ya que corrigen posibles sesgos de sobre o subrepresentación de determinados grupos y garantizan que la distribución poblacional se refleje adecuadamente. Asimismo, estas ponderaciones contribuyen a obtener estimaciones de varianza más exactas, pues consideran la estratificación, el agrupamiento y las probabilidades desiguales de selección, lo cual genera errores estándar, intervalos de confianza y pruebas estadísticas confiables.

Dentro del enfoque basado en el diseño, los coeficientes de regresión se estiman a partir de ecuaciones poblacionales ajustadas con ponderaciones. Esto permite que los resultados ponderados se aproximen a valores insesgados comparables a los que se obtendrían en un censo completo, incluso cuando el modelo estadístico no esté formulado de manera óptima.

Sin embargo, el uso de ponderaciones en la muestra puede aumentar la varianza de las estimaciones de los parámetros, en especial cuando los pesos presentan una gran dispersión. En situaciones donde existen valores extremos o muy variables, las estimaciones tienden a volverse inestables, ya que ciertas observaciones llegan a ejercer una influencia desproporcionada sobre el ajuste del modelo. En este sentido, cuando el propósito es explicativo o analítico (como en el análisis de relaciones entre variables), los modelos sin ponderar pueden, en ocasiones, generar resultados más consistentes y eficientes.

## Métodos de ajuste de pesos

### Pesos tipo Senado

Este procedimiento ajusta los pesos de manera que su suma coincida con el tamaño de la muestra en lugar del de la población. El objetivo es mantener la representatividad reduciendo la variabilidad de los pesos, lo cual resulta ventajoso en muestras grandes donde los pesos originales presentan una dispersión elevada:
$$w_k^{Senate} = w_k \times \frac{n}{\sum w_k}$$

### Pesos normalizados

Este enfoque reescala los pesos originales para que su suma sea igual a uno, evitando así un aumento innecesario de la varianza. Es especialmente útil al comparar modelos que utilizan diferentes subconjuntos de datos o cuando se busca minimizar la inflación de la varianza:
$$w_k^{Normalized} = \frac{w_k}{\sum w_k}$$

En estos métodos, los pesos se obtienen aplicando funciones multiplicativas directas a los pesos muestrales originales. Por ello, no son útiles para calcular totales ni modifican el coeficiente de variación de los mismos. Asimismo, no inciden en las estimaciones de razones, como medias o proporciones.

## Revisión y diagnóstico del modelo

### Importancia de la validación del modelo

Al aplicar modelos estadísticos a datos de encuestas de hogares, resulta indispensable comprobar su idoneidad para asegurar conclusiones válidas. Problemas comunes en la regresión clásica —como la multicolinealidad o la existencia de casos con gran influencia— también aparecen en el análisis de encuestas complejas. Para afrontarlos, se sugiere llevar a cabo procedimientos de diagnóstico que permitan evaluar los supuestos del modelo y su funcionamiento. Estos análisis ayudan a verificar si el modelo describe correctamente la información y si los resultados obtenidos son confiables. Entre los aspectos principales que deben revisarse se encuentran:

- **Adecuación del ajuste**: evaluar si el modelo explica una proporción significativa de la variabilidad de la variable de interés.
- **Normalidad de los errores**: comprobar si los errores siguen una distribución normal.
- **Homogeneidad de la varianza**: confirmar que la variabilidad de los errores se mantenga constante.
- **Independencia de los errores**: verificar que no exista correlación entre los errores de distintas observaciones.
- **Casos influyentes**: detectar observaciones con un efecto desproporcionado sobre la estimación del modelo.
- **Datos atípicos**: identificar registros que se apartan de la tendencia general de la información.

### Coeficiente de determinación (R²)

El coeficiente de determinación, o R², es una medida habitual para evaluar la bondad de ajuste de los modelos de regresión. Indica qué proporción de la varianza de la variable dependiente es explicada por el modelo y sus valores oscilan entre 0 y 1. Un valor cercano a 1 significa que el modelo explica gran parte de la variabilidad de los datos, mientras que un valor cercano a 0 sugiere lo contrario.

En encuestas con diseños de muestreo complejos, el estimador ponderado de R² se expresa como:
$$\hat{R}_\omega^2 = 1 - \frac{(\widehat{SSE})_\omega}{(\widehat{SST})_\omega}$$

donde $(\widehat{SSE})_\omega$ es la suma ponderada de los errores al cuadrado, calculada mediante:
$$(\widehat{SSE})_\omega = \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik} \,(y_{hik} - x_{hik}\hat{\beta})^2$$

y $(\widehat{SST})_\omega$ corresponde a la suma total ponderada de cuadrados, definida como:
$$(\widehat{SST})_\omega = \sum_{h=1}^{H} \sum_{i \in s_{1h}} \sum_{k \in s_{hi}} w_{hik}\,(y_{hik} - \hat{\bar{Y}})^2$$

Esta formulación permite evaluar el ajuste del modelo considerando los pesos muestrales propios de encuestas complejas.

### Análisis de residuos

#### Residuos estandarizados

Los residuos representan la diferencia entre los valores observados y los valores estimados por el modelo. Su análisis es esencial para determinar si el modelo cumple con los supuestos fundamentales. Al graficar los residuos frente a los valores predichos o las variables independientes, no debería observarse ningún patrón particular; si aparece alguno, podría señalar la presencia de varianza no constante (heterocedasticidad) o una relación no lineal entre las variables.

El análisis gráfico se emplea con frecuencia para identificar posibles problemas en el modelo, utilizando comúnmente los gráficos de residuos frente a los valores predichos como herramienta diagnóstica. Examinar estos residuos detenidamente permite al investigador verificar si el ajuste del modelo cumple con los supuestos establecidos o si alguno de ellos ha sido violado, lo que podría requerir una revisión de la especificación del modelo o del procedimiento de ajuste. Los residuos se definen de la siguiente manera:
$$r_{(p_k)} = \frac{y_k - \hat{\mu}_k}{\sqrt{V(\hat{\mu}_k)/w_k}} \quad (9-49)$$

Aquí, $\hat{\mu}_k$ representa el valor predicho de $y_k$ según el modelo ajustado, $w_k$ es el peso de encuesta correspondiente a la unidad $k$ en la muestra compleja, y $V(\hat{\mu}_k)$ es la función de varianza del resultado. Estos residuos se utilizan para evaluar tanto la normalidad como la homogeneidad de la varianza.

Si el supuesto de varianza constante no se cumple, los estimadores continúan siendo insesgados y consistentes, aunque pierden eficiencia, es decir, ya no tienen la menor varianza posible entre todos los estimadores insesgados. Una manera de evaluar este supuesto es mediante gráficos, representando los residuos del modelo frente a $\hat{y}$ o frente a $x_j$. La aparición de un patrón distinto a una dispersión aleatoria sugiere que la varianza de los errores no es constante.

### Detección de observaciones influyentes

En el análisis diagnóstico de modelos, otra técnica clave es la identificación de observaciones influyentes. Algunos puntos de datos pueden ejercer un efecto desproporcionado sobre el ajuste del modelo. Aunque estos puntos no necesariamente sean valores atípicos, pueden alterar de manera significativa las estimaciones de los parámetros. Una observación se considera influyente si su exclusión provoca cambios sustanciales en el ajuste del modelo. Cabe destacar que un punto influyente puede o no coincidir con un valor atípico.

Para detectar este tipo de observaciones, es esencial especificar el tipo de influencia que se desea evaluar. Entre las técnicas más utilizadas se encuentran:

- **Distancia de Cook**: cuantifica el efecto de eliminar un punto sobre el ajuste global del modelo.
- **Estadístico $D_f \text{Beta}_{(k)}$**: analiza el impacto de suprimir un punto en los coeficientes de regresión individuales, midiendo los cambios en el vector de coeficientes estimados.
- **Estadístico $D_f \text{Fits}_{(k)}$**: evalúa la influencia de un punto sobre el ajuste total del modelo, observando la variación en el ajuste al eliminar esa observación.

Estos métodos se explican con detalle en textos tradicionales de regresión, pero requieren adaptaciones para encuestas complejas. Para ello, se puede usar software especializado, como el paquete svydiags de R, que proporciona la mayoría de las herramientas necesarias para estos diagnósticos (véase Valliant, 2024).

## Consideraciones finales

Cuando el modelo está mal especificado, la regresión sin ponderaciones puede producir estimaciones poco útiles o carentes de validez. Por ello, es recomendable que los analistas se esfuercen en seleccionar e incorporar las variables pertinentes para lograr una especificación adecuada. Sin embargo, aun en los casos en que el modelo esté correctamente definido, es indispensable tener en cuenta la estratificación y el conglomerado del diseño muestral al calcular los errores estándar bajo un enfoque no ponderado. Del mismo modo, es necesario aplicar un análisis diagnóstico riguroso a los modelos estimados.

La elección entre los diferentes enfoques debe basarse en los objetivos específicos de la investigación, la naturaleza del diseño muestral y la calidad de la especificación del modelo. La práctica recomendada incluye la comparación sistemática de resultados obtenidos con diferentes métodos y la documentación transparente de las decisiones metodológicas adoptadas.
# (7) Gráficas en R


En esta sección se aborda la manera de presentar datos y estimaciones provenientes de encuestas de hogares mediante representaciones gráficas. Los gráficos, cuando se diseñan de forma adecuada, permiten identificar patrones, tendencias y relaciones en los datos, lo que facilita la interpretación de los hallazgos y su comunicación a públicos diversos. Si bien es posible utilizar gráficos estándar para mostrar distribuciones y asociaciones a partir de datos muestrales sin ponderar, estos pueden generar interpretaciones erróneas respecto a las distribuciones y asociaciones poblacionales. Por ello, se recomienda emplear gráficos modificados que incorporen los factores de expansión o pesos muestrales.

Por ejemplo, un gráfico de barras que represente la distribución del ingreso debe incorporar los pesos muestrales para reflejar de manera adecuada la distribución estimada de la población total. De igual forma, los diagramas de dispersión que analizan asociaciones entre variables deben utilizar marcadores ponderados o ajustes de densidad con el fin de asegurar representaciones más precisas. Además, al mostrar estimaciones derivadas de encuestas, que están sujetas a error de muestreo, es esencial reflejar esta condición presentando tanto los estimadores puntuales como los errores estándar o los intervalos de confianza.

Incorporar las características del diseño en las visualizaciones contribuye a que los lectores comprendan la incertidumbre inherente a las estimaciones de encuestas, favoreciendo interpretaciones más fundamentadas. Cuando las unidades de la encuesta poseen diferentes pesos de muestreo, estos deben ser considerados en los gráficos. La razón principal es que los pesos reflejan la cantidad de unidades poblacionales que representa cada unidad de la muestra.

Si los gráficos se elaboran sin considerar los pesos, la representación visual reflejará la muestra en lugar de la población. Esta discrepancia puede distorsionar las distribuciones, proporciones o relaciones entre variables. Incorporar los pesos garantiza que los gráficos ofrezcan una representación precisa de la población.

El objetivo de este capítulo es mostrarle al lector cómo hacer gráficos generales en `R` que incorporen adecuadamente los pesos muestrales. En todo análisis de encuestas, el componente gráfico es fundamental para revisar tendencias en algunas variables de interés. También son muy necesarias las gráficas cuando el objetivo es chequear algunos supuestos en el ajuste de modelos, por ejemplo, varianza constante en los errores, normalidad, etc.

Uno de los paquetes más usados para graficar en `R` es ggplot2, el cual es un paquete potente y flexible, implementado por Hadley Wickham, para producir gráficos elegantes. El gg en ggplot2 significa Grammar of Graphics, el cual es un concepto gráfico que describe gráficos usando gramática.

Como es de costumbre, se inicia este capítulo cargando las librerías y bases de datos:

```{r}
# knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, error = FALSE)
options(digits = 4)
library(survey)
library(srvyr)
library(convey)
library(TeachingSampling)
library(printr)
library(ggplot2)
library(patchwork)
```

El cargue de la base de datos se hace a continuación:

```{r}
data(BigCity, package = "TeachingSampling")
encuesta <- readRDS("Data/encuesta.rds")
```

A continuación, se define el diseño de muestreo:

```{r}
library(srvyr)
diseno <- encuesta %>%
  as_survey_design(
    strata = Stratum,
    ids = PSU,
    weights = wk,
    nest = T
  )
```

A partir de las variables de la encuesta, para efectos de los ejemplos, se definen las siguientes variables:

```{r, echo=TRUE, eval=TRUE}
diseno <- diseno %>% mutate(
  pobreza = ifelse(Poverty != "NotPoor", 1, 0),
  desempleo = ifelse(Employment == "Unemployed", 1, 0),
  edad_18 = case_when(
    Age < 18 ~ "< 18 años",
    TRUE ~ ">= 18 años"
  )
)
```

Como se mostró en capítulos anteriores, se divide la muestra en subgrupos para ejemplificar los conceptos que se mostrarán en este capítulo:

```{r}
sub_Urbano <- diseno %>% filter(Zone == "Urban")
sub_Rural <- diseno %>% filter(Zone == "Rural")
sub_Mujer <- diseno %>% filter(Sex == "Female")
sub_Hombre <- diseno %>% filter(Sex == "Male")
```

Para crear las gráficas en este texto se utilizará por defecto el tema que la CEPAL tiene asignado por defecto. El tema se define a continuación:

```{r}
theme_cepal <- function(...) {
  theme_light(10) +
    theme(
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      legend.position = "bottom",
      legend.justification = "left",
      legend.direction = "horizontal",
      plot.title = element_text(size = 20, hjust = 0.5),
      ...
    )
}
```

Este capítulo se enfocará en mostrar cómo crear visualizaciones que incorporen adecuadamente los pesos muestrales, incluyendo gráficos de barras ponderados, diagramas de dispersión con ajustes de densidad, y representaciones que muestren intervalos de confianza para reflejar la incertidumbre de las estimaciones. La correcta implementación de estas técnicas garantizará que las visualizaciones representen fielmente las características de la población bajo estudio.




## (7.1) Histogramas para graficar variables continuas

Los histogramas muestran cómo se distribuyen los valores de una variable numérica continua en la encuesta. El eje horizontal se divide en intervalos (bins), y cada barra representa el número de observaciones que caen dentro de ese rango. En un histograma, el área de cada barra corresponde al número o proporción de observaciones en el intervalo, y la altura se obtiene dividiendo este valor entre el ancho del intervalo. Esto difiere de un gráfico de barras, en el que la altura por sí sola representa el valor y el ancho de las barras no tiene un significado analítico. Al mostrar datos muestrales, deben incorporarse los pesos de muestreo para reflejar las frecuencias o frecuencias relativas de las unidades poblacionales dentro de los intervalos del histograma. Una aplicación común es el uso de histogramas para representar la distribución del ingreso o del gasto. Estas visualizaciones permiten a los investigadores observar la distribución expandida de la población, incluyendo su forma, dispersión y tendencias.

Los histogramas pueden servir para comparar subgrupos, como zonas geográficas (urbanas y rurales) o características demográficas como el sexo (masculino y femenino). Esto permite detectar diferencias importantes, por ejemplo, al analizar cómo se distribuyen los gastos entre áreas rurales y urbanas. Este tipo de comparaciones facilita visualizar posibles disparidades entre subgrupos, haciendo los resultados más comprensibles para quienes no están familiarizados con métodos técnicos de estimación. Cuando se combinan con estimaciones de densidad suavizadas, los histogramas ofrecen una visión más completa y precisa a nivel poblacional.

Un histograma es una representación gráfica de los datos de una variable empleando rectángulos (barras) cuya altura es proporcional a la frecuencia de los valores representados y su ancho proporcional a la amplitud de los intervalos de la clase.

Como se mencionó anteriormente, las gráficas se realizarán principalmente con la librería `ggplot2` y nos apoyamos en la librería `patchwork` para organizar la visual de las gráficas. A continuación, se presenta cómo realizar un histograma para la variable ingresos utilizando los factores de expansión de la encuesta. En primera instancia se define la fuente de información (data), luego se definen la variable a graficar (x) y los pesos de muestreo (weight). Una vez definido los parámetros generales del gráfico se define el tipo de gráfico, que para nuestro caso como es un histograma es geom_histogram. Se definen los títulos que se quiere que tenga el histograma y por último, se aplica el tema de la CEPAL.

```{r, hist1, echo = TRUE, eval = TRUE}
plot1_Ponde <- ggplot(
  data = encuesta,              
  aes(x = Income, weight = wk)) +
  geom_histogram(               
    aes(y = ..density..)) +        
  ylab("") +                   
  ggtitle("Ponderado") +        
  theme_cepal()  
plot1_Ponde
```

De forma análoga se define el siguiente histograma, note que en este caso se omitió el parámetro `weight`. Es decir, se genera un histograma sin pesos de muestreo:

```{r, hist1a, echo = TRUE, eval = TRUE}
plot1_SinPonde <-
  ggplot(encuesta, aes(x = Income)) +
  geom_histogram(aes(y = ..density..)) +
  ylab("") +
  ggtitle("Sin ponderar") +
  theme_cepal()
plot1_SinPonde
```

Ahora bien, para efectos de comparación, se grafica la variable ingreso tomada de la población (BigCity) y se muestran los tres histogramas para notar las diferencias que tienen en comparación con el poblacional.  

```{r, hist1b, echo = TRUE, eval = TRUE}
plot1_censo <- ggplot(BigCity, aes(x = Income)) +
  geom_histogram(aes(y = ..density..)) +
  ylab("") +
  ggtitle("Poblacional") +
  theme_cepal() +
  xlim(0, 2500)

plot1_censo | plot1_Ponde | plot1_SinPonde
```

Por otro lado, repetimos ahora la secuencia de gráficos pero en este caso para la variable *Expenditure*:

```{r, hist2, echo = TRUE, eval = TRUE}
plot2_Ponde <- ggplot(
  data =  encuesta,
  aes(x = Expenditure, weight = wk)
) +
  geom_histogram(aes(y = ..density..)) +
  ylab("") +
  ggtitle("Ponderado") +
  theme_cepal()
plot2_Ponde
```

```{r, hist2a, echo = TRUE, eval = TRUE}
plot2_SinPonde <- ggplot(data = encuesta,
      aes(x = Expenditure)) +
      geom_histogram(aes(y = ..density..)) +
      ylab("") +
      ggtitle("Sin ponderar") +
      theme_cepal()
plot2_SinPonde
```

```{r, hist2b, echo = TRUE, eval = FALSE}
plot2_censo <- ggplot(BigCity, aes(x = Expenditure)) +
  geom_histogram(aes(y = ..density..)) +
  ylab("") +
  ggtitle("Poblacional") +
  theme_cepal() +
  xlim(0, 1500)

plot2_censo | plot2_Ponde | plot2_SinPonde
```

Como conclusión, de ambos ejercicios, se puede observar que el histograma que mejor se aproxima al poblacional es aquel que utiliza los pesos de muestreo, aunque el gráfico que no los utiliza se aproxima bien y esto debido a la correcta selección de la muestra.

Por otro lado, cuando el interés ahora es realizar comparaciones entre dos o más agrupaciones, es posible hacer uso del parámetro `fill`, el cual "rellena" las barras del histograma con diferentes colores según sea el grupo. Para este ejemplo, se van a graficar subgrupos por zonas:   

```{r, hist3, echo = TRUE, eval = TRUE}
plot3_Ponde <- ggplot(
  encuesta,
  aes(x = Income, weight = wk)) +
  geom_histogram(
    aes(y = ..density.., fill = Zone),
    alpha = 0.5,
     position = "identity" 
  ) +
  ylab("") +
  ggtitle("Ponderado") +
  theme_cepal()
plot3_Ponde
```

Como se pudo observar en la generación del histograma, se utilizó el parámetro position el cual permite que las barras del gráfico sean distingibles.

Ahora se graficará la misma variable pero esta vez sin los pesos de muestreo:

```{r, hist3a, echo = TRUE, eval = TRUE}
plot3_SinPonde <- ggplot(encuesta, aes(x = Income)) +
  geom_histogram(aes(y = ..density.., fill = Zone),
    alpha = 0.5, position = "identity"
  ) +
  ggtitle("Sin ponderar") +
  theme_cepal() +
  ylab("")
plot3_SinPonde
```

Ahora, siguiendo el esquema de comparación anterior, se graficará la variable ingreso usando la información de la población y los subgrupos de zonas definidos anteriormente y por último, se muestran los 3 histogramas para poder compararlos:

```{r, echo = TRUE, eval = TRUE}
plot3_censo <- ggplot(BigCity, aes(x = Income)) +
  geom_histogram(aes(y = ..density.., fill = Zone),
    alpha = 0.5, position = "identity"
  ) +
  ggtitle("Poblacional") +
  theme_cepal() +
  xlim(0, 1500) +
  ylab("")
plot3_censo | plot3_Ponde | plot3_SinPonde
```

Ahora, repetimos la secuencia de gráficos anteriores pero, para la variable *Expenditure*:

```{r, echo = TRUE, eval = TRUE}
plot4_Ponde <- ggplot(
  encuesta,
  aes(x = Expenditure, weight = wk)
) +
  geom_histogram(aes(y = ..density.., fill = Zone),
    alpha = 0.5, position = "identity"
  ) +
  ylab("") +
  ggtitle("Ponderado") +
  theme_cepal()
plot4_Ponde
```

Sin ponderar,

```{r, echo = TRUE, eval = TRUE}
plot4_SinPonde <- ggplot(
  encuesta,
  aes(x = Expenditure)
) +
  geom_histogram(aes(y = ..density.., fill = Zone),
    alpha = 0.5, position = "identity"
  ) +
  ggtitle("Sin ponderar") +
  theme_cepal() +
  ylab("")
plot4_SinPonde
```

Poblacional,

```{r, echo = TRUE, eval = FALSE}
plot4_censo <- ggplot(BigCity, aes(x = Expenditure)) +
  geom_histogram(aes(y = ..density.., fill = Zone),
    alpha = 0.5, position = "identity"
  ) +
  ggtitle("Poblacional") +
  theme_cepal() +
  xlim(0, 1500) +
  ylab("")
plot4_censo | plot4_Ponde | plot4_SinPonde
```

Ahora, repetimos la secuencia de gráficos para la variable *Income*, pero hacemos las particiones por la variable *sexo*. Primero, hagamos el histograma ponderado:

```{r, echo = TRUE, eval = TRUE}
plot5_Ponde <- ggplot(
  encuesta,
  aes(x = Income, weight = wk)
) +
  geom_histogram(aes(y = ..density.., fill = Sex),
    alpha = 0.5, position = "identity"
  ) +
  ylab("") +
  ggtitle("Ponderado") +
  theme_cepal()
plot5_Ponde
```

Sin ponderar,

```{r, echo = TRUE, eval = TRUE}
plot5_SinPonde <- ggplot(encuesta, aes(x = Income)) +
  geom_histogram(aes(y = ..density.., fill = Sex),
    alpha = 0.5, position = "identity"
  ) +
  ggtitle("Sin ponderar") +
  theme_cepal() +
  ylab("")
plot5_SinPonde
```

Poblacional,

```{r, echo = TRUE, eval = TRUE}
plot5_censo <- ggplot(BigCity, aes(x = Income)) +
  geom_histogram(aes(y = ..density.., fill = Sex),
    alpha = 0.5, position = "identity"
  ) +
  ggtitle("Poblacional") +
  theme_cepal() +
  xlim(0, 1500) +
  ylab("")
plot5_censo | plot5_Ponde | plot5_SinPonde
```

Ahora, repetimos la secuencia de gráficos para la variable *Expenditure* desagregada por la variable *sexo*, primero, ponderado: 

```{r, echo = TRUE, eval = TRUE}
plot6_Ponde <- ggplot(
  encuesta,
  aes(x = Expenditure, weight = wk)
) +
  geom_histogram(aes(y = ..density.., fill = Sex),
    alpha = 0.5, position = "identity"
  ) +
  ylab("") +
  ggtitle("Ponderado") +
  theme_cepal()
plot6_Ponde
```

Sin ponderar,

```{r, echo = TRUE, eval = TRUE}
plot6_SinPonde <- ggplot(encuesta, aes(x = Expenditure)) +
  geom_histogram(aes(y = ..density.., fill = Sex),
    alpha = 0.5, position = "identity"
  ) +
  ggtitle("Sin ponderar") +
  theme_cepal() +
  ylab("")
plot6_SinPonde
```

Poblacional,

```{r, echo = TRUE, eval = TRUE}
plot6_censo <- ggplot(BigCity, aes(x = Expenditure)) +
  geom_histogram(aes(y = ..density.., fill = Sex),
    alpha = 0.5, position = "identity"
  ) +
  ggtitle("Poblacional") +
  theme_cepal() +
  xlim(0, 1500) +
  ylab("")
plot6_censo | plot6_Ponde | plot6_SinPonde
```

## (7.2) Agregando densidades y graficando Boxplot

Dadas las cualidades de la librería ggplot2, se pueden agregar nuevas capas a los gráficos, particularmente, a los histogramas antes realizados. La densidad se agrega con el argumento `geom_density` y se incorpora el parámetro `alpha` que regula la transparencia del relleno. A continuación, se muestra cómo se agregan las densidades:

```{r, out.width="60%", fig.align="center"}
plot1_Ponde + geom_density(fill = "blue", alpha = 0.3) |
  plot2_Ponde + geom_density(fill = "blue", alpha = 0.3)
```

Ahora bien, al aplicar `aes(fill = Zone)` permite que la densidad sea agregada para cada una de las agrupaciones como se muestra a continuación:

```{r,out.width="60%", fig.align="center"}
plot3_Ponde + geom_density(aes(fill = Zone), alpha = 0.3) |
  plot4_Ponde + geom_density(aes(fill = Zone), alpha = 0.3)
```

En esta oportunidad se agrega la densidad por sexo:

```{r}
plot5_Ponde + geom_density(aes(fill = Sex), alpha = 0.3) |
  plot6_Ponde + geom_density(aes(fill = Sex), alpha = 0.3)
```

Estos histogramas ponderados permiten visualizar adecuadamente las distribuciones poblacionales de variables continuas, incorporando los pesos muestrales para reflejar fielmente las características de la población. La comparación entre distribuciones de subgrupos facilita la identificación de patrones y disparidades, mientras que la adición de curvas de densidad suavizadas proporciona una visión más completa de la forma de las distribuciones.





## (7.3) Diagramas de dispersión

Los diagramas de dispersión son la herramienta ideal para explorar relaciones entre dos variables continuas, pudiendo revelar patrones o tendencias en los datos. Es importante mostrar en el gráfico que las distintas observaciones de la muestra tienen distintos pesos. Para tamaños de muestra pequeños o moderados, esto se puede representar usando círculos o puntos de tamaños variables, donde el tamaño del símbolo refleja el peso de muestreo de cada observación. Este tipo de gráficos puede generarse con software estándar de encuestas. Según Lumley (2010), al trabajar con conjuntos de datos grandes, mostrar todos los puntos en un diagrama de dispersión puede resultar abrumador y confuso, y existen varias estrategias para solucionarlo:

[1] **Submuestreo**: Seleccionar una submuestra aleatoria más pequeña y manejable del conjunto de datos completo. La submuestra debe elegirse con probabilidades proporcionales a los pesos de muestreo, garantizando que se comporte aproximadamente como una muestra aleatoria simple de la población. Esto mantiene la representatividad y mejora la interpretación.

[2] **Diagramas de dispersión con hexágonos**: Dividir el área del gráfico en una cuadrícula de hexágonos. En lugar de graficar puntos individuales, representar cada hexágono con sombreado o tamaño según la suma de los pesos de muestreo de los puntos dentro de él. Esta técnica condensa los datos en una visualización clara y comprensible.

[3] **Diagramas de dispersión suavizados**: Evitar graficar los puntos individuales y, en su lugar, estimar y mostrar tendencias. Por ejemplo, calcular cuantiles específicos de la variable del eje y condicionados a la variable del eje x y suavizar estos valores a lo largo del eje x. Este método resalta las tendencias mientras reduce el desorden visual.

Los diagramas de dispersión son una herramienta versátil y efectiva para explorar relaciones entre variables en datos de encuestas. Al incorporar los pesos de muestreo y aplicar estrategias para manejar grandes conjuntos de datos, pueden ofrecer información clara y significativa sobre los patrones a nivel poblacional. Ya sea usando puntos ponderados, binning hexagonal o técnicas de suavizado, los diagramas de dispersión continúan siendo fundamentales en la visualización de datos de variables continuas.

Un diagrama de dispersión o Scatterplot representa cada observación como un punto, posicionado según el valor de dos variables. Además de una posición horizontal y vertical, cada punto también tiene un tamaño, un color y una forma. Estos atributos se denominan estética y son las propiedades que se pueden percibir en el gráfico. Cada estética puede asignarse a una variable o establecerse en un valor constante. Para realizar este tipo de gráfico se usará la función `geom_point`. Para ejemplificar el uso de esta función, se graficarán las variables ingresos y gastos como se muestra a continuación:   

```{r, echo = TRUE, eval = TRUE}
plot19_Ponde <- ggplot( 
  data = encuesta,
      aes(
      y = Income,
      x = Expenditure,
      weight = wk)) +
  geom_point() +
  theme_cepal()
plot19_Ponde
```

Note que en este caso el parámetro `weight` no está aportando información visual al gráfico. El parámetro `weight` se puede usar para controlar el tamaño de los puntos, y así, tener un mejor panorama del comportamiento de la muestra:

```{r, hist14, echo = TRUE, eval = TRUE}
plot20_Ponde <- ggplot(
  data = encuesta,
    aes(y = Income, x = Expenditure)) +
  geom_point(aes(size = wk), alpha = 0.3) +
  theme_cepal()
plot20_Ponde
```

Otra forma de usar la variable `wk` es asignar la intensidad del color según el valor de la variable:

```{r, echo = TRUE, eval = TRUE}
plot21_Ponde <- ggplot(
  data = encuesta,
    aes(y = Income, x = Expenditure)) +
  geom_point(aes(col = wk), alpha = 0.3) +
  theme_cepal()
plot21_Ponde
```

Se puede extender las bondades de los gráficos de `ggplot2` para obtener mayor información de la muestra. Por ejemplo, agrupar los datos por Zona. Para lograr esto se introduce el parámetro `shape`:  

```{r, echo = TRUE, eval = TRUE}
plot22_Ponde <- ggplot(
  data = encuesta,
    aes(y = Income, 
        x = Expenditure,
        shape = Zone)) + 
  geom_point(aes(size = wk, color = Zone), alpha = 0.3) +
  labs(size = "Peso") + scale_color_manual(values = colorZona) +
  theme_cepal()
plot22_Ponde
```

De forma similar se puede obtener el resultado por sexo: 

```{r, echo = TRUE, eval = TRUE}
plot23_Ponde <- ggplot(
  data = encuesta,
    aes(
      y = Income,
      x = Expenditure,
      shape = Sex)) +
  geom_point(aes(
    size = wk,
    color = Sex),
  alpha = 0.3) +
  labs(size = "Peso") +
  scale_color_manual(values = colorSex) +
  theme_cepal()
plot23_Ponde
```

Un resultado equivalente se obtiene por región: 

```{r, echo = TRUE, eval = TRUE}
plot24_Ponde <- ggplot(
  data = encuesta,
        aes(
      y = Income,
      x = Expenditure,
      shape = Region)) +
  geom_point(aes(
    size = wk,
    color = Region),
  alpha = 0.3) +
  labs(size = "Peso") +
  scale_color_manual(values = colorRegion) +
  theme_cepal()
plot24_Ponde
```

Estos diagramas de dispersión ponderados permiten visualizar adecuadamente las relaciones entre variables continuas en datos de encuestas, incorporando los pesos muestrales para reflejar la estructura poblacional. La representación mediante tamaños y colores variables según los pesos facilita la identificación de patrones y tendencias a nivel poblacional, mientras que la agrupación por categorías permite explorar diferencias entre subgrupos.







## (7.4) Gráficos de barras para variables categóricas

Los gráficos de barras se utilizan con frecuencia para visualizar datos categóricos provenientes de tablas de contingencia, resumiendo conteos o proporciones ponderadas y asegurando que los resultados reflejen las características a nivel poblacional en lugar de limitarse a los datos muestrales. Es recomendable superponer líneas de error sobre las barras para indicar los intervalos de confianza, transmitiendo así la incertidumbre asociada a las estimaciones puntuales.

Como ejemplo, el gráfico de barras compara la proporción estimada de hogares que declararon haber gastado dinero en artículos no alimentarios, incorporando líneas de error que indican los intervalos de confianza del 95% para cada estimación.

Para realizar estos gráficos, en primer lugar, se deben realizar las estimaciones puntuales de los tamaños que se van a graficar:

```{r, echo = TRUE, eval = TRUE}
tamano_zona <- diseno %>%
  group_by(Zone) %>%
  summarise( Nd = survey_total(vartype = c("se", "ci")))
tamano_zona 
```

Ahora, se procede a hacer el gráfico como se mostró en las secciones anteriores:

```{r, echo = TRUE, eval = T}
plot25_Ponde <- ggplot(
  data = tamano_zona, 
  aes(
    x = Zone,         
    y = Nd,           
    ymax = Nd_upp,    
    ymin = Nd_low,    
    fill = Zone)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(position = position_dodge(width = 0.9),
    width = 0.3) +
  theme_bw()
plot25_Ponde
```

Como se ha visto en los gráficos anteriores, se pueden extender a variables con muchas más categorías:

```{r, echo = TRUE, eval = TRUE}
tamano_pobreza <- diseno %>%
  group_by(Poverty) %>%
  summarise(Nd = survey_total(vartype = c("se", "ci")))
tamano_pobreza
```

El gráfico se obtiene con una sintaxis homologa a la anterior: 

```{r, echo = TRUE, eval = TRUE}
plot26_Ponde <- ggplot(
  data = tamano_pobreza,
  aes(
    x = Poverty,
    y = Nd,
    ymax = Nd_upp,
    ymin = Nd_low,
    fill = Poverty)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(
    position = position_dodge(width = 0.9),
    width = 0.3) +
  theme_bw()
plot26_Ponde
```

De forma similar a los gráficos Boxplot, es posible realizar comparaciones entre más dos variables. 

```{r, echo = TRUE, eval = TRUE}
tamano_ocupacion_pobreza <- diseno %>%
  group_by(desempleo, Poverty) %>%
  summarise(Nd = survey_total(vartype = c("se", "ci"))) %>% as.data.frame() %>% 
  mutate(desempleo = ifelse(is.na(desempleo),"Ninos",desempleo))
tamano_ocupacion_pobreza
```

El gráfico para la tabla anterior queda de la siguiente manera: 

```{r, echo = TRUE, eval = T}
plot27_Ponde <- ggplot(
  data = tamano_ocupacion_pobreza,
    aes(
      x = Poverty,
      y = Nd,
      ymax = Nd_upp,
      ymin = Nd_low,
      fill = as.factor(desempleo))) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(
    position = position_dodge(width = 0.9),
    width = 0.3) +
  theme_bw()
plot27_Ponde
```

En estos gráficos también se pueden presentar proporciones, como se muestra a continuación: 

```{r, echo = TRUE, eval = TRUE}
prop_ZonaH_Pobreza <- sub_Hombre %>%
  group_by(Zone, Poverty) %>%
  summarise(prop = survey_prop(vartype = c("se", "ci")))
prop_ZonaH_Pobreza
```

Después de tener la tabla con los valores a presentar en el gráfico, los códigos computacionales para realizar el gráfico es el siguiente:

```{r, echo = TRUE, eval = T}
plot28_Ponde <- ggplot(
  data = prop_ZonaH_Pobreza,
  aes(
    x = Poverty, y = prop,
    ymax = prop_upp, ymin = prop_low,
    fill = Zone)) + 
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(
    position = position_dodge(width = 0.9),
    width = 0.3
  ) + scale_fill_manual(values = colorZona) +
  theme_bw()
plot28_Ponde
```

Ahora bien, grafiquemos la proporción de hombres en condición de pobreza por región:

```{r, echo = TRUE, eval = TRUE}
prop_RegionH_Pobreza <- sub_Hombre %>%
  group_by(Region, pobreza) %>%
  summarise(
    prop = survey_prop(vartype = c("se", "ci"))) %>%
  data.frame()
prop_RegionH_Pobreza
```

El gráfico de barras es el siguiente:

```{r, echo = TRUE, eval = T}
plot29_Ponde <- ggplot(
  data = prop_RegionH_Pobreza,
  aes(
    x = Region, y = prop,
    ymax = prop_upp, ymin = prop_low,
    fill = as.factor(pobreza))) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(
    position = position_dodge(width = 0.9),
    width = 0.3
  ) +
  theme_bw()
plot29_Ponde
```

Estos gráficos de barras ponderados permiten visualizar adecuadamente las distribuciones poblacionales de variables categóricas, incorporando la incertidumbre muestral a través de los intervalos de confianza. La correcta implementación de estas visualizaciones garantiza que las representaciones gráficas reflejen fielmente las características de la población bajo estudio, evitando distorsiones que podrían surgir al utilizar únicamente conteos muestrales sin ponderar.










# Referencias

Binder, David A. 1983. "On the Variances of Asymptotically Normal Estimators from Complex Surveys." *International Statistical Review* 51 (3): 279-92. https://doi.org/10.2307/1402588.

Binder, David. A. 2011. "Estimating Model Parameters from a Complex Survey under a Model-Design Randomization Framework." *Pakistan Journal of Statistics* 27(4): 371-390.

Binder, David A., and Milojica S. Kovacevic. 1995. "Estimating Some Measures of Income Inequality from Survey Data: An Application of the Estimating Equations Approach." *Survey Methodology* 21 (2): 137-45.

Brewer, K. R. W. (2002). *Combined Survey Sampling Inference: Weighing Basu's Elephants*. Arnold. 

Bruch, C., Munnich, R., & Zins, S. (2011). "Variance Estimation for Complex Surveys." *Advanced Methodology for European Laeken Indicators - European Commission*.

Chambers, R., Fabrizi, E., & Salvati, N. (2019). "Small area estimation with linked data." *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 83(1): 78-98.

Chambers, R. L. & Silva, A. D. D. (2020). "Improved secondary analysis of linked data: a framework and an illustration." *Journal of the Royal Statistical Society, Series A*, 183 (1): 37-59.

Christen, P. (2012). *Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection*. Springer.

Dean, Natalie, and Marcello Pagano. 2015. "Evaluating Confidence Interval Methods for Binomial Proportions in Clustered Surveys." *Journal of Survey Statistics and Methodology* 3 (4): 484-503. https://doi.org/10.1093/jssam/smv024.

Efron, Bradley. 1979. "Bootstrap Methods: Another Look at the Jackknife." *The Annals of Statistics* 7 (1): 1-26.

Fay, R. E. 1979. "On Adjusting the Pearson Chi-Square Statistic for Clustered Sampling." *ASA Proceedings of the Social Statistics Section*: 402-8.

Fay, R. E., & Herriot, R. A. (1979). "Estimates of income for small places: An application of James-Stein procedures to census data." *Journal of the American Statistical Association*, 74(366): 269-277.

Fellegi, Ivan P. 1980. "Approximate Joint Estimation of the Parameters of Multinomial Distributions in the Analysis of Data from Complex Surveys." *Journal of the American Statistical Association* 75 (370): 261-68.

Fellegi, I. P., & Sunter, A. B. (1969). "A Theory for Record Linkage." *Journal of the American Statistical Association*, 64(328): 1183-1210. https://doi.org/10.1080/01621459.1969.10501049

Freedman, D.A. (2005). *Statistical Models: Theory and Practice*. Cambridge University Press, New York.

Fuller, Wayne A. 1975. "Regression Analysis for Sample Survey." *Sankyha, Series C* 37: 117-32.

———. 2002. "Regression Estimation for Survey Samples (with Discussion)." *Survey Methodology* 28 (1): 5-23.

Gutiérrez, Hugo Andrés. 2015. *TeachingSampling: Selection of Samples and Parameter Estimation in Finite Population*. R package. https://CRAN.R-project.org/package=TeachingSampling.

Gutiérrez, Hugo Andrés. 2020. *Samplesize4surveys: Sample Size Calculations for Complex Surveys*. R package. https://CRAN.R-project.org/package=samplesize4surveys

Hansen, Morris H., William N. Hurwitz, and William G. Madow. 1953. *Sample Survey Methods and Theory*. Vol. 1 and 2. New York: John Wiley & Sons.

Heeringa, Steven G., Brady T. West, and Patricia A. Berglund. 2017. *Applied Survey Data Analysis*. Chapman and Hall CRC Statistics in the Social and Behavioral Sciences Series. CRC Press.

IBM. 2017. *IBM SPSS Complex Samples*. ftp://public.dhe.ibm.com/software/analytics/spss/documentation/statistics/23.0/en/client/Manuals/IBM_SPSS_Complex_Samples.pdf.

Jacob, Guilherme, Anthony Damico, and Djalma Pessoa. 2024. *Poverty and Inequality with Complex Survey Data*. https://www.convey-r.org/.

Kish, Leslie. 1965. *Survey Sampling*. New York: John Wiley & Sons.

Kish, Leslie, and Martin R Frankel. 1974. "Inference from Complex Samples." *Journal of the Royal Statistical Society, Series B* 36: 1-37.

Korn, E.G., and Graubard, B. (1995). "Examples of differing weighted and unweighted estimates from a sample survey." *American Statistician*, 49(3): 291-295.

Kovar, J. G., J. N. K. Rao, and C. F. J. Wu. 1988. "Bootstrap and Other Methods to Measure Errors in Survey Estimates." *Canadian Journal of Statistics* 16 (Suppl.): 25-45.

Langel, Matti, and Yves Tillé. 2013. "Variance Estimation of the Gini Index: Revisiting a Result Several Times Published." *Journal of the Royal Statistical Society: Series A (Statistics in Society)* 176 (2): 521-40. https://doi.org/10.1111/j.1467-985X.2012.01048.x.

Lumley, Thomas. 2010. *Complex Surveys: A Guide to Analysis Using r*. Wiley Series in Survey Methodology. John Wiley & Sons.

———. 2024. "survey: analysis of complex survey samples." (Version 4.4). R package. https://cran.r-project.org/package=survey 

Miller, Jane E. 2004. *The Chicago Guide to Writing about Numbers*. Chicago: University of Chicago Press.

Molina, E. A. and Skinner, C. J. 1992. "Pseudo-likelihood and quasi-likelihood estimation for complex sampling schemes," *Computational Statistics and Data Analysis*, Volume 13, Issue 4. Pages 395-405.

Molina, I., & Rao, J. N. K. (2010). "Small area estimation of poverty indicators." *The Canadian Journal of Statistics*, 38(3): 369-385.

Neter, John, William Wasserman, and Michael H. Kutner. 1996. *Applied Linear Statistical Models*. McGraw-Hill.

Noland, D., and Speed, T. (2000). *Stat Labs: Mathematical Statistics through Applications*. Springer, New York.

Osier, Guillaume. 2009. "Variance Estimation for Complex Indicators of Poverty and Inequality." *Journal of the European Survey Research Association* 3 (3): 167-95. http://ojs.ub.uni-konstanz.de/srm/article/view/369.

Park, Inho, Marianne Winglee, Jay Clark, Keith Rust, Andrea Sedlak, and David Morganstein. 2003. "Design Effects and Survey Planning." *Proceedings of the 2003 Joint Statistical Meetings - Section on Survey Research Methods*, 8.

Pessoa, D., Damico, A., & Jacob, G. (2024). *convey: Estimation of indicators on social exclusion and poverty and its linearization, variance estimation* (Version 1.0.1) [R package]. https://github.com/ajdamico/convey/

Pfeffermann, Danny. 2011. "Modelling of Complex Survey Data: Why Model? Why Is It a Problem? How Can We Approach It?" *Survey Methodology* 37 (2): 115-36.

R Core Team. 2024. *R: A Language and Environment for Statistical Computing*. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.

Rao, J. N. K., & Molina, I. (2015). *Small-Area Estimation*. 2nd edition. John Wiley & Sons, Ltd. 

Rao, J. N. K., and A. J. Scott. 1984. "On Chi-Squared Tests for Multiway Contingency Tables with Cell Proportions Estimated from Survey Data." *The Annals of Statistics* 12: 46-60.

Rao, J. N. K., C F J Wu, and K. Yue. 1992. "Some Recent Work on Resampling Methods for Complex Surveys." *Survey Methodology* 18: 209-17.

Rothbaum, J., & Bee, A. (2021). "Addressing nonresponse bias in household surveys using linked administrative data." *American Economic Association*.

Rust, Keith F., and Valerie Hsu. 2007. "Confidence Intervals for Statistics for Categorical Variables from Complex Samples." In. https://api.semanticscholar.org/CorpusID:195852485.

Särndal, Carl-Erik, Bengt Swensson, and Jan Wretman. 1992. *Model Assisted Survey Sampling*. New York: Springer-Verlag.

SAS. 2010. *SAS/STAT 9.22 User's Guide - Survey Sampling and Analysis Procedures*. https://support.sas.com/documentation/cdl/en/statugsurveysamp/63778/PDF/default/statugsurveysamp.pdf.

Shah, B. V., M. M. Holt, and R. F. Folsom. 1977. "Inference about Regression Models from Sample Survey Data." *Bulletin of the International Statistical Institute* 41 (3): 43-57.

Skinner, Chris J, Daniell Holt, and Tom M F Smith. 1989. *Analysis of Complex Surveys*. New York: John Wiley & Sons.

Stata 2023. *Stata 18 documentation*. https://www.stata.com/features/documentation/.

Thomas, D. R., and J. N. K. Rao. 1987. "Small-Sample Comparisons of Level and Power for Simple Goodness-of-Fit Statistics Under Cluster Sampling." *Journal of the American Statistical Association* 82: 630-36.

Tillé, Yves, and Alina Matei. 2016. *Sampling: Survey Sampling*. https://CRAN.R-project.org/package=sampling.

United Nations. 2005. *Household Surveys in Developing and Transition Countries*. New York, NY: United Nations.

United Nations. 2008. *Designing Household Survey Samples: Practical Guidelines*. Studies in Methods / Department of Economic and Social Affairs, Statistics Division Series f. New York, NY: United Nations.

United Nations. 2010. *Post-enumeration surveys: Operational guidelines* (Series F, No. 98). United Nations. https://unstats.un.org/unsd/publication/seriesf/seriesf_98e.pdf

United Nations Economic Commission for Europe. (2014). *Generic Statistical Business Process Model* (ECE/CES/2014/1). United Nations. https://unstats.un.org/unsd/nationalaccount/workshops/2015/gabon/bd/GSBPM-ENG.pdfUNECE+8

Valliant, R. (2020). "Comparing Alternatives for Estimation from Nonprobability Samples." *Journal of Survey Statistics and Methodology*, 8(2): 231-263. https://doi.org/10.1093/jssam/smz003

Valliant, R. (2024). *svydiags: Regression Model Diagnostics for Survey Data* (Version 0.7). R package. https://cran.r-project.org/web/packages/svydiags/index.html.

Valliant, R., Dever, J. A., & Kreuter, F. (2018). *Practical Tools for Designing and Weighting Survey Samples*. Springer International Publishing. https://doi.org/10.1007/978-3-319-93632-1

Valliant R., Dorfman A. H., Royall R. M. (2000). *Finite Population Sampling and Inference: A Prediction Approach*. John Wiley & Sons, Inc., New York.

Valliant, R., & Rust, K. F. (2010.). "Degrees of Freedom Approximations and Rules-of-Thumb." *Journal of Ofﬁcial Statistics*, 26(4): 585-602.

Westat. 2007. *WesVar 4.3. Users Guide*. http://users.nber.org/~jroth/chap1.pdf.

Wolter, K.M. (2007). *Introduction to Variance Estimation*. New York: Springer.



