---
title: "Análisis de encuestas de hogares con R"
subtitle: "Módulo 5: Modelos de regresión"
author: ""
date: "CEPAL - Unidad de Estadísticas Sociales"
output:
  beamer_presentation:
    colortheme: dove
    fonttheme: default
    incremental: yes
    theme: Berkeley
    toc: yes
    slide_level: 2
    #highlight: pygments
  ioslides_presentation:
    incremental: yes
    widescreen: yes
    toc: yes
  slidy_presentation:
    incremental: yes
Email: andres.gutierrez@cepal.org
editor_options:
  markdown:
    wrap: 90
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE, warning = FALSE, message = FALSE, error = FALSE)
options(digits = 4)
options(tinytex.verbose = TRUE)
library (survey)
library(srvyr)
library(convey)
library(TeachingSampling)
library(printr)
library(stargazer)
library(jtools)
library(broom)
```

## Lectura de la base
El proceso inicia con la lectura de la muestra y definición del objeto `survey.design` 
```{r}
options(survey.lonely.psu="adjust")
encuesta <- readRDS("../Data/encuesta.rds")
data("BigCity", package = "TeachingSampling")

library(srvyr)
diseno <- encuesta %>%
  as_survey_design(
    strata = Stratum,
    ids = PSU,
    weights = wk,
    nest = T
  )
```

## Sub-grupos

Dividir la muestra en sub-grupos de la encuesta.

```{r}
sub_Urbano <- diseno %>%  filter(Zone == "Urban")
sub_Rural  <- diseno %>%  filter(Zone == "Rural")
sub_Mujer  <- diseno %>%  filter(Sex == "Female")
sub_Hombre <- diseno %>%  filter(Sex == "Male")
```

```{r, echo=FALSE, eval=TRUE}
theme_cepal <- function(...) theme_light(10) + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position="bottom", 
        legend.justification = "left", 
        legend.direction="horizontal",
        plot.title = element_text(size = 20, hjust = 0.5),
        ...) 
```

## Modelo de regresión

$$
y=\boldsymbol{\beta}_{0}+\boldsymbol{\beta}_{1}x+\epsilon
$$ 
$$
E\left(y\mid x\right)=B_{0}+B_{1}x
$$
donde $B = [B_0, B1]$ y el estimador de $B$ esta dado por: 

$$
\hat{B}=\boldsymbol{\left(X^{T}WX\right)^{-1}X^{T}Wy}
$$ 

$$
F\left(B\right)=\sum_{i=1}^{N}\left(y_{i}-\boldsymbol{x}_{i}\boldsymbol{B}\right)^{2}
$$

$$
\widehat{WSSE}_{pop}=\sum_{h}^{H}\sum_{\alpha}^{a_{h}}\sum_{i=1}^{n_{h\alpha}}w_{h\alpha i}\left(y_{hai}-\boldsymbol{x}_{h\alpha i}\boldsymbol{B}\right)^{2}
$$

## Modelo nulo (Q_W)
El primer modelo ajustado es utilizado para construir los **qweights**, que serán de útilidad más adelante.   
```{r}
modNul <- svyglm(Income ~ 1, design = diseno)
fit_Nul <- lm(wk ~ 1, data = encuesta)
qw <- predict(fit_Nul)

encuesta %<>% mutate(wk1 = wk/qw)

diseno_qwgt <- encuesta %>%
  as_survey_design(
    strata = Stratum,
    ids = PSU,
    weights = wk1,
    nest = T
  )
modNul_qw <- svyglm(Income ~ 1, design = diseno_qwgt)

```

## Scatterplot con los datos poblacionales
Para observar que existe una correlación entre el ingreso y el gasto es construido un scatterplot.
```{r, plot1, echo = TRUE, eval = FALSE}
library(ggplot2); library(ggpmisc)
plot_BigCity <- 
  ggplot(data = BigCity,
         aes(x = Expenditure, y = Income)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              formula = y ~ x) +
  theme_cepal()

plot_BigCity + stat_poly_eq(formula = y~x, 
  aes(label = paste(..eq.label..,
   ..rr.label.., sep = "~~~"),size = 3), parse = TRUE)

```

## Scatterplot con los datos poblacionales
El resultado obtenido es: 
```{r, plot1, echo = FALSE, eval = TRUE}
```

## Modelo poblacional
El modelo poblacional es estimado con `lm`
```{r, tab1, results='asis', echo=TRUE, eval = TRUE}
library(modelsummary)
fit <- lm(Income ~ Expenditure, data = BigCity)
```
La función `modelsummary` permite tener la siguiente tabla

```{r, results='asis', echo=FALSE, eval = TRUE}
modelsummary(list(Pob = fit),statistic = NULL, 
             title = "Modelo BigCity",
              output = "markdown", 
             gof_omit = 'BIC|Log|AIC|F' )
```

## Scatterplot con los datos encuesta sin ponderar
Ahora, una sintaxis similar permite construir el scatterplot en la muestra. 
```{r, plot2, echo = TRUE, eval = FALSE}
plot_sin <- 
  ggplot(data = encuesta,
         aes(x = Expenditure, y = Income)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              formula = y ~ x) +
  theme_cepal()
plot_sin + stat_poly_eq(formula = y~x, 
  aes(label = paste(..eq.label..,
     ..rr.label.., sep = "~~~"), size = 5),
  parse = TRUE)
```

## Scatterplot con los datos encuesta sin ponderar

```{r, plot2, echo = FALSE, eval = TRUE}
```

## Modelo sin ponderar
El modelo ignorando los factores de expansión quedas así: 
```{r, tab2, echo = TRUE, eval = FALSE}
fit_sinP <- lm(Income ~ Expenditure, data = encuesta)
stargazer(fit_sinP, header = FALSE,
          title = "Modelo encuesta Sin ponderar", 
          style = "ajps")
```

## Modelo sin ponderar

```{r, tab2, results='asis', echo = FALSE, eval = TRUE}
```

## Scatterplot con los datos encuesta ponderado
Para que el gráfico tenga en cuenta las ponderaciones debe agregar ` mapping = aes(weight = wk)` en la función `geom_smooth`.
```{r, plot3, echo = TRUE, eval = FALSE}
plot_Ponde <- 
  ggplot(data = encuesta,
         aes(x = Expenditure, y = Income)) +
  geom_point(aes(size = wk)) +
  geom_smooth(method = "lm",
              se = FALSE,
              formula = y ~ x, 
              mapping = aes(weight = wk)) +
  theme_cepal()
plot_Ponde + stat_poly_eq(formula = y~x, 
  aes(weight = wk, 
    label = paste(..eq.label..,
      ..rr.label.., sep = "~~~")), 
  parse = TRUE,size = 5)
```

## Scatterplot con los datos encuesta sin ponderar

```{r, plot3, echo = FALSE, eval = TRUE}
```

## Modelo ponderado `lm`
La función `lm` permite incluir los `weights` en la estimación de los coeficientes. 
```{r, tab3, echo = TRUE, eval = FALSE}
fit_Ponde <- lm(Income ~ Expenditure, 
                data = encuesta, weights = wk)
stargazer(fit_Ponde, header = FALSE,
          title = "Modelo encuesta ponderada", 
          style = "ajps")
```

## Modelo ponderado lm

```{r, tab3, results='asis', echo = FALSE, eval = TRUE}
```

## Modelo ponderado svyglm
Ahora, emplee la función `svyglm` de `survey`
```{r, tab4, echo = TRUE, eval = TRUE}
fit_svy <- svyglm(Income ~ Expenditure, 
                  design = diseno)
modNul <- svyglm(Income ~ 1, design = diseno)
s1 <- summary(fit_svy)
s0 <-summary(modNul)
```

## Calculo del $R^2$

$$
R^2=1-\frac{SSE}{SST}
$$ 

donde

$$
SSE=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\left(y_{i}-\boldsymbol{x}_{i}\boldsymbol{B}\right)^{2}
$$

$$
R_{weighted}^{2}=1-\frac{WSSE}{WSST}
$$

## Calculo del $R^2$

```{r, echo = TRUE, eval = TRUE}
s1$dispersion
s0$dispersion
(R2 = 1-119563/243719)
n = sum(diseno_qwgt$variables$wk)
(R2Adj = 1-((1-R2)*(n-1)/(n-1-1)))
```

## Resumen del Modelo

```{r, tab4a, results='asis', echo = FALSE, eval = TRUE}
stargazer(fit_svy, header = FALSE,
          title = "Modelo encuesta ponderada, svyglm", 
          style = "ajps", omit.stat = "ll")
```

## Comparando los resultados

```{r, plot4, echo=TRUE, eval=FALSE}
df_model <- data.frame(
  intercept = c(coefficients(fit)[1],
               coefficients(fit_sinP)[1], 
               coefficients(fit_Ponde)[1],
               coefficients(fit_svy)[1]), 
  slope = c(coefficients(fit)[2],
               coefficients(fit_sinP)[2], 
               coefficients(fit_Ponde)[2],
               coefficients(fit_svy)[2]),
  Modelo = c("Población", "Sin ponderar", 
             "Ponderado(lm)", "Ponderado(svyglm)"))
plot_BigCity +  geom_abline( data = df_model,
    mapping = aes( slope = slope,
      intercept = intercept, linetype = Modelo,
      color = Modelo ), size = 2
  )

```

## Comparando los resultados

```{r, plot4, echo=FALSE, eval=TRUE}
```

## Comparando los resultados

```{r, echo = FALSE, eval = TRUE}
library('modelsummary')
options("modelsummary_format_numeric_latex" = "plain")
modelsummary(list(Pob = fit),statistic = NULL,
             output = "markdown", gof_omit = 'BIC|Log' )

```

## Comparando los resultados

```{r, echo = FALSE, eval = TRUE}
modelsummary(list("Sin Pond" = fit_sinP,
                  "Ponde(lm)" = fit_Ponde,
                  "Ponde(svyglm)" = fit_svy), 
             statistic = c("p = ({p.value})"),
             output = "markdown", gof_omit = 'BIC|Log' )

```

## Metodología de los Q_Weighting de pfefferman
Ahora, se definen los Q_Weighting de pfefferman: 
```{r}
fit_wgt <- lm(wk ~ Expenditure, data = encuesta)
wgt_hat <- predict(fit_wgt)
encuesta %<>% mutate(wk2 = wk/wgt_hat)

diseno_qwgt <- encuesta %>%
  as_survey_design(
    strata = Stratum,
    ids = PSU,
    weights = wk2,
    nest = T
  )

```

## Modelos empleando los Q_Weighting
Estimando los coeficientes del modelo con los Q_Weighting de pfefferman
```{r}
library(tidyr)
fit_svy_qwgt <- svyglm(Income ~ Expenditure,
                       design = diseno_qwgt)
modNul <- svyglm(Income ~ 1, design = diseno_qwgt)
s0 <- summary(modNul)
s1 <- summary(fit_svy_qwgt)
tidy(fit_svy_qwgt)
```

## Calculo del $R^2$
Obtenido el $R^2$
\scriptsize
```{r}
s1$dispersion
s0$dispersion
(R2 = 1-119563/243719)
n = sum(diseno_qwgt$variables$wk2)
(R2Adj = 1-((1-R2)*(n-1)/(n-1-1)))

```

## Modelos empleando los Q_Weighting

```{r,echo=FALSE}
modelsummary(list("svyglm(wgt)" = fit_svy, 
                  "svyglm(qwgt)" = fit_svy_qwgt), 
             output = "markdown", 
              statistic = c("p = ({p.value})"),
             title = "Comprando Modelos con Q Weighting",
             gof_omit = 'BIC|Log')
```

## Modelo propuesto
Después de realizar la comparación entre las diferentes formas de estimar los coeficientes del modelo se opta  por la metodología consolidadas en  `svyglm`
```{r, mod1, echo=TRUE, eval=FALSE, results='asis'}
diseno_qwgt %<>% mutate(Age2 = Age^2) 
mod_svy <- svyglm(
  Income ~ Expenditure + Zone + Sex + Age2 ,
                       design = diseno_qwgt)
s1 <- summary(mod_svy)
s0 <- summary(modNul)
stargazer(mod_svy, header = FALSE,single.row = T,
           title = "Modelo propuesto", 
           style = "ajps",  omit.stat=c("bic", "ll"))
```

## Resumen del modelo propuesto

```{r,mod1, echo=FALSE,eval=TRUE,  results='asis'}
```

## Calculo del $R^2$
Estimación del $R^2$ para el modelo propuesto 
```{r}
s1$dispersion
s0$dispersion
(R2 = 1-121651/250708)
n = sum(diseno_qwgt$variables$wk2)
(R2Adj = 1-((1-R2)*(n-1)/(n-1-1)))

```

## Residuales estándarizados

$$
r_{pi}=\left(y_{i}-\mu_{i}\left(\hat{\boldsymbol{B}}_{w}\right)\right)\sqrt{\frac{w_{i}}{V\left(\hat{\mu}_{i}\right)}}
$$

$$
H=W^{1/2}X\left(X^{T}WX\right)^{-1}W^{1/2}
$$ donde 

$$
W=diag\left\{ \frac{w_{1}}{V\left(\hat{\mu}_{1}\right)\left[g'\left(\mu_{1}\right)\right]^{2}},\ldots,\frac{w_{n}}{V\left(\hat{\mu}_{1}\right)\left[g'\left(\mu_{n}\right)\right]^{2}}\right\} 
$$


con $g$ es una función de enlace que es especificada mediante un Modelo lineal
generalizado.

## Diagnostico del Modelo

```{r}
par(mfrow = c(2,2))
plot(mod_svy)
```

## Pruebas de normalidad

$H_0$: Los errores proviene de una distribución normal.
$H_1$: Los errores no proviene de una distribución normal.

Algunas librerías que podemos emplear son:

```{r, echo = TRUE, eval=TRUE}
library(normtest) #REALIZA 5 PRUEBAS 
library(nortest)  #REALIZA 10 PRUEBAS 
library(moments)  #REALIZA 1 PRUEBA
```

## Librería `svydiags` para el diagnostico del modelo. 
La librería `svydiags` esta pensada ayudar en el diagnostico de modelos de regresión lineal, siendo una extensión más para complementar el paquete `survey`. 

Con las librería svydiags se extraen los residuales estandarizados así: 

```{r, echo = TRUE, eval=TRUE}
library(svydiags)
stdresids = as.numeric(svystdres(mod_svy)$stdresids)
diseno_qwgt$variables %<>% 
  mutate(stdresids = stdresids)
```

## Histograma de los residuales
El primer análisis de normalidad se hace por medio del histograma. 
```{r, hist1, echo = TRUE, eval = FALSE}
ggplot(data = diseno_qwgt$variables,
       aes(x = stdresids)) +
  geom_histogram(aes(y = ..density..),
                 colour = "black", 
                 fill = "blue", alpha = 0.3) +
  geom_density(size = 2, colour = "blue") +
  geom_function(fun = dnorm, colour = "red",
                size = 2) +
  theme_cepal()+labs(y = "")
```
## Histograma de los residuales

```{r, hist1, echo=FALSE, eval = TRUE}
```

## Pruebas de normalidad Kolmogorov-Smirnov

```{r}
nortest::lillie.test(diseno_qwgt$variables$stdresids)
```

## Varianza constante
Agregando las predicciones a la base de datos. 
```{r, echo=TRUE, eval=TRUE,size="tiny"}
library(patchwork)
diseno_qwgt$variables %<>% 
  mutate(pred = predict(mod_svy))
g2 <- ggplot(data = diseno_qwgt$variables,
       aes(x = Expenditure, y = stdresids))+
  geom_point() +
  geom_hline(yintercept = 0) + theme_cepal()
g3 <- ggplot(data = diseno_qwgt$variables,
       aes(x = Age2, y = stdresids))+
  geom_point() +
  geom_hline(yintercept = 0) + theme_cepal()
```

## Varianza constante

```{r, plot6, echo=TRUE, eval=FALSE,size="tiny"}
g4 <- ggplot(data = diseno_qwgt$variables,
       aes(x = Zone, y = stdresids))+
  geom_point() +
  geom_hline(yintercept = 0) + theme_cepal()
g5 <- ggplot(data = diseno_qwgt$variables,
       aes(x = Sex, y = stdresids))+
  geom_point() +  geom_hline(yintercept = 0) +
  theme_cepal()

(g2|g3)/(g4|g5)
```

## Varianza constante

```{r, plot6, echo=FALSE, eval=TRUE}
```

## Distancia de cook

$$
c_{i}=\frac{w_{i}^{*}w_{i}e_{i}^{2}}{p\phi V\left(\hat{\mu}_{i}\right)\left(1-h_{ii}\right)^{2}}\boldsymbol{x}_{i}^{t}\left[\widehat{Var}\left(U_{w}\left(\hat{\boldsymbol{B}}_{w}\right)\right)\right]^{-1}\boldsymbol{x}_{i}
$$

donde,

-   $w_i^* =$ Pesos de la encuesta.
-   $w_i$ Elementos por fuera de la diagonal de la matriz hat
-   $e_i=$ residuales
-   $p=$ número de parámetros del Modelo de regresión.
-   $\phi =$ parámetro de dispersión en el glm
-   $\widehat{Var}\left(U_{w}\left(\hat{\boldsymbol{B}}_{w}\right)\right) =$ estimación de
    varianza linealizada de la ecuación de puntuación, que se utiliza para pseudo MLE en
    Modelos lineales generalizados ajustados a datos de encuestas de muestras complejas

## Distancia de cook

Una vez que se ha determinado el valor de la $D$ de Cook para un elemento de muestra
individual, se puede calcular la siguiente estadística de prueba para evaluar la
importancia de la estadística $D$:

$$
\frac{\left(df-p+1\right)\times c_{i}}{df} \doteq F_{\left(p,df-p\right)}
$$

donde $df=$ grados de liberta basados en el diseño.

Por otro lado, la literatura considera a las observaciones influyentes cuando sean mayores a 2 o 3

## Detección de observaciones influyentes (Distancia de cook)
La función `svyCooksD` pertenece a la librería `svydiags`
```{r, dcook, eval=FALSE}
 d_cook = data.frame(
   cook = svyCooksD(mod_svy), 
     id = 1:length(svyCooksD(mod_svy)))

ggplot(d_cook, aes(y = cook, x = id)) +
  geom_point() + 
  theme_bw(20)
```

## Detección de observaciones influyentes (Distancia de cook)

```{r, dcook, eval=TRUE, echo=FALSE}
```

## $D_fBeta_{(i)}$

$$
D_fBeta_{(i)} = \hat{B}-\hat{B}_{\left(i\right)}=\frac{\boldsymbol{A}^{-1}\boldsymbol{X}_{\left(i\right)}^{t}\hat{e}_{i}w_{i}}{1-h_{ii}}
$$

Donde $\boldsymbol{A} =\boldsymbol{X}^{t}\boldsymbol{WX}$ $\hat{B}_{(i)}$ es el vector de parámetros estimados una vez se ha eliminado la
i-ésima observación, $h_{ii}$ es el correspondiende elemento de la diagonal de *H* y
$\hat{e}_i$ es el residual de la i-ésima observación.

## $D_fBetas_{(i)}$

$$
D_fBetas_{\left(i\right)}=\frac{{c_{ji}e_{i}}\big/{\left(1-h_{ii}\right)}}{\sqrt{v\left(\hat{B}_{j}\right)}}
$$

donde: 

  - $c_{ji}=$ es el ji-estimo elemento de $\boldsymbol{A}^{-1}w_{i}^{2}\boldsymbol{X}_{\left(i\right)}\boldsymbol{X}_{\left(i\right)}^{t}\boldsymbol{A}^{-1}$

 - El estimador de $v\left(\hat{B}_{j}\right)$ basado en el Modelo se obtiene como: $v_{m}\left(\hat{B}_{j}\right)=\hat{\sigma}\sum_{i=1}^{n}c_{ji}^{2}$ con $\hat{\sigma}=\sum_{i\in s}w_{i}e^2/ \left( \hat{N} - p \right)$ y $\hat{N} = \sum_{i \in s}w_{i}$

  - La i-ésima observación es influyente
para $B_j$ si $\mid D_{f}Betas_{\left(i\right)j}\mid\geq\frac{z}{\sqrt{n}}$ con $z=$ 2 o 3
  - Como alternativa puede usar $t_{0.025,n-p}/\sqrt(n)$ donde  $t_{0.025,n-p}$ es el percentil $97.5$

## Detección de observaciones influyentes ($D_{f}Betas_{\left(i\right)j}$)

```{r}
d_dfbetas = data.frame(t(svydfbetas(mod_svy)$Dfbetas))
colnames(d_dfbetas) <- paste0("Beta_", 1:5)
d_dfbetas %>% slice(1:10L)
```

## Detección de observaciones influyentes ($D_{f}Betas_{\left(i\right)j}$)

```{r eval=TRUE, echo=TRUE}
d_dfbetas$id <- 1:nrow(d_dfbetas)
d_dfbetas <- reshape2::melt(d_dfbetas, 
                            id.vars = "id")
cutoff <- svydfbetas(mod_svy)$cutoff
d_dfbetas %<>%
  mutate(
    Criterio = ifelse(
      abs(value) > cutoff, "Si", "No"))

tex_label <- d_dfbetas %>% 
  filter(Criterio == "Si") %>%
  arrange(desc(abs(value))) %>%
  slice(1:10L)
```

## Detección de observaciones influyentes ($D_{f}Betas_{\left(i\right)j}$)

```{r eval=FALSE, plot_dfbetas, echo=TRUE}
ggplot(d_dfbetas, aes(y = abs(value), x = id)) +
  geom_point(aes(col = Criterio)) +
  geom_text(data = tex_label,
            angle = 45,
            vjust = -1,
            aes(label = id)) +
  geom_hline(aes(yintercept = cutoff)) +
  facet_wrap(. ~ variable, nrow = 2) +
  scale_color_manual(
    values = c("Si" = "red", "No" = "black")) +
  theme_cepal()
```

## Detección de observaciones influyentes ($D_{f}Betas_{\left(i\right)j}$)

```{r eval=TRUE, plot_dfbetas, echo=FALSE}
```

## Matriz H asociada al PMLE
  - La matriz asociada al Estimador de Pseudo Máxima Verosimilitud (PMLE) de $\hat{\boldsymbol{B}}$ es $\boldsymbol{H}=\boldsymbol{XA}^{-1}\boldsymbol{X}^{-t}\boldsymbol{W}$ cuya diagonal esta dado por $h_{ii} = \boldsymbol{x_{i}^tA}^{-1}\boldsymbol{x_{i}}^{-t}w_{i}$.

  - Una observación puede ser grande y, como resultado, influir en las predicciones, cuando un $x_i$ es considerablemente diferente del promedio ponderado $\bar{x}_w=\sum_{i\in s}w_{i}\boldsymbol{x_{i}}\big/\sum_{i\in s}w_i$.

## Detección de observaciones influyentes ($h_{ii}$)

```{r, hat, eval=FALSE, echo=TRUE}
vec_hat <- svyhat(mod_svy, doplot = FALSE)
d_hat = data.frame(hat = vec_hat, 
                    id = 1:length(vec_hat))
d_hat %<>% mutate(
  C_cutoff = ifelse(hat > (3 * mean(hat)),
                    "Si", "No"))

ggplot(d_hat, aes(y = hat, x = id)) +
  geom_point(aes(col = C_cutoff)) + 
  geom_hline(yintercept = (3 * mean(d_hat$hat))) +
  scale_color_manual(
    values = c("Si" = "red", "No" = "black"))+
  theme_cepal()
```

## Detección de observaciones influyentes ($h_{ii}$)

```{r, hat, eval=TRUE, echo=FALSE}
```

## Estadístico $D_{f}Fits_{\left(i\right)}$

$$
D_{f}Fits_{\left(i\right)}= \frac{h_{ii}e_{i}\big/\left(1-h_{ii}\right)}{\sqrt{v\left(\hat{\beta}_{j}\right)}}
$$
Donde, $\sqrt{v\left(\hat{\beta}_{j}\right)}$ puede ser aproximada por el diseño o el Modelo. 
La i-ésima observación se considera influyente en el ajuste del Modelo si
$\mid DfFits\left(i\right)\mid\geq z\sqrt{\frac{p}{n}}$ con $z =$ 2 o 3

## Detección de observaciones influyentes ($D_{f}Fits_{\left(i\right)}$)

```{r,plot_dffit, echo=TRUE, eval=FALSE}
d_dffits = data.frame(
  dffits = svydffits(mod_svy)$Dffits, 
  id = 1:length(svydffits(mod_svy)$Dffits))

cutoff <- svydffits(mod_svy)$cutoff

d_dffits %<>% mutate(
    C_cutoff = ifelse(abs(dffits) > cutoff, "Si", "No"))
ggplot(d_dffits, aes(y = abs(dffits), x = id)) +
  geom_point(aes(col = C_cutoff)) + 
  geom_hline(yintercept = cutoff) + 
   scale_color_manual(
    values = c("Si" = "red", "No" = "black"))+
  theme_cepal()
```

## Detección de observaciones influyentes ($D_{f}Fits_{\left(i\right)}$)

```{r,plot_dffit, echo=FALSE, eval=TRUE}
```

## Inferencia sobre los parámetros del Modelo

$$
t=\frac{\hat{\beta}_{k}-\beta_{k}}{se\left(\hat{\beta}_{k}\right)}\sim t_{n-p}
$$

$$
\hat{B}\pm t_{\left(1-\frac{\alpha}{2},df\right)}\times se\left(\hat{B}\right)
$$

## Estimación del dato

$$
\hat{E}(y_{i}\mid\boldsymbol{x}_{obs,i})=\boldsymbol{x}_{obs,i}\hat{\boldsymbol{\beta}}
$$

$$
\hat{E}(y_{i}\mid\boldsymbol{x}_{obs,i})=\beta_{0}+\beta_{1}x_{1i}+\beta_{2}x_{2i}+\beta_{3}x_{3i}+\beta_{4}x_{4i} 
$$

```{r, echo=FALSE}
mod_svy %>% broom::tidy()

```
\footnotesize
$$
\hat{E}(y_{i}\mid\boldsymbol{x}_{obs,i})= 91.6319 +1.0893x_{1i} + 48.7667x_{2i} + 8.0933x_{3i} + 0.0115x_{4i}
$$

## Estimación del dato

```{r, echo=FALSE}
 model.matrix(mod_svy) %>%
  as.data.frame()%>% slice(1:7)
```
\footnotesize
$$
\hat{y_i}= 91.6319 +1.0893(247.9) + 48.7667(0) + 8.0933(1) + 0.0115(55^2) = 404.6
$$

## Estimando el IC de predicción

$$
var\left(\hat{E}\left(y_{i}\mid\boldsymbol{x}_{obs,i}\right)\right)=\boldsymbol{x}_{obs,i}^{t}cov\left(\boldsymbol{\beta}\right)\boldsymbol{x}_{obs,i}
$$

\scriptsize
```{r}
vcov(mod_svy)
```

## Estimando el IC de predicción

```{r, echo=TRUE}
xobs <- model.matrix(mod_svy) %>%
  data.frame() %>% slice(1) %>% as.matrix()

cov_beta <- vcov(mod_svy) %>% as.matrix()

as.numeric(sqrt((xobs)%*%cov_beta%*%t(xobs)))
```

## Intervalo de confianza para la predicción

$$
\boldsymbol{x}_{obs,i}\hat{\beta}\pm t_{\left(1-\frac{\alpha}{2},n-p\right)}\sqrt{var\left(\hat{E}\left(y_{i}\mid\boldsymbol{x}_{obs,i}\right)\right)}
$$

## Utilizando la función predict

```{r,pred01, echo=TRUE,eval=FALSE}
pred <- data.frame(predict(mod_svy, type = "link"))
pred_IC <- data.frame(
  confint(predict(mod_svy, type = "link")))
colnames(pred_IC) <- c("Lim_Inf", "Lim_Sup")
pred <- bind_cols(pred, pred_IC)
pred$Expenditure <- encuesta$Expenditure
pred %>% slice(1:6L)
```

## Utilizando la función predict
```{r,pred01, echo=FALSE,eval=TRUE}
```

## Scatterplot de la predicción

```{r, plot_pred, echo=TRUE,eval=FALSE}
pd <- position_dodge(width = 0.2)
ggplot(pred %>% slice(1:100L),
       aes(x = Expenditure , y = link)) +
  geom_errorbar(aes(ymin = Lim_Inf,
                    ymax = Lim_Sup),
                width = .1,
                linetype = 1) +
  geom_point(size = 2, position = pd) +
  theme_bw()
```

## Scatterplot de la predicción

```{r, plot_pred, echo=FALSE,eval=TRUE}
```

## Predicción fuera de las observaciones.

```{r}
datos_nuevos <- data.frame(Expenditure = 1600, 
                           Age2 = 40^2, Sex = "Male", 
                           Zone = "Urban")
```
\footnotesize 
$$
\hat{y_i}= 91.6319 +1.0893(1600) + 48.7667(0) + 8.0933(1) + 0.0115(40^2) =  1910
$$
\normalsize
$$
var\left(\hat{E}\left(y_{i}\mid\boldsymbol{x}_{obs,i}\right)\right)=\boldsymbol{x}_{obs,i}^{t}cov\left(\boldsymbol{\beta}\right)\boldsymbol{x}_{obs,i} + \hat{\sigma}^2_{yx}
$$

```{r}
x_noObs = matrix(c(1,1600,1,1,40^2),nrow = 1)
as.numeric(sqrt(x_noObs%*%cov_beta%*%t(x_noObs)))
```

## Intervalo de confianza para la predicción

$$
\boldsymbol{x}_{obs,i}\hat{\beta}\pm t_{\left(1-\frac{\alpha}{2},n-p\right)}\sqrt{var\left(\hat{E}\left(y_{i}\mid\boldsymbol{x}_{obs,i}\right)\right)+\hat{\sigma}_{yx}^{2}}
$$

## Predicción fuera de las observaciones.

```{r}
predict(mod_svy, newdata = datos_nuevos, type =  "link")
confint(predict(mod_svy,newdata = datos_nuevos))
```



# Procesando múltiples bases. 

## Lectura de múltiples bases

Para realizar la lectura de múltiples bases debemos conocer las rutas donde estas estas guardadas para ello empleamos la función `file.list` del paquete base, que nos permite tener un listado completo de los archivos.


```{r, tab_mult0, eval=FALSE}

(data_path <- list.files("Z:/BC/",full.names = TRUE, 
                        pattern = "2020") %>% 
  tibble(path = .) %>%  
   mutate( pais = gsub("Z:\\/BC\\/(.*)_.*","\\1",
                       x = path)))

```
*Note* que utiliza la función `gsub` para separar el nombre del país de la ruta.


## Lectura de múltiples bases

```{r, tab_mult0, eval=TRUE, echo=FALSE}
```


## Lectura de encuestas

Para la lectura de los archivos, se procede de la siguiente forma.

```{r, eval=TRUE}
require(purrr)
require(haven)
data_path %<>% 
  mutate(encuesta = path %>% map(~read_dta(.x) %>% 
           transmute(upm = `_upm`,
           estrato =`_estrato`,
           sexo, areageo2,lp,li,ingcorte,
           fep =`_fep`)))
```

La función `map` es utilizada para trabajar con los elementos de una lista. 
Las variables seleccionadas son sexo, área geográfica (areageo2), Linea de pobreza (lp), Linea de indigencia (li), Íngreso persona (ingcorte) y factor de expansión por persona (fep).

## Lectura de encuestas (resultado)

![](Imagenes/Otros/Lectura%20multiple%20base1.PNG)

El resultado es objeto tipo `tibble` el cual permite observar de forma compacta el contenido de una lista e indica el tipo y tamaño de cada objeto en la contenido en la lista.

## Definir el diseño

Ahora se debe definir un diseño para cada encuesta, para nuestro ejemplo se define el diseño muestral.

```{r}
options(survey.lonely.psu="adjust")
data_path %<>% mutate(
  diseno = encuesta %>%
           map(~as_survey_design(.data = .x,
               ids = upm,
               strata = estrato, 
               weights = fep,
               nest = T
               )))
```

## Definir el diseño (resultado)

![](Imagenes/Otros/Lectura%20multiple%20base2.PNG)

## Estimación de los coeficiente del modelo en multiples encuestas
\footnotesize
```{r, tab_mult1, eval=FALSE}
library(tidyr)
data_path[1:4,] %>% mutate(
  model = map(diseno,
              ~svyglm(ingcorte~sexo+areageo2,.x) %>% 
               coef() %>%
               data.frame(estimado = .) %>% 
               tibble::rownames_to_column(var = "Coef"))) %>% 
  dplyr::select(pais,model) %>% unnest(model) 
```

## Estimación de los coeficiente del modelo en multiples encuestas (Resultado)

```{r, tab_mult1, eval=TRUE, echo=FALSE}
```



## Alternativa para el procesamiento de múltiples archivos.

En ocasiones solo se desea obtener un resultado rápido para realizar un reporte o una comparación rápida de información, en estas ocasiones no es necesario guardar en la memoria de `R` toda la encuesta, por esta razón se ilustra una alternativa de procesamiento de múltiples archivos.  

- **Paso 1** Leer archivo y organizar encuestas. 
- **Paso 2** Definir diseño muestral. 
- **Paso 3** Procesar información.
- **Paso 4** Organizar y presentar resultados. 

## Creando función para el procesamiento de múltiples archivos.
\tiny

```{r}
options(survey.lonely.psu="adjust")
model_aux <- function(input_file){
  ## Paso 1 
  encuesta <- read_dta(input_file) %>% 
    transmute(upm = `_upm`,   estrato =`_estrato`,
              sexo, areageo2,lp,li,ingcorte, fep =`_fep`)
  ## Paso 2 
  diseno <- as_survey_design(.data = encuesta,
                             ids = upm,
                             strata = estrato, 
                             weights = fep,
                             nest = T)
  ## Paso 3 
  s <- svyglm(ingcorte~sexo+areageo2,diseno) %>% summary()
  s$coefficients %>% 
    data.frame() %>% 
    tibble::rownames_to_column(var = "Coef")
}

```

## Procesando encuestas múltiples
Para el *Paso 4* realizamos la siguiente sintaxis.

```{r,tab_mult7, echo=TRUE, eval=FALSE}
list.files("Z:/BC/",full.names = TRUE, 
                        pattern = "2020")[1:3] %>% 
  map_df(~model_aux(.x)) 
```
Los resultados se muestran en el orden de lectura de los archivos
\tiny
```{r,tab_mult7, echo=FALSE, eval=TRUE}
```


## ¡Gracias!

::: yellow
*Email*: [andres.gutierrez\@cepal.org](mailto:andres.gutierrez@cepal.org){.email}
:::




